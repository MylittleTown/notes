### 什么是机器学习

“在没有明确设置的情况下，使计算机具有学习能力的研究领域” -- Arthur Samuel

“计算机程序从经验E 中学习，解决某一任务T 进行某一性能度量P，通过P 测定在T 上的表现因经验E 而提高” -- Tom Mitchell

### 监督学习

也称为“回归问题”，用来预测连续的数值输出，“回归”这个术语是指设法预测连续值的属性。对于离散值的预测，就变成了分类问题，“分类”是指预测一个离散值的输出，实际上在分类问题中，也有两个以上的可能的输出值。

在监督学习中，对于数据集中的每个样本，我们想要算法预测并得出的“正确答案”

监督学习：分类问题和回归问题

监督学习算法的工作流程：

1. 向学习算法提供训练集

2. 学习算法的任务是输出一个函数，通常用小写字母h 表示“假设函数”，假设函数是一个引导从x 得到y 的函数

3. 决定怎么表示这个假设函数h，最初的假设函数如下方表示：

   ![image-20210101111240219](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210101111240219.png)

4. 预测y 是关于x 的线性函数，训练集的作用是预测y 是关于x 的线性函数h(x)

### 无监督学习

对于监督学习中的每一个样本，我们已经被清楚的告知了什么是所谓的“正确答案”，在无监督学习中，我们所用的数据和之前不同，没有任何标签的，换句话说都具有相同的标签或者都没有标签。

无监督学习算法，可以将数据分成多个不同的簇，也就是聚类算法

### 线性回归

在监督学习中，我们有一个数据集，它被称为一个训练集，监督学习的任务就是从这个数据集中学习如何预测离散值或者连续值的输出。

之后的学习过程中，小写字母m 通常用于表示训练样本的数量；小写字母x 来代表输入变量或者说是特征；用y 来表示输出变量，也就是要预测的目标变量；(x, y) 来表示一个训练样本，用(xi, yi) 来表示一个特定训练样本，表示第i 个训练样本，其中i 是训练集中的索引，如果有两个下标，那么通常x(ab) 中的a 表示第a 个训练样本，也就是索引，b 表示第几个特征

由于线性的情况是学习的基础，先学习拟合线性函数，然后将在此基础之上，最终处理更加复杂的模型，以及学习更加复杂的学习算法，线性回归的假设函数对应的模型被称为“线性回归”

### 代价函数

线性回归模型中的假设函数，也就是用来预测的函数是一种线性函数形式。theta0 和theta1 这些theta(i) 被称为模型参数。

为了决定假设函数h，需要得出theta0 和theta1两个参数的值，来让假设函数表示的直线尽量的与这些数据点很好的拟合。也就是我们要选择theta0 和theta1 能使h(x) 也就是输出x 时根据h(x) 预测得到的值最接近该样本对应的y 值。

在线性回归中，我们要解决的是一个最小化问题，也就是要让(h(x) - y) 的结果尽量小。

将训练样本集中的1 到m 的样本实际值和根据假设函数预测得到的值的差的平方求和，线性回归中的目的就是希望尽量减小这个求和的结果（从个体i 到样本集合，采用求和的方式，这里采用求和的方式可能可以换吧），不难发现，这里其实是**个体估计总体的“方差”的结构，那是因为方差有个很好的性质 -- 可以求导，有利于未来的优化，但是要注意，这里不是方差，由于方便之后的求导过程（通过梯度下降算法来推导最小值，梯度下降算法需要对该表达式求导），所以我们在“设计”代价函数时乘以1/m，目的是为了对原函数求导的结果不依赖训练集中元素的数量**。假设函数是关于x 的单变量线性函数，同时也可以看作是关于theta0 和theta1 的函数（因theta0 和theta1 的变化而变化），所以上述关于代价函数的推导表示关于theta0 和theta1 的最小化过程，我们需要做的就是找到theta0 和theta1 的值来使这个表达式（代价函数）的值最小

那么原问题就变成“找到能使训练集中预测值和真实值的差的平方和的1/2m 最小的theta0 和theta1的值”，代价函数我们用 J(theta0, theta1)来表示，上述过程我们将表示为

![image-20210101114519163](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210101114519163.png)

代价函数也被称为平方误差函数，或平方误差代价函数

损失函数主要是指单样本上的差，损失函数累计成为代价函数

### 梯度下降法

梯度下降是常用的算法，不仅被用在线性回归上来最小化线性回归的代价函数J ，还被广泛应用于机器学习的众多领域。

思路梗概：

- 给定theta0 和theta1 的初始值
- 不停的一点点的改变theta0 和theta1 来使J(theta0, theta1) 变小，直到找到J 的最小值或者局部最小值

梯度下降算法定义：重复执行下面的赋值操作直到收敛

![image-20210101202920413](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210101202920413.png)

（其中j = 0 和1）

上述表达式中的alpha 被称为学习速率，用来控制梯度下降时的步长，也就是确定以多大的幅度更新这个参数theta(j)，如果alpha 值很大，梯度下降就很迅速，如果alpha 值很小，梯度下降的速度会很慢；后半部分是一个导数项；对于这个表达式，对于这个更新方程，需要**同时**更新theta0 和theta1，也就是先计算右边的部分，如下

![image-20210101203637076](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210101203637076.png)

梯度下降法默认一个阈值，当变化幅度在这个阈值之内的时候就认为是局部最优，设置这个阈值也是为了避免抖动增加算法复杂性，当然，当上述表达式的右侧导数项接近0或者等于0 的时候被认为是局部最优，如果从结果来反推这个导数项会认为这里存在极大值和极小值的情况，但是从导数项来看，表达式中alpha > 0 ，原theta 减去导数项确保了每次更新theta 参数都是朝着局部极小值变化。

在线性回归模型中应用梯度下降算法时，参数theta 更新的表达式就可以写成：

![image-20210101211053461](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210101211053461.png)

通常我们将上述表达式对应的线性回归算法称为Batch 梯度下降，意味着每一步梯度下降，都遍历了整个训练集的样本（因为在线性回归模型中我们采用了对整个训练集样本的损失函数进行求和推导出代价函数J），所以在每一个单独的梯度下降的操作（更新theta 参数）时，我们最终是计算m 个训练样本的总和。当然也有其他不是Batch 梯度下降的梯度下降算法，它们没有全览整个训练集，每次只关注了小子集。

### 矩阵和向量

矩阵：由数字组成的矩形阵列，并写在方括号内；矩阵的维数用“矩阵的行数 x 列数” 表示

如何表示矩阵的某个特定元素，指的是矩阵的项，也就是矩阵内部的某个数，例如，Aij 表示的是矩阵的第i 行第j 列对应的那个数

向量是一种特殊的矩阵，是只有一列的矩阵，也就是n x 1 矩阵，其中n 指的是行数，所以我们也称这个向量是n 维向量来表示这时一个有n 个元素的向量

我们通常用yi 来表示某一向量y 的第i 个元素，默认使用的是1-下标向量，也就是从1开始计数

只有相同维度的两个矩阵才能相加，就是将两个矩阵的每一个元素都对应的逐个相加，结果将会是一个与相加的两个矩阵维度相同的矩阵；不同维度的矩阵相加是没有意义的，会报错

矩阵和标量的乘法运算中，标量可能是一个复杂的结构代表一个数字或者说是实数，结果就是将矩阵中的所有元素都逐一与标量（实数）相乘，得到的是相同维度的矩阵。标量和矩阵的乘法运算顺序无关，符合交换律

下面介绍矩阵向量乘法，细节如下

![image-20210102110420268](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210102110420268.png)

其中矩阵A 是一个m x n 矩阵，x 表示n 维向量，也就是n x 1 矩阵，注意这里矩阵A 和向量x 的n 是相等的，也就是矩阵A 的列数和向量x 的维数（也可以说是另一个矩阵的行数），最后的结果是一个m 维的向量y，这里的行数m 将等于矩阵A 的行数。

过程简单介绍就是然A 的第i 行元素分别乘以向量x 中的元素并且相加起来，然后将这些相乘的结果加起来，加和的结果就是y 的第i 行元素（可以理解为给向量的各维属性值加权）

接下来介绍矩阵和矩阵的乘法

![image-20210103102631419](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210103102631419.png)

能够相乘的矩阵必须满足一个特征：矩阵的维度相互匹配。也就是第一个矩阵的列的数目必须等于第二矩阵中的行的数目。而且相乘得到的结果会是一个像矩阵C 这样m x o 的矩阵，之前介绍的矩阵向量相乘的运算符合o 等于1 的特殊情况，也就是B 是一个向量。过程可以理解为，要获得矩阵C 的第i 列，要用矩阵A 和矩阵B 的第i 列相乘得到，i 的取值从1 到o。

矩阵和矩阵乘法在线性回归算法的应用基础是，A: 系数矩阵，B: 特征变量矩阵（由假设函数的theta0 和theta1 组成的列向量拼接而成），C: 结果变量矩阵

注意以下几点：

1. 不同于实数乘法运算或者说标量的乘法运算中的交换律，也就是A x B = B x A，矩阵和矩阵的乘法运算中这个交换律是不成立的，矩阵乘法的顺序不能随意颠倒，矩阵乘法是不可交换的
2. 数乘的结合律，事实证明，在矩阵乘法中也成立
3. 单位矩阵（通常记作I ）：当处理实数或标量的时候，数值1 可以看作一个乘法单位，也就是对于任何实数Z，数字1 乘以Z 等于Z 乘以1，它们的结果等于Z。通常我们用I(n x n) 来表示n x n 的单位矩阵，不同的单位矩阵有不同维度n，单位矩阵拥有一个特性即沿对角线上都是1，其他位置都是0。单位矩阵有一个特性，对于任何矩阵A，A 乘以单位矩阵等于单位矩阵乘以A 等于A（结构上和实数与数值1 相乘的结构相同，但是为了保证等式有意义，单位矩阵的维度需要根据矩阵A 的维度进行调整）

### 矩阵逆运算和矩阵的转置运算

矩阵的逆运算的定义：如果A 是一个m x m 的矩阵，假设A 矩阵有其逆矩阵，那么这个逆矩阵可以写成矩阵A^(-1)，矩阵A 乘以它的逆矩阵等于逆矩阵（A^(-1)）乘以A 等于单位矩阵

只有m x m 的矩阵才有其逆矩阵

不存在逆矩阵的矩阵，专有名词是奇异矩阵或者叫做退化矩阵。矩阵的行列式不为0，则可逆；或者说满秩即可逆，也叫非奇异矩阵

矩阵A 的转置矩阵通常记为A^T，假设A 是一个m x n 矩阵，并设矩阵B 等于A 的转置，那么B 是一个n x m 矩阵，维度和A 相反，记为B(ij) = A(ji)，也就是矩阵B 的第i 行第j 列的项等于矩阵A 的第j 行第i 列的项