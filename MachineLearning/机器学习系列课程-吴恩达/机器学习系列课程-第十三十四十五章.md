### 无监督学习

监督学习问题指的是我们有一系列标签然后用假设函数去拟合它；作为对比，在无监督学习中，我们的数据并不带有任何标签。从训练集来看，监督学习的训练集我们通常用$\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\cdots,(x^{(m)},y^{(m)})\}$来表示，在无监督学习中可以写成$\{x^{(1)},x^{(2)},\cdots,x^{(m)}\}$，这些样本没有标签y。

因此在无监督学习中要做的就是将这系列无标签的数据输入到算法中，然后我们要让算法找到一些隐含在数据中的结构（簇）。这就引入了我们的第一个无监督学习算法，聚类算法，它能够找出训练集样本在特征空间中对应的点集形成的簇。当然，数据的结构不单单只有簇，所以无监督学习算法除了聚类算法，还有其他类型的能找出其他类型的结构或者数据的其他类型的模型的算法。

下面我们将介绍一个具体的聚类算法：K-means算法。

假设我们有一个无标签的数据集，并且我们想将其分为两个簇。第一步是随机生成两点，这两点就叫做聚类中心，也就是图上的两个叉

![K均值算法训练集样本示例](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E5%8D%81%E4%B8%89%E5%8D%81%E5%9B%9B%E5%8D%81%E4%BA%94%E7%AB%A0-K%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95%E8%AE%AD%E7%BB%83%E9%9B%86%E6%A0%B7%E6%9C%AC%E7%A4%BA%E4%BE%8B.png)

这里解释下为什么选取两个点，因为我们想将这些数据聚成两类。K 均值算法是一个迭代算法，每次内循环分为两步，第一个是簇分配，第二个是移动聚类中心。

簇分配指的就是遍历每个样本，也就是图上的每个绿点，然后根据每一个点是与红色聚类中心更近还是与蓝色聚类中心更近来将每个数据点分配给两个聚类中心之一；移动聚类中心指的是我们将两个聚类中心移动到簇分配结果的两个簇的点集的均值处（重新赋值），如下图所示

![K均值算法第一步结果示例](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E5%8D%81%E4%B8%89%E5%8D%81%E5%9B%9B%E5%8D%81%E4%BA%94%E7%AB%A0-K%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95%E7%AC%AC%E4%B8%80%E6%AD%A5%E7%BB%93%E6%9E%9C%E7%A4%BA%E4%BE%8B.png)

其中红色的点集和蓝色的点集是簇分配的结果，在移动聚类中心这一步中，我们要做的就是找出所有红色的点，然后算出它们的均值，也就是所有红色点的平均位置，然后把红点的聚类中心移动到这里，蓝色聚类中心也一样，找出所有蓝点并计算它们的均值，然后把蓝色聚类中心移动到那里，下图就是更新后的新的两个聚类中心

![K均值算法第二步结果示例](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E5%8D%81%E4%B8%89%E5%8D%81%E5%9B%9B%E5%8D%81%E4%BA%94%E7%AB%A0-K%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95%E7%AC%AC%E4%BA%8C%E6%AD%A5%E7%BB%93%E6%9E%9C%E7%A4%BA%E4%BE%8B.png)

接着我们进入下一个簇分配步骤，我们再次检查，然后根据这些点与红色还是蓝色聚类中心更近重新分配给一个聚类中心，分成红色和蓝色点集（簇），然后再次移动聚类中心。当我们继续执行K 均值算法的迭代聚类中心也不会再改变并且点的颜色也不会改变了，此时我们可以说，K 均值已经聚合了。该算法在找出数据中两个簇的方面表现的相当好。

下面将介绍K 均值算法的一般形式

K 均值算法接受两个输入，一个是参数K，它表示我们想从数据中聚类出的簇的个数，另外一个输入是一系列无标签的只用特征向量来表示的数据集。同时在非监督学习的K 均值算法里，我们约定所有样本的特征向量是n 维实数向量（不考虑$x_0=1$）

第一步是随机初始化K 个聚类中心，记作$\mu_1,\mu_2,\cdots,\mu_K$，这时K 均值的内循环如下

首先，对于每个训练样本我们用变量$c^{(i)}$来表示第1 到第K 个最接近$x^{(i)}$的聚类中心的索引，也就是说$c^{(i)}$等于$\{1,2,\cdots\,K\}$中的一个值，我们要用第i 个样本$x^{(i)}$计算出这个样本距离每个聚类中心$\mu_k$的距离$\parallel x^{(i)}-\mu_k\parallel$，其中K 表示的是聚类中心的个数，这里的小写的k 则是不同的中心的下标。然后要最小化k 值，找出某个k 值能最小化$x^{(i)}$与聚类中心的距离，将最小化这项的k 值赋值给$c^{(i)}$

K 均值算法内循环中的另一步是移动聚类中心，对于每个聚类中心，也就是对于k 等于1 到K，$\mu_k$就等于这个簇中所有点的均值

在算法的第一步中我们对于训练集中的每个样本都遍历K 个聚类中心计算该样本到某个聚类中心$\mu_k$的距离，将距离最小的聚类中心对应的索引k 赋值给$c^{(i)}$，且两个样本遍历所有聚类中心相互独立，可能存在一个没有点的聚类中心，因为可能对于所有样本对应的点都有其他的聚类中心与它的距离更近。如果真的存在这样的聚类中心，最常见的做法是直接移除那个聚类中心，这样我们最终得到的是K-1 个簇而不是K 个簇。有时我们确实需要有K 个簇，我们就可以重新随机初始化这个聚类中心。

在介绍K 均值算法的优化目标之前，我们对两组变量进行定义，$c^{(i)}$表示的是当前样本$x^{(i)}$所属的哪个簇的索引或者序号，另外一组变量$\mu_k$表示第k 个聚类中心的位置，基于这两个变量，我们用$\mu_{c^{(i)}}$来表示$x^{(i)}$所属的那个簇的聚类中心。通过上述的几组变量的定义，我们就能写出K 均值算法的优化目标：
$$
J(c^{(1)},\cdots,c^{(m)},\mu_1,\cdots,\mu_K)=\frac{1}{m}\sum_{i=1}^m\parallel x^{(i)}-\mu_{c^{(i)}}\parallel^2\\
\underset{c^{(1)},\cdots,c^{(m)},\\\mu_1,\cdots,\mu_K}{min}\,J(c^{(1)},\cdots,c^{(m)},\mu_1,\cdots,\mu_K)
$$
根据K 均值算法的优化目标我们重新来观察算法的两步，第一步簇分配，我们要把每一个点划分给各自所属的聚类中心，同时保持最近的聚类中心$\mu_1$到$\mu_K$的位置固定不变，可以用数学证明，这个簇分配步骤实际上就是在最小化代价函数；然后是K 均值算法的第二步，也就是移动聚类中心，它所作的就是选择$\mu$值来最小化代价函数

总的来说，K 均值算法做的实际上就是把这两个系列的变量分成两部分，第一组是变量c，第二组是变量$\mu$，首先它会最小化代价函数J 关于变量c，接着最小化J 关于变量$\mu$，然后保持迭代

下面我们将讨论如何初始化K 均值聚类算法，以及如何使算法避开局部最优。

有几种不同的方法可以用来随机初始化聚类中心，但是事实证明有一种方法比其他大多数可能考虑到的方法表现的更好。当运行K 均值算法时，我们通常将聚类中心的个数K 设置为比训练集样本容量m 小的值，通常用来初始化K 均值聚类的方法是随机挑选K 个训练样本，然后设定$\mu_1$到$\mu_K$让它们等于这K 个样本，实际上，按照这种方法随机初始化聚类中心至少会有一个点属于簇。

事实上，随机初始化状态不同，K 均值算法最后可能会得到不同的结果。如下图所示，如果随机初始化得到的结果不好，就可能会得到不同的局部最优值，而不是全局最优值

![K均值算法在不同初始状态得到的结果示例](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E5%8D%81%E4%B8%89%E5%8D%81%E5%9B%9B%E5%8D%81%E4%BA%94%E7%AB%A0-K%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95%E5%9C%A8%E4%B8%8D%E5%90%8C%E5%88%9D%E5%A7%8B%E7%8A%B6%E6%80%81%E5%BE%97%E5%88%B0%E7%9A%84%E7%BB%93%E6%9E%9C%E7%A4%BA%E4%BE%8B.png)

局部最优值这个术语指的是代价函数J 的局部最优，像上面三个图中下方的这些局部最优所对应的情况其实是K 均值算法落在了局部最优，因而不能很好的最小化代价函数。如果我们想让K 均值算法找到最有可能的聚类结果就像上面三幅图中上方那副展示的，我们可以尝试多次随机初始化，不是仅仅初始化一次，并运行K 均值算法很多次，在所有这么多次运行得到的聚类中心和分配结果中，选取代价函数最小的一个，以此来确保我们最终能得到一个足够好的结果，一个尽可能好的局部或全局最优值，典型的运行次数一般在50到1000之间。

事实证明，如果运行K 均值算法时所用的聚类数相当小，比如在2 到10 的话，那么多次随机初始化通常能够保证找到较好的局部最优解，保证找到最好的聚类，但是如果K 非常大的话，比如K 比10 大很多，那么多次随机初始化就不会有太大改善，更有可能第一次随机初始化就会给出相当好的结果，后面多次随机初始化可能会得到稍微好一点的结果，但是不会好太多，但是在聚类数相对较小的情况下，随机初始化会有较大的影响，可以保证很好的最小化代价函数并且能得到一个很好的聚类结果

下面介绍如何选择聚类数量，或者说如何选择参数K 的值。

当人们在讨论选择聚类数量的方法时，可能会谈到一个方法叫做“肘部法则”，在该法则中，我们所要做的是改变K，也就是聚类总数，我们先用一个类来聚类，这就意味着所有的数据都会分到一个类里，然后计算代价函数，然后我们再用两个类来跑K 均值算法，可能多次随机初始化，也可能只初始化一次，但是在K = 2的情况下，一般代价函数的局部最优解会小一些，再用三个类来跑K 均值算法，很可能得到一个更小的代价函数的局部最优解，然后再用4类，5类等等来跑K 均值算法，最后我们会得到一条随着聚类数量K 的增多代价函数的局部最优值下降的曲线，如下图所示

![K均值算法关于簇的数量的学习曲线](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E5%8D%81%E4%B8%89%E5%8D%81%E5%9B%9B%E5%8D%81%E4%BA%94%E7%AB%A0-K%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95%E5%85%B3%E4%BA%8E%E7%B0%87%E7%9A%84%E6%95%B0%E9%87%8F%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF.png)

根据这幅图，“肘部法则”就是观察这个图，会发现从1 到2，从2 到3 代价函数的最优值会迅速下降，到了某一个K 值对应的点（我们称为“肘部”）时，在此之后，代价函数的最优值就下降的非常慢。通常我们就会选择这个K 值，因为这个K 值对应的点相当于这个曲线的拐点。

但是在实际问题中“肘部法则”并不那么常用，原因是在实际运用到聚类问题上时，往往最后我们会得到一条看上去相当模糊的曲线，也就是没有一个清晰的拐点，看上去最优值是连续下降的，光滑的，无法准确确定拐点合适的位置。

还有另外一种选择K 值的思路，就是将我们想用K 均值算法的目的作为一个评估标准，也就是说决定聚类的更好的方式是看哪个聚类数量能更好的应用于后续目的，比如将数据集分成多少类有利于我们之后的使用或者分析，来决定我们的K 值。

### 降维，数据压缩

数据压缩不仅能让我们对数据进行压缩，使得数据占用较少的内存或硬盘空间，它还能让我们对学习算法进行加速。

降维是什么。如果特征高度相关，那么我们可能真的需要降低维数，消除（近似忽略）某个特征的影响。具体操作就是，对于n个特征$x^{(i)}_1,x^{(i)}_2,\cdots,x^{(i)}_n$，如果要消除特征$x^{(i)}_j$，将样本投影到由新特征向量$\left[\begin{matrix}z^{(i)}_1\\z^{(i)}_2\\\vdots\\z^{(i)}_{j-1}\\0\\z^{(i)}_{j+1}\\\vdots\\z^{(i)}_n\end{matrix}\right]\in R^n$表示的新特征空间中，因为$x_j$已经固定为0了，所以可以消除，对于样本集的所有样本$x^{(i)}$的特征值$x^{(i)}_j$，这个值越接近于0，降维带来的影响就越小，换言之，转换到新特征空间对原数据集的影响越小。关于如何找到这个影响很小的特征$x_j$，我的想法是从模型学习到的假设函数的参数向量$\theta$来看，作为权重，当$\theta_j$越小，说明它对应的特征对假设函数的输出值，也是模型的预测结果影响越小。

其实这种**特征空间变换相当于是在原特征向量基础上与某一个相同维度的向量内积**，感觉还可以做一个延伸，在原特征空间中，我们任意选择k 个特征$x^{(i)}_{j+1},x^{(i)}_{j+2},\cdots,x^{(i)}_{j+k}$，考虑用新的特征向量$z\in R^{k-1}$来表示这k 个特征，选择消除一个特征，与之前的过程相似，但是新特征完全与原特征空间无关，用$z\in R^{k-1}$表示，在原特征空间中可以看作新特征$z$在原特征空间的新的坐标轴，原样本$x^{(i)}=\left[\begin{matrix}x^{(i)}_1\\x^{(i)}_2\\\vdots\\x^{(i)}_j\\\vdots\\x^{(i)}_k\\\vdots\\x^{(i)}_n\end{matrix}\right]\in R^{n}$转换为$x^{(i)}=\left[\begin{matrix}x^{(i)}_1\\x^{(i)}_2\\\vdots\\z^{(i)}\\\vdots\\x^{(i)}_n\end{matrix}\right]\in R^{n-1}$

这种降维能让学习算法运行的更快，同时这也是数据压缩中减少内存使用以及磁盘空间需求的一种应用

降维的应用除了数据压缩，它的第二个应用是可视化数据。通常对于多维特征的数据，我们无法很好的可视化来观察，那么通过降维我们可以获得每个样本对应的二维数据，这样我们就能只用一对数字来概述每个n 维特征向量代表的样本的n 个特征值，然后把这些样本在二维平面上表示出来。当我们来看降维的输出时，新特征空间的特征往往是不具备物理意义的，如果把这些特征画出来，我们就可以在图中找到规律，如下图

![多维数据通过数据降维实现可视化的示例](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E5%8D%81%E4%B8%89%E5%8D%81%E5%9B%9B%E5%8D%81%E4%BA%94%E7%AB%A0-%E5%A4%9A%E7%BB%B4%E6%95%B0%E6%8D%AE%E9%80%9A%E8%BF%87%E6%95%B0%E6%8D%AE%E9%99%8D%E7%BB%B4%E5%AE%9E%E7%8E%B0%E5%8F%AF%E8%A7%86%E5%8C%96%E7%9A%84%E7%A4%BA%E4%BE%8B.png)

可以发现，这些点都代表无标签的样本，符合无监督学习算法，这也是聚类经常作用于降维的原因。通过降维到二维，二维图能帮助我们最便捷的捕捉到不同样本之间在特征上的区别。当然，除了二维图，我们也能画出三维图并更好的理解这些数据。

### 主成分分析方法（PCA）

PCA 所做的就是试图寻找一个投影平面对数据进行投影，使得能最小化投影距离。用具体示例解释就是，如果想将数据从二维降到一维，我们要试着找一个向量，假设是向量$u^{(i)}\in R^n$，在这个例子中n = 2，我们要找一个数据投影到对这个向量进行延展的直线上后能够最小化投影误差的方向，用这个向量$u^{(i)}$来表示这个方向，最后我们会得到非常小的重构误差。

更通常的情况是我们会有N 维的数据，并且我们想将其降到K 维，这种情况下，我们不只是想找单个向量来对数据进行投影，而是想找K 个方向来对数据进行投影来最小化投影误差，用数学术语来解释就是，我们要找出一组向量$u^{(1)},u^{(2)},\cdots,u^{(k)}$，将这些数据投影到这k 个向量展开的线性子空间上

这里来解释一下PCA 和线性回归模型的区别，同样会拟合出一个多维“平面”$\theta^Tx$，线性回归模型中我们要做的是拟合出来假设函数来最小化每个训练集样本的标签和该假设函数的预测值之间的平方误差，两者间的差值是与假设函数的输出值y 所在的坐标轴平行的线段长度；不同的是在PCA 中，它要做的是试图最小化投影距离的长度，投影距离是垂直于拟合出来的多维“平面”的，是求最短的正交距离

在使用PCA 之前，首先要做的就是进行数据预处理，一般是两个过程，均值标准化和特征缩放。对于均值标准化，我们首先计算每个特征的均值$\mu_j=\frac{1}{m}\sum_{i=1}^mx_j^{(i)}$，我们用它减去它的均值的结果$x_j-\mu_j$来取代每个特征的特征值$x_J^{(i)}$，这将使得新的特征的均值正好为0，也就是$\frac{1}{n}\sum_{j=1}^n(x_j^{(i)}-\mu_j)=0$。接下来，如果不同的特征有相差非常大的取值范围，我们也可以缩放每个特征，也称为归一化，在无监督学习中，比监督学习多了一步均值标准化，所以第j 个特征$x_j^{(i)}$将会由$\frac{x_J^{(i)}-\mu_j}{s_j}$取代，这里的$s_j$表示的是特征$x_j$的一些测量值，通常会用样本集中它的最大值减去最小值，称为特征的标准偏差

下面详细介绍PCA 的过程。我们想把n 维的数据降到k 维度，首先要做的是计算协方差，协方差的计算表达式为$\Sigma=\frac{1}{m}\sum_{i=1}^m(x^{(i)})(x^{(i)})^T$，最后得到的$\Sigma$是一个n*n 的矩阵，假定$X=\left[\begin{matrix}(x^{(1)})^T\\(x^{(2)})^T\\\vdots\\(x^{(m)})^T\end{matrix}\right]$，其中$x^{(j)}$表示的是样本的特征向量，那么它的向量化实现可以写成$\Sigma=\frac{1}{m} * X^T * X$，将该计算结果作为输入调用奇异值分解的库函数，得到三个矩阵分别用U, S, V 表示，在PCA 中我们要用到的就是矩阵U，矩阵U 是一个n * n 的矩阵，表示为$\left[u^{(1)}\:u^{(2)}\:\cdots\:u^{(n)}\right]$，其中$u^{(j)}$是n * 1的列向量，如果我们想将数据的维数从n 维降到k 维，那么我们需要做的就是提取前k 个向量$u^{(1)},u^{(2)},\cdots,u^{(k)}$，而这k 个列向量对应我们新特征空间的k 维（也就是k 个方向），而值得说明的是，奇异值分解得到的矩阵U 实际上是由n 个两两正交的特征向量组成的，且奇异值是从大到小排列的，所以选择前k 个就代表了选择了影响最大的k 个基。

最后，降维的过程，也就是将原数据的特征向量$x\in R^n$转换为$z\in R^k$的过程就是
$$
z=[u^{(1)}\:u^{(2)}\:\cdots\:u^{(k)}]^Tx\\
U_{reduce}=[u^{(1)}\:u^{(2)}\:\cdots\:u^{(k)}]\\
z=(U_{reduce})^Tx
$$
其中$u^{(j)}\in R^n$是n 维列向量，那么可以推导得到新向量$z\in R^k$，也正是我们想要的k 维向量

那么如何选择降维的目标维数k 呢？这里引入两个术语：平均平方投影误差和数据方差
$$
Average\,squared\,projection\,error=\frac{1}{m}\sum_{i=1}^m\parallel x^{(i)}-x_{approx}^{(i)}\parallel^2\\
Total\,variation=\frac{1}{m}\sum_{i=1}^m\parallel x^{(i)}\parallel^2
$$
其中$x^{(i)}_{approx}$表示的是低维子空间上样本$x^{(i)}$的投影，根据向量的差的定义，$\parallel x^{(i)}-x_{approx}^{(i)}\parallel$表示的就是投影距离，另外，$\parallel x^{(i)}\parallel$表示样本到原点$(0,0)$的距离。

当我们要选择k 时，通常是选择使得下面这个不等式成立的最小的k：
$$
\frac{\frac{1}{m}\sum_{i=1}^m\parallel x^{(i)}-x_{approx}^{(i)}\parallel^2}{\frac{1}{m}\sum_{i=1}^m\parallel x^{(i)}\parallel^2}\leq0.01
$$
根据这个不等式，另一种PCA 的说法就是99%的方差被保留下来了，之前解释过，降维过程的投影实际上就是放弃某些特征，而投影误差我们可以理解为放弃的特征值的测量值。除了0.01，人们还是用其他的一些数字比如0.05，表示5%，而保留了95%的方差以及0.1等等。因为很多数据都存在相关性很高的特征，所以降维依然能保留95%到99% 的特性（或者特征，这里没有字幕，没有听得很清楚）

具体实现方式就是，假定k = 1，计算$U_{reduce}$以及新特征空间下的样本$z^{(1)},z^{(2)},\cdots,z^{(m)}$，和样本投影到低维子空间的新向量$x^{(1)}_{approx},x^{(2)}_{approx},\cdots,x^{(m)}_{approx}$，需要说明的是新特征空间中的样本$z^{(i)}\in R^k$是k 维列向量，而低维子空间中的新向量$x^{(i)}_{approx}\in R^n$是n 维列向量，然后代入到分式中检查不等式是否成立，如果不满足就尝试k = 2，如此继续，直到某个k 的值不等式成立，我们就用该值作为PCA 的目标维数。

这里提到我们用奇异值分解的库函数得到的三个矩阵U, S, V 中的矩阵S，矩阵S 是一个n * n 的方阵，也是一个对角矩阵，写成$S=\left[\begin{matrix}s_{11}\:0\:0\:\cdots\:0\\0\:s_{22}\:0\:\cdots\:0\\\:\:\ddots\\0\:0\:0\:\cdots\:s_{nn}\end{matrix}\right]$，上面的不等式就可以写成
$$
1-\frac{\sum_{i=1}^ks_{ii}}{\sum_{i=1}^ns_{ii}}\leq0.01\\
\frac{\sum_{i=1}^ks_{ii}}{\sum_{i=1}^ns_{ii}}\geq0.99
$$
如果使用这种方法来选择k，我们就不需要每次增加k 的时候都重复调用奇异值分解的库函数来获取$U_{reduce}$以及计算对应的$x_{approx}$

下面将介绍如何从新特征空间的特征向量$z^{(i)}$转换回到原来的表示$x^{(i)}$，这将解释上面的不等式中我们该如何获得$x_{approx}^{(i)}$

根据之前的定义，新旧特征空间的转换可以用如下表达式实现
$$
z=U_{reduce}^Tx
$$
那么反过来我们要推导原特征空间的$x$的话，上面的方程就改成
$$
x_{approx}=(U_{reduce})^{-1}z=U_{reduce}z
$$
这里涉及到正交矩阵，也就是我们通过调用奇异值分解的库函数得到的矩阵U 的一个性质，即正交矩阵M 满足$M^TM=I$，也就是说对于正交矩阵，其逆矩阵等于转置矩阵。

当然，因为$z^{(i)}$在原特征空间中是拟合出来的低维子空间，所以通过上面的表达式转换得到的$x^{(i)}_{approx}$在原特征空间中也满足低维子空间的性质，降维过程中的投影误差已经丢失，所以这样获得的$x_{approx}$是原来数据$x$不错的近似，有损失，但不是等量转换。我们也把这个过程称为“原始数据的重构”

在应用PCA 的监督学习算法中，PCA 要做的是定义一个从x 到z 的映射，这个x 到z 的映射只能通过在训练集上运行PCA 来定义，具体来说，PCA 学习的这个映射所做的就是计算一系列特征向量进行特征缩放和均值归一化，还计算矩阵$U_{reduce}$，但所有这些东西，比如$U_{reduce}$，都是通过学习PCA 得到的参数，这个学习过程只发生在训练集上，所以我们应该只在训练集上拟合这些参数而不是在交叉验证集或者在测试集上。$U_{reduce}$这些值应该是根据训练集样本运行PCA 得到的，不能直接用于别的样本，然后找到$U_{reduce}$或者找到参数来进行特征缩放或均值归一化以及划分特征的合适的缩放规模。在训练集上找到所有这些参数后，我们就可以把这个映射用在交叉验证集或测试集的其他样本中

总结一下，当在运行PCA 时，仅仅在训练集中的数据上运行，不能用在交叉验证集和测试集数据，定义了x 到z 的映射后，我们就可以应用这个映射到交叉验证集或测试集。换个角度理解，对于训练集样本来说，交叉验证集和测试集样本都是新样本，分别用于选择合适的模型参数和测试泛化能力，如果在交叉验证集或测试集上对样本重新学习得到一系列参数以及$U_{reduce}$，那么对于交叉验证集来说就多了一个自变量参数可能会出现过拟合问题，影响到待选择的参数不同时每个模型的预测的表现，对于测试集来说，我们应该以从训练集上学习到的模型经由交叉验证集选择后的模型做测试，而不同的参数（比如根据测试集获得的矩阵$U_{reduce}$是不同于训练集和交叉验证集经PCA 获得的$U_{reduce}$的）对应不同的模型导致我们无法评估这个学习算法获得的模型的泛化能力

有一种错误的想法是认为“PCA 是一种防止过拟合的方法，因为我们能通过PCA 获得比原样本小得多的特征数量”，但这时PCA 错误的应用，并且不建议这样做，通常我们还是会通过使用正则化而不是使用PCA 来减少数据维度防止过拟合，显然，前者是一个非常好的方法。解释一下原因的话，我们可以回顾PCA 在监督学习中的工作，它不需要使用标签y，仅仅是使用输入的$x^{(i)}$，使用它去寻找低维数据来近似我们的数据，所以PCA 会舍掉一些信息，它扔掉或减少数据的维度并且不关心y 的值是什么，所以如果99% 的方差信息被保留或者说保留了大部分方差（每个特征的投影误差），这时使用PCA 的效果是可以的，但是它也可能会丢掉一些有价值的信息。事实证明，当我们保留了99%或95%的方差信息，只使用正则化常常会带来至少一样好的效果，用正则化来防止过拟合的效果通常会更好，因为它通常用在线性回归，逻辑回归模型或其他一些方法时，这些方法的优化目标是需要样本的标签值y 的，所以不太可能损失掉一些有价值的信息。

### 异常检测

异常检测过程中，我们要做的是给定无标签的训练集，对数据建模p(x)，也就是说我们将对x 的分布概率建模，其中x 是样本对应的特征向量，当我们建立了x 的概率模型之后，对于新的样本$x_{test}$，如果$p(x_{test})$低于阈值$\varepsilon$，那么就将其标记为异常，反之，如果新的样本$x_{test}$对应的概率p 大于等于给定的阈值$\varepsilon$，我们就认为它是正常的。

下面我们来介绍一下高斯分布，也称为正态分布。

假设x 是一个实数的随机变量，如果x 的概率分布服从高斯分布，其中均值为$\mu$，方差为$\sigma^2$，那么将它记作$X\sim N(\mu,\sigma^2)$，其中$\sim$表示变量“服从....分布”，大写N 表示正态分布。高斯分布有两个参数，一个是均值我们记作$\mu$，另一个是方差我们记作$\sigma^2$。高斯分布的概率密度函数绘制结果类似一个钟形曲线，是一个对称曲线，如下图

![高斯分布概率密度函数图像示例](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E5%8D%81%E4%B8%89%E5%8D%81%E5%9B%9B%E5%8D%81%E4%BA%94%E7%AB%A0-%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F%E7%A4%BA%E4%BE%8B.png)

这个钟形曲线有两个参数分别是$\mu$和$\sigma$，其中$\mu$控制这个钟形曲线的中心位置，$\sigma$控制这个钟形曲线的宽度，我们也称它为标准差。这条钟形曲线决定了x 取不同值时的概率。高斯分布的数学公式，即x 的概率分布，可以表示为$p(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})$

正态分布在计算概率密度的时候，是根据距离中心值（平均值）的距离，然后求出对应图像的面积即对应的概率。因此，既然是求得面积后得出对应的概率值。当我们输入x，也就会得到与中心值（平均值）的距离，也就是宽，那么也需要高，这里的高就是正态分布曲线存在高的意义。从统计学的角度来解释，在正态分布的特性中，距离若干个$\sigma$范围内概率是固定值，如下图是在若干倍的$\sigma$范围内的概率分布

![高斯分布关于标准差的性质](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E5%8D%81%E4%B8%89%E5%8D%81%E5%9B%9B%E5%8D%81%E4%BA%94%E7%AB%A0-%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E5%85%B3%E4%BA%8E%E6%A0%87%E5%87%86%E5%B7%AE%E7%9A%84%E6%80%A7%E8%B4%A8.png)

由于概率是固定值，那么对于样本集改变标准差$\sigma$，曲线的最高点的高度也会改变，这也能从侧面反映正态分布图像的高的意义。

下面我们将讨论参数估计的问题，那么什么是参数估计问题？

假设我们有一个数据集，其中有m 个样本$\{x^{(1)},x^{(2)},\cdots,x^{(m)}\},x^{(i)}\in R$并且每个样本都是实数，现在假设每一个样本$x^{(i)}$服从某个分布，例如$x^{(i)}\sim N(\mu,\sigma^2)$，但是我们不知道服从的分布的参数的值是多少。正态分布的参数估计问题就是给定数据集，找到能够估算出$\mu$和$\sigma^2$的值，这里写一下估计$\mu$和$\sigma^2$参数的标准公式：
$$
\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)}\\
\sigma^2=\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)^2
$$
这里方差$\sigma^2$的定义是所有样本与样本均值的差值平方的均值，样本均值就是$\mu$，也是分布的均值，是正态分布的曲线的中心位置

下面我们将来构建一个异常检测算法。假设我们有一个共有m 个样本的无标签训练集，训练集里的每个样本都是一个n 维的特征向量，表示为$\{x^{(1)},\cdots,x^{(m)}\}\in R^n$，我们处理异常检测的方法是用数据集建立起概率模型p(x)，来计算出哪些特征量出现的概率比较高，哪些特征量的出现概率比较低，因此x 作为一个由n 个特征量组成的向量，这样p(x) 可以表示为x 的第一个特征量$x_1$出现的概率乘以$x_2$的概率直到最后一个特征量$x_n$，表达式写成$p(x^{(i)})=p(x^{(i)}_1)p(x^{(i)}_2)\cdots p(x^{(i)}_n)$，接下来我们就要对这些特征量建模，假定特征$x_1$是服从高斯正态分布的，它存在期望为$\mu_1$以及方差$(\sigma_1)^2$，这样$p(x_1)$的模型就可以用正态分布的概率密度函数表示，因为$x_1\sim N(\mu_1,\sigma_1^2)$，就有$p(x_1)=p(x_1;\mu_1,\sigma_1^2)$，所以关于样本$x^{(i)}$的模型的表达式就可以写成
$$
p(x^{(i)})=p(x^{(i)}_1;\mu_1,\sigma_1^2)p(x^{(i)}_2;\mu_2,\sigma_2^2)\cdots p(x^{(i)}_n;\mu_n,\sigma_n^2)
$$
这个等式其实等同于一个特征$x_1$到$x_n$上的独立假设

综上所述，整理一下，这就是我们的异常检测算法：

第一步是选择特征量或是想出一些特征量$x_i$来帮助我们评估哪些样本属于反常，这些特征在样本出现异常时其特征值会异常的非常大或者异常的特别小，但通常来说，还是选择那些能够描述我们数据集中样本的一般特性的特征；下一步是给出训练集，也就是m 个未作标记的样本$\{x^{(1)},\cdots,x^{(m)}\}\in R^n$，之后我们进行参数拟合，获得相应的n 个特征量在该训练集中的均值$\mu_1,\cdots,\mu_n$以及方差$\sigma_1^2,\sigma_2^2,\cdots,\sigma_n^2$，这个过程就是之前提到的参数估计，下面是均值和方差的计算表达式：
$$
\mu_j=\frac{1}{m}\sum_{i=1}^mx_j^{(i)}\\
\sigma_j^2=\frac{1}{m}\sum_{i=1}^m(x_j^{(i)}-\mu_j)^2
$$
$\mu_j$作为特征量$x_j$的平均值，上面的计算表达式向量化可以写成：
$$
\mu=\left[\begin{matrix}\mu_1\\\mu_2\\\vdots\\\mu_n\end{matrix}\right]=\frac{1}{m}\sum_{i=1}^mx^{(i)}
$$
最后一步，我们将计算得到某个新样本x 在概率模型p(x) 的输出值，计算表达式如下：
$$
p(x)=\prod_{j=1}^np(x_j;\mu_j,\sigma_j^2)=\prod_{j=1}^n\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})
$$
如果计算结果也就是这个概率值很小，比如小于某个阈值$p(x)<\varepsilon$，那么就将这一项标注为异常

### 异常检测算法的评估

假设我们有一些带标签的数据，这些标签用来指明哪些是异常样本哪些是正常样本，并规定y = 0 表示无异常样本或称为正常样本，y = 1 代表异常样本。再假设我们有训练集$\{x^{(1)},x^{(2)},\cdots,x^{(m)}\}$并将该训练集是无标签的，我们通常将它看作是一个容量很大的正常样本的或者说无异常样本的集合（即使其中包含了一些实际上是异常样本也没关系）。接下来我们定义一个交叉验证集和测试集来评估异常检测算法$\{(x_{cv}^{(1)},y_{cv}^{(1)}),\cdots,(x_{cv}^{(m_{cv})},y_{cv}^{(m_{cv})})\},\{(x_{test}^{(1)},y_{test}^{(1)}),\cdots,(x_{test}^{(m_{test})},y_{test}^{(m_{test})})\}$，并且通常我们假设在交叉验证集和测试集中包含一些已知是异常的样本，所以在交叉验证集和测试集中我们会有一些标签是y = 1 的样本。

下面是如何推导和评估算法。

首先，我们使用训练集拟合模型p(x)，也就是把m 个无标签样本都用高斯函数来拟合，获得样本的每个特征量$x_j$关于高斯正态分布的参数$\mu_j$和$\sigma_j^2$，然后构建关于样本x 的模型$p(x)=p(x_1;\mu_1,\sigma_1^2)\cdots p(x_n;\mu_n,\sigma_n^2)$。然后在交叉验证集和测试集中，对于样本x 有
$$
y=\begin{cases}
1&if\:p(x)<\varepsilon\\
0&if\:p(x)\geq\varepsilon
\end{cases}
$$
规定y = 0 代表样本正常，无异常；y = 1 代表异常样本。

评估部分如下，

用无标签训练集的数据拟合模型p(x) 分别预测交叉验证集和测试集的样本。和我们在监督学习中相同的一点是，我们的测试集也是带标签的并且我们学习得到的监督算法要对测试集的样本进行预测，然后通过算法预测成功的次数来对算法进行评估。需要注意的是，这些标签会很倾斜，类似于偏斜类的分类问题，因为y = 0 的情况，也就是正常样本会比y = 1 的异常样本常见的多，所以，对于异常检测算法的评估指标，我们引入查准率和召回率来表示算法的精度，或者计算真阳性，假阳性，假阴性以及真阴性的比例，也可以通过计算单一评估度量F 值来总结和反映精度。

最后，异常检测算法中阈值的选择也是由我们手动设置的，所以，如果我们有一个交叉验证集，一个选择参数$\varepsilon$的方法就是尝试去使用许多不同的$\varepsilon$的值，然后从中选择一个$\varepsilon$ 的值，使得F 值最大化或者在其他方面有良好表现

### 异常检测 vs 监督学习

如果问题中的正例（即y = 1）数量很少，那么我们可以考虑使用异常检测算法；另一方面，在一个典型的异常检测中，我们经常会有一个相比正例样本数量更大的负例样本，那么我们可以用这庞大数量的负样本来拟合出p(x) 的值。因此在许多异常检测应用中，有这样一个思想，假设有很少的正例样本和很多负例样本时，当我们在拟合所有的高斯参数来估计p(x) 的值的过程中，我们只需要负例样本就够了，所以如果有大量的负例样本，我们仍然可以很好的拟合p(x) 的值

与此相反，对于监督学习来说，更为典型的情况是，在合理范围内会有大量的正例样本和负例样本。

使用异常检测算法的过程中需要注意的一点是，对于异常检测应用来说，经常有许多不同类型的异常，因此，我们只有很少数量的正例样本，那么对于一个算法就很难去从小数量的正例样本中去学习异常是什么，尤其是未来可能出现的异常会与已知的或者显而易见的异常**截然不同**。对于这种情况，我们通常会选择对具有庞大数量的负例样本用高斯分布模型p(x) 来建模，而不是费尽心机的对正例样本建模。

在另一种相反的情况中，假设我们有足够数量的正例样本或是一个已经能识别正例样本的算法，尤其是假如认为未来可能出现的正例样本与当前训练集中的正例样本**类似**，那么使用一个监督学习算法会更合理，它能够观察大量正例样本和大量负例样本来学习相应的参数从而能够尝试区分正例和负例

### 选择合适的异常检测特征

当我们应用异常检测时，使用什么特征或者说选择什么特征来实现异常检测算法对运行有很大影响。下面将会介绍如何设计或选择异常检测算法的特征。

关于样本数据的分布，这里还涉及对数变换，幂变换等，这里详细介绍下对数变换。在此之前，声明一个可能以前都没有注意过的问题，方差和标准差的区别：

在概率论和统计方差是衡量随机变量或一组数据时离散程度的度量。概率论中方差用来度量随机变量和其数学期望（即均值）之间的偏离程度。统计学中的方差（样本方差）是各个样本数据和平均数之差的平方和的平均数。

对于一组随机变量或者统计数据，其期望值（平均数）用E(X) 表示，即随机变量或统计数据的均值，然后对各个数据与均值的差的平方和，表示为$\sum[X-E(X)]^2$，最后对平方和再求期望就得到了方差公式，如下：
$$
D(x)=E\{\sum[X-E(X)]^2\}
$$
标准差是方差的平方根，公式如下，其中$\mu$表示期望：
$$
\sigma=\sqrt{\frac{1}{N}\sum_{i=1}^N(x_i-\mu)^2}
$$
根号里的内容就是我们刚提到的方差。不难发现，方差和我们要处理的数据的**量纲**是不一致的，虽然能很好的描述数据与均值的偏离程度，但是处理结果是不符合我们的直观思维的。相比较于方差，我们常在概率密度函数中使用标准差来划分区域对应的概率，例如之前提到的正态分布的概率密度曲线。

再引入两个定义，均方误差是各数据偏离**真实值**差值的平方和的平均数，也就是误差平方和的平均数，均方误差的开方叫均方根误差，均方根误差和标准差形式上接近，但注意，不是相同，差别就在于前者与真实值相关，后者标准差是样本（数据集合）的平均值。均方差，也就是标准差。

举个例子，测量房间里的温度，测量5次的结果为$[x_1,x_2,x_3,x_4,x_5]$，假设房间里温度的真实值是$x_0$，数据样本与真实值的误差为$e_i=x_0-x_i$，那么均方误差可以表示为$\frac{\sum e_i^2}{n}$，其中n 表示数据样本的数量，例子中n = 5。

总的来说，方差是数据序列和均值（均值是由数据序列求得的）的关系，而均方误差是数据序列与真实值之间的关系

下面将通过抽象化的模型来解释对数变换后的数据为什么能近似为正态分布

假定一个模型下，分布的标准差与均值呈线性相关的关系，因此我们有$\sqrt{Var)Z_t}=\mu_t*k$，期望记为$E(Z_t)=\mu_t$

以下为推导过程：
$$
\because Z_t=Z_t\\
\therefore Z_t=\mu_t(1+\frac{Z_t-\mu_t}{\mu_t})（左右两边相等）\\
\because log(1+x)\approx x（微积分知识）\\
\therefore log(Z_t)\approx log(\mu_t)+\frac{Z_t-\mu_t}{\mu_t}（用上式的原理）\\
\because E(\frac{Z_t-\mu_t}{\mu_t})=\frac{1}{\mu_t}E(Z_t-\mu_t)=\frac{1}{\mu_t}(\mu_t-\mu_t)=0（也就是上式中的第三项的均值为常数）\\
\therefore E(log(Z_t))\approx log(\mu_t)（*原数据做对数变换后的均值）\\
\because Var(log(Z_t))=E[log(Z_t)-E(log(Z_t))]^2（方差的定义）\\
=E[log(Z_t)-log(\mu_t)]^2\\
\approx E(\frac{Z_t-\mu_t}{\mu_t})^2\\
=\frac{1}{(\mu_t)^2}E(Z_t-E(Z_t))^2\\
=\frac{1}{(\mu_t)^2}Var(Z_t)（注意上面就是方差的公式）\\
=\frac{1}{(\mu_t)^2}*(\mu_t)^2*k^2（方差Var(Z_t)在模型中的定义，见之前假设部分）\\
\approx k^2
\therefore Var(log(Z_t))\approx k^2（*原数据对数变换后的方差）
$$
综上，我们推导出$E(log(Z_t))\approx log(\mu_t)$以及$Var(log(Z_t))\approx k^2$，我们可以看到，原数据的方差由开始时随着均值变化而变化，取对数后便与均值无关，近似成为了一个常量，这就符合正态分布。

当然，对于$X\rightarrow X^{\frac{1}{k}}$的幂变换，按照上面的推导过程，取对数进行等价变换后会发现过程几乎相似，所以，幂变换也能将某些不规则分布的数据变换成服从正态分布。

将原数据转换成服从正态分布后，下面将介绍一个误差分析的步骤来选择异常检测算法的特征。我们会先完整的训练出一个算法，然后在一组交叉验证集上运行算法，找出那些预测出错的样本，并看看我们能否找到一些其他特征来帮助学习算法来让那些在交叉验证集中判断出错的样本表现的更好。降维是为了程序运行的效率，升维是为了更多的信息以保证模型的准确度，所以我们增加的特征在异常检测拟合出的模型p(x) 中输出值足够小从而帮助模型判定这个样本是异常的，也就是让检测模型更加准确。**通常在异常检测过程中，我们选择的特征在异常样本中要么特别大，要么特别小**，新增的特征还可以通过原特征通过数学计算得到，可能不具备物理意义。

### 多元高斯分布

假设我们有特征变量$x\in R^n$，不同于之前异常检测算法中分别对x 的每个特征量的分布建模，而是要建立一个整体的p(x) 模型。多元高斯分布的参数分别有$\mu\in R^n$以及$\Sigma\in R^{n*n}$，$\Sigma$又被称为协方差矩阵。

在之前的异常检测算法中建模p(x) 基于独立假设，即假设各特征是独立同分布的且特征之间互不影响，有$p(x)=p(x_1;\mu_1,\sigma_1^2)p(x_2;\mu_2,\sigma_2^2)\cdots p(x_n;\mu_n,\sigma_n^2)$，而多元高斯分布的思想是基于特征之间或多或少存在相关性，而考虑整体建模，用到了协方差矩阵。

这里提一下协方差矩阵本身的含义，即随机变量之间的线性相关关系，所以称其为相关系数矩阵，同时协方差矩阵是半正定矩阵。作为实对称矩阵，其主要性质之一就是可以正交对角化，即存在正交矩阵U，使得协方差矩阵$\Sigma$满足$U^T\Sigma U=A$，其中A 是对角线元素都非负的对角矩阵。作为半正定矩阵，我们可以对协方差矩阵进行Cholesky 分解：半正定矩阵$\Sigma$可以分解为$\Sigma=U^TAU$，其中U 是上三角矩阵，A 是对角线元素都非负的对角矩阵。所以有$\Sigma=U^TAU=[U^TA^{\frac{1}{2}}][A^{\frac{1}{2}}U]=[A^{\frac{1}{2}}U]^T[A^{\frac{1}{2}}U]$，这样一来，矩阵$\Sigma=C^TC$，其中$C=A^{\frac{1}{2}}U$。

回到主题上，对于特征向量x 的整体建模有
$$
p(x;\mu,\Sigma)=\frac{1}{\sqrt{(2\pi)^n}|\Sigma|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))
$$
其中$|\Sigma|$表示$\Sigma$的行列式，是矩阵的一个数学函数

要计算协方差，对于特征向量$x^{(i)}$，其协方差的计算表达式为$\Sigma=\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T$，其中$\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)}$表示均值，计算结果为一个n * n的矩阵

多元高斯分布最重要的优势就是能够让我们描述两个特征变量之间可能存在正相关或者负相关的情况。

接下来我们来讨论参数的拟合或者说是参数估计问题。假如我们有一组样本$\{x^{(1)},x^{(2)},\cdots,x^{(m)}\},\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)},\Sigma=\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T$，其中$x\in R^n$，当我们有一个新样本x 即一个测试样本时，我们需要用这个多元高斯分布的公式来计算p(x)，如果得到p(x)  的值很小，小于某个阈值，就标记该样本为异常，相反，如果p(x) 大于阈值，就不进行异常标记

### 原始模型和多变量高斯分布

原始模型对比多元高斯模型来说，在二维特征向量下的图像的等高线（一般情况称为“轮廓”）总是轴对齐的，实际上，对于多元高斯分布，二维特征向量下的图像的等高线会形成多个同心椭圆，它会形成这些椭圆的分布轮廓。

原始模型实际上对应一种特殊情况下的多元高斯分布，这种特殊情况用数学术语来表示可以写成，对于多元高斯分布模型
$$
p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))
$$
当其中的协方差矩阵$\Sigma$的非对角线上都是0的时候，具体的说就是这个协方差矩阵$\Sigma$的对角线上是$(\sigma_1)^2,(\sigma_2)^2,\cdots,(\sigma_n)^2$，而在非对角线上（主对角线上面和下面）的元素都是0，这个时候多元高斯分布模型和原始模型是等效的，原始模型的概率表达式$p(x)=p(x_1;\mu_1,\sigma_1^2)\cdots p(x_n;\mu_n,\sigma_n^2)$中的$\sigma_i^2$恰恰就是该协方差矩阵的主对角线上的值。图像上就会表现为“轴对齐”

一般情况下，图像中椭圆轮廓的旋转角度与非主对角线上的元素有关。

那么异常检测的应用中如何选择使用原始模型还是多元高斯分布模型呢？考虑原特征量在异常样本中是否会非常大或非常小，如果是，在原始模型中，我们必须手动创建新特征（例如$\frac{x_1}{x_2}$或者$\frac{x_1*x_2}{x_1+x_2}$）确保新特征在异常样本中能非常大或非常小，而多元高斯分布模型能自动的捕捉这种不同特征之间的关系

原始模型的一个巨大优势就是其计算成本比较低，换句说法就是它能适应巨大规模的特征数量n，即适应数量巨大的特征；在多元高斯分布模型中，因为我们需要计算协方差矩阵$\Sigma$的逆矩阵，而$\Sigma$是一个n * n 的矩阵，当n 十分巨大的时候，计算成本将会非常高昂，所以多元高斯分布模型能适应的n 值的范围比较小

最后一个比较，对于原始模型而言，即使训练集容量m 比较小也能顺利运行；而对于多元高斯分布模型，这个算法有一些数学推导过程必须保证一些数学性质，m 大于n，也就是样本的数量要大于特征的数量，如果在参数估计阶段我们设置的m 和n 不能满足这个数学性质，那么协方差矩阵$\Sigma$就会是不可逆的，或者说协方差矩阵就会是个奇异矩阵而无法应用于多元高斯分布模型的p(x) 计算中。在实践中，通常m 远大于n 时多元高斯分布模型的效果更好，因为相比较m 即样本容量来说特征个数n 还是比较大，那么协方差矩阵作为一个n * n 的矩阵需要计算$n^2$量级个数的参数，那么我们需要确保有足够的训练样本来拟合这些参数变量才不会欠拟合

值得注意的一点是，如果在使用多元高斯分布模型来做异常检测时发现协方差矩阵$\Sigma$是奇异矩阵，通常有两种原因，第一种是训练集容量m 没有远大于特征数量n，第二种是存在冗余的特征，冗余特征的意思包括重复特征，无意义特征，比如$x_2=x_1$或者$x_3=x_4+x_5$这种高度相关的特征，$x_3$实际上并不包含任何额外信息，只是简单的把两个其他特征加到了一起而已。所以如果我们实现一个多元高斯分布模型时发现协方差矩阵$\Sigma$不可逆，首先确保m 远大于n，其次检查一下有没有冗余特征，如果有两个重复特征，那么就去掉一个，如果有$x_3$那样的多余特征，那么直接把它去掉。从线性代数的角度解释，以上检查冗余特征还可以直接求特征的相关系数来判断是否线性相关