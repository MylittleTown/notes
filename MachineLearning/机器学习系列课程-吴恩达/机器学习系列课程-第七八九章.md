### 过拟合问题

欠拟合：用于描述某个模型没有很好的拟合训练集，具有高偏差

过拟合：当使用高级函数拟合训练集时，其图像会显示出振荡，也就是不停的上下波动，另一种说法就是这个模型具有高方差。这一类模型的函数可能出现太过庞大，变量太多的问题，导致没有足够的数据来约束它来获得一个好的假设函数。

总结下来就是，过度拟合的问题通常会出现在变量过多的时候，这时训练出的假设（函数）能很好的拟合训练集，这个时候的代价函数实际上可能非常接近于0 或者恰好等于0，这个时候我们会得到一个千方百计的拟合训练集的函数曲线导致它无法泛化（这里的“泛化”指的是一个假设模型应用到新样本的能力）到新的样本中，即无法预测新样本。换句话说，在**不重要的特征**下，相似结果的样本在该维度下差异会很大，故方差（在回归模型中我们通常称之为损失值或亏损值，损失值在样本集合上的累加就是代价函数）会很大

在回归模型中我们通常绘制假设模型曲线来选择合适的多项式阶次，每个阶次通常对应一个参数，多项式最高阶次与我们参数向量theta 的维度相关，很多时候一个问题包含多个特征变量，我们可以根据每个变量与预测值y 对应关系绘制的**曲线决定阶次**，然后通过合理的变换替代来减少特征个数，即用新的特征替换原问题中的特征，从而降低模型的假设函数的最高阶次，降低模型的复杂度

### 正则化

思想：如果我们的参数值较小，意味着减小高频（高阶次）分量，使曲线平滑，得到一个更简单的模型，换句话说，我们通过给参数theta(i) 添加惩罚项，在最小化代价函数求出参数向量theta 的过程中使得theta(i) 值非常小，当参数接近0 的时候，我们得到的假设函数就会越平滑，也越简单，也更不容易出现过拟合的问题

所以在正则化中，我们要做的就是修改代价函数来缩小特定的参数，在后面添加一个额外的正则化项来缩小k 个参数的值，值得一提的是，这里选择参数theta(i) 的平方为了确保代价函数对theta(i) 求偏导后的函数与theta(i) 相关。如果不知道该选哪些参数去缩小，这里的k 可以等于特征个数n

![代价函数的正则化表达式](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96%E8%A1%A8%E8%BE%BE%E5%BC%8F.png)

上述正则化项中的lamda 称为正则化参数，它的作用就是控制两个不同目标之间的取舍，第一个目标与目标函数的第一项有关，也就是对训练集样本的损失值求和项，第二个目标是要我们要保持参数theta(i) 尽量的小。也就是说，正则化参数lamda 的作用是控制这两个目标之间的平衡关系，即更好的拟合训练集的目标和将参数控制的更小的目标，从而保持假设模型的相对简单，避免出现过拟合的情况

在不知道如何选择缩小哪些参数时，将k 设置为n，即缩小所有参数，当正则化参数lamda 被设置的太大的话，其结果就是我们对所有参数的惩罚程度太大，那么最后这些参数都会接近于0，这样的话，相当于把假设函数的全部项都“忽略”掉了，假设模型只剩下一个theta(0)

![正则化参数过大](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%82%E6%95%B0%E8%BF%87%E5%A4%A7.png)

差不多相当于用一条直线去拟合数据，这就是一个欠拟合的例子

### 线性回归的正则化

对于正则化线性回归，我们的惩罚对象是参数theta1, theta2, ....一直到theta(n)，默认为没有惩罚theta(0)，当我们使用梯度下降法做线性回归时，将反复去更新参数theta(j) 的值，下式

![线性回归的梯度下降法的正则化](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96.png)

这样做的话，就可以对正则化代价函数J(theta) 用梯度下降法进行最小化，我们可以将上面的更新步长的表达式等价的写成如下的形式

![线性回归的梯度下降法的正则化（2）](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%882%EF%BC%89.png)

通常上面的式子中第一项theta(j) 的系数的值将会是一个只比1 略小一点的数，因为通常学习率alpha 很小，但训练集样本数量m 却很大，整体来看这一项相当于把theta(j) 向0 的方向缩小了一点点，这使得theta(j) 变小了一点点，正式的解释就是theta(j) 的平方范数（平方范数通常表示与原点的距离）变小了

在使用正规方程时的正则化可以表示为如下形式

![正规方程表示梯度下降正则化](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E8%A1%A8%E7%A4%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%AD%A3%E5%88%99%E5%8C%96.png)

其中lamda 的系数是一个(n+1)x(n+1) 维的矩阵

### 逻辑回归（Logistic）的正则化

关于逻辑回归模型的正则化，我们需要将其代价函数做一些修改，在后面增加一项（这里的k 和线性回归模型正则化中的分析相同）

![逻辑回归代价函数的正则化](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96.png)

不同于线性回归模型中我们采用平方误差的求和来表示损失值累计，这里第一项求和的系数是1/m，因为不需要考虑使用梯度下降法时求偏导过程中的数值“2”（当然，实数系数的倍数对模型本身影响不大）类似的，新增的一项的系数（lamda/2m）也是为了方便后续梯度下降法的更新过程中的合并简化。

新增的这一项的作用是惩罚参数theta1, theta2, ... theta(n) 防止它们过大，这样做的话，产生的效果就是即使当你拟合的模型阶次（阶数）很高且参数很多，只要添加了这个正则化项保持参数较小，仍然可以得到一条更能合理划分不同类别（在二元分类问题中划分正样本和负样本）的决策边界（假设函数模型）

下面来推导逻辑回归模型使用正则化的梯度下降法的更新表达式

![逻辑回归的梯度下降法的正则化](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96.png)

这里要注意的是，虽然上述表达式与之前线性回归模型使用正则化后的梯度下降法的更新表达式结构相同，但是我们一定要记得两者假设函数h_theta(x) 的定义不同，所以它们不是同一种算法。

### 非线性假设

首先我们需要声明一个观点，当初始特征个数n 很大时，将这些**高阶多项式项数**（例如对于仅三个特征x1, x2, x3组成的二次项包括x1^2, x2^2, x3^2, x1x2, x1x3, x2x3）包括到特征里会使得假设模型的特征空间急剧膨胀，当特征个数n 很大时，增加特征来建立非线性分类器并不是一个好做法。

### 神经网络

这里我想到从一本书上看到的理论，我们人类大脑在生活中不断的去存储各种事物的“形象”，我们在生活中大脑充当的角色或者说起到的作用就是将任务分成很多只需要判断“是”“否”的步骤，当我们完成了这些步骤的判断后，就能得出结论或者完成动作。类似于我们描述一个人的行为的句子可以按照语法分成多个句法成分，我们所要做的就是就是将这些成分补充完整，例如“画一幅图画”可以分为“画”“一幅”“图画”三个成分，大脑第一步要从存储的动作中补充完整谓语是“画”“写”“扔”等等动作，然后是从“一幅”“两幅”等等量词来决定状语，最后决定画的是一幅“画”。显然，大脑完成这些操作需要具备足够的存储做判断。

神经网络所作的假设是人类大脑所做的这些五花八门的奇妙的事情不需要用上千个不同的程序去实现，相反，大脑处理的方法只需要一个学习算法就可以了。

人工神经网络中我们将使用一个很简单的模型来模拟神经元的工作，我们将神经元模拟成一个逻辑单元，如下图

![单个神经元模拟图](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E5%8D%95%E4%B8%AA%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E6%8B%9F%E5%9B%BE.png)

其中黄色的圆圈表示类似于神经元细胞体的东西，与特征量$x_1, x_2, x_3$之间的连接表示树突或者说输入通道，传递给神经元细胞体一些信息，然后神经元做一些计算并通过它的输出通道，也就是黄色圆圈和$h_\theta(x)$之间的连线表示轴突，输出计算结果

必要的时候会增加一个额外的节点$x_0$，该节点被称作偏置单元或偏置神经元，在模型使用过程中$x_0$总是等于1

在神经网络术语中，激活函数是指代非线性函数g(z)，这里$g(z)=\frac{1}{1+e^{-z}}$，也称为Sigmoid 或者Logistic 激活函数；在神经网络术语中，人们也会将$\theta$称为模型的权重

上面的图代表的是单个的神经元，神经网络其实就是一组神经元连接在一起的集合

![神经网络模拟图](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E6%8B%9F%E5%9B%BE.png)

图中表示的神经网络中有3个输入单元$x_1, x_2, x_3$，3个神经元分别为$a_1, a_2, a_3$以及最后一层第三层有一个节点，正是这第三个节点输出假设函数$h(x)$计算的结果。如果用神经网络的术语来描述，网络中的第一层被称为输入层，我们在这一层输入特征$x_1, x_2, x_3$，最后一层也被称为输出层，这一层的神经元输出假设函数的最终计算结果，中间的第二层被称作隐藏层，神经网络中可以有不止一个隐藏层，实际上任何非输入层或非输出层的层都被称为隐藏层

神经网络究竟在做什么？为了解释这个神经网络具体的计算步骤，这里还有些记号要解释：

- $a_i^{(j)}$表示第j 层第i 个神经元或单元的激活项，所谓激活项是指由一个具体神经元计算并输出的值
- $\theta^{(j)}$表示权重矩阵，它控制从某一层到下一层的映射

根据上面的定义，我们就可以将上图中表示的计算表达如下：
$$
a_1^{(2)}=g(\theta_{10}^{(1)}x_0+\theta_{11}^{(1)}x_1+\theta_{12}^{(1)}x_2+\theta_{13}^{(1)}x_3)\\
a_2^{(2)}=g(\theta_{20}^{(1)}x_0+\theta_{21}^{(1)}x_1+\theta_{22}^{(1)}x_2+\theta_{23}^{(1)}x_3)\\
a_3^{(2)}=g(\theta_{30}^{(1)}x_0+\theta_{31}^{(1)}x_1+\theta_{32}^{(1)}x_2+\theta_{33}^{(1)}x_3)\\
h_{\theta}(x)=a_1^{(3)}=g(\theta_{10}^{(2)}a_0^{(2)}+\theta_{11}^{(2)}a_1^{(2)}+\theta_{12}^{(2)}a_2^{(2)}+\theta_{13}^{(2)}a_3^{(2)})
$$
其中$\theta_{ij}^{(k)}$需要说明的是，i 表示的是映射的下一层的单元编号，j 表示映射的当前层的单元编号，不要混淆；更一般的，如果一个网络在第j 层有$s_j$个单元，在j+1 层有$s_{j+1}$个单元，那么两层之间的参数矩阵$\theta^{(j)}$即控制第j 层到第j+1 层映射的矩阵，它的维度为$s_{j+1}*(s_j+1)$；另外，这里有$x_0, a_0$都是当前层的偏置单元，偏置单元也就是参数矩阵$\theta^{(j)}$计算维度时$(s_j+1)$中的“1” 的原因

关于激活函数g(z) 给出定义$a_i^{(j)}=g(z_i^{(j)})$，这里的底i 表示该单元在当前层的编号，j 表示当前层的编号。接下来将介绍如何通过将神经网络的计算向量化来高效的进行计算，我们将特征向量x 定义为$\left[\begin{matrix}x_0\\x_1\\x_2\\x_3\end{matrix}\right]$其中$x_0$ 照常等于1，作为偏置单元；定义$z^{(2)}$为z 值组成的向量，即$\left[\begin{matrix}z_1^{(2)}\\z_2^{(2)}\\z_3^{(2)}\end{matrix}\right]$，可以看到$z^{(2)}$是一个三维向量；下面我们将根据上述的四个表达式来计算$a_1^{(2)}, a_2^{(2)}, a_3^{(2)}$ :
$$
z^{(2)}=\theta^{(1)}x\\
a^{(2)}=g(z^{(2)})
$$
需要明白的是这里的$z^{(2)}$和$a^{(2)}$都是三维向量，这里的激活项g 是Sigmoid 函数会逐个元素的作用于$z^{(2)}$中的每个元素。注意一点，这里的$\theta$ 不再表示为列向量，而是一个维度为3*4 的矩阵。同样，隐藏层也具有额外的偏置单元，在计算的时候需要加上$a_0^{(2)}=1$，这样一来，$a^{(2)}$就是一个四维的特征向量。

最后要计算假设函数的实际输出值，只需要计算$z^{(3)}$，它根据上面的四个表达式中的第四个式子用$a^{(2)}$和$\theta^{(2)}$可表示为
$$
z^{(3)}=\theta^{(2)}a^{(2)}\\
h_{\theta}(x)=a^{(3)}=g(z^{(3)})
$$
以上这个计算h(x) 的过程称为“前向传播”。形象的概括就是，这个神经网络所做的事情就像是逻辑回归，但是它不是使用原本的特征向量x :$\left[\begin{matrix}x_1\\x_2\\x_3\end{matrix}\right]$作为特征，而是用$a_1, a_2, a_3$作为新的特征。有趣的是，特征项$a_1, a_2, a_3$是学习得到的函数输入值，具体来说，就是从第一层映射到第二层的函数，这个函数由参数矩阵$\theta^{(1)}$决定，也就是说，在神经网络中它没有用输入特征$x_1, x_2, x_3$来训练逻辑回归，而是自己训练逻辑回归的输入$a_1, a_2, a_3$。根据为参数矩阵$\theta$ 选择的不同参数，可以学习得到一些新的特征就可以得到一个更好的假设函数，比使用原始输入$x_1, x_2, x_3$ 时得到的假设更好。

当然输入的特征不仅限于单个特征组成的一次项，也可以选择多项式项作为输入项，但这个算法可以灵活的尝试快速学习任意的特征项，最终把这些$a_1, a_2, a_3$输入最后的单元

**注意：介绍完神经网络的向量化计算，可以解释输出层外每一层中的偏置单元的作用，如果不加偏置项的话，多少层隐藏层都等于一层，因为多个参数矩阵相乘可以得到一个矩阵，等价于这个矩阵和输入层的特征向量相乘得到输出层的激活项**

下面将通过使用神经网络来实现逻辑运算的例子解释为什么神经网络可以用来学习复杂的非线性假设模型，以及处理非线性假设模型的思路。最经典的非线性分类器的例子就是“异或”逻辑运算$x_1{\:}XOR{\:}x_2$（其中$x_1, x_2$都是二进制）

![逻辑运算“异或”的图像](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E2%80%9C%E5%BC%82%E6%88%96%E2%80%9D%E7%9A%84%E5%9B%BE%E5%83%8F.png)

为了建立能够拟合“异或”逻辑运算的神经网络，我们先从一个比较简单的，能够拟合“与”逻辑运算的单个神经元的网络入手。这里定义前提
$$
x_1, x_2{\,}\in\{0, 1\}\\
y=x_1{\,}AND{\,}x_2
$$
网络可以描述为

![逻辑运算“异或”的神经网络](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E2%80%9C%E5%BC%82%E6%88%96%E2%80%9D%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)

现在给这个神经网络的参数矩阵$\theta$赋值为$\left[\begin{matrix}-30\\20\\20\end{matrix}\right]$，这样我们的假设模型h(x) 就可以表示为$h_{\theta}(x)=g(-30+20x_1+20x_2)$，根据激活项Sigmoid 函数的性质可以推导出真值表

![逻辑运算“与”的神经网络对应的真值表](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E2%80%9C%E5%BC%82%E6%88%96%E2%80%9D%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AF%B9%E5%BA%94%E7%9A%84%E7%9C%9F%E5%80%BC%E8%A1%A8.png)

通过观察真值表的结果值可以发现$h_{\theta}(x)$ 近似等于$x_1{\,}AND{\,}x_2$。下面按照同样的思路我们使用含有单个神经元的神经网络来实现“或”逻辑运算，其网络可以描述为

![逻辑运算“或”的神经网络](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E2%80%9C%E4%B8%8E%E2%80%9D%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)

对应的假设模型就是$h_{\theta}(x)=g(-10+20x_1+20x_2)$，推导出其真值表

![逻辑运算“或”的神经网络对应的真值表](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E2%80%9C%E4%B8%8E%E2%80%9D%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AF%B9%E5%BA%94%E7%9A%84%E7%9C%9F%E5%80%BC%E8%A1%A8.png)

从表中的结果值可以看出它的确实现了“或”逻辑运算的功能。下面我们将在这些例子的基础之上解释一个拥有多层神经元的神经网络如何计算更复杂的逻辑运算，例如XOR函数，或者XNOR函数。

首先，类似逻辑“与”运算和逻辑“或”运算，我们构造一个计算逻辑“非”运算的神经网络，如下

![逻辑运算“非”的神经网络](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E2%80%9C%E9%9D%9E%E2%80%9D%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)

给该神经网络设置权重参数$\theta=\left[\begin{matrix}10\\-20\end{matrix}\right]$，这样假设模型是在计算$h(x)=g(10-20x_1)$，经过核验可以发现这个神经网络实现了逻辑“非”运算。（我们也可以发现，若要实现逻辑“非”运算，大体思想就是在预期得到“非”结果的变量前面放一个很大的负权重）

综上所述，我们得到了实现逻辑“与”运算，逻辑“或”运算以及逻辑“非”运算的神经网络

![三种逻辑运算的神经网络](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E4%B8%89%E7%A7%8D%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)

为了实现逻辑表达式$x_1{\,}XNOR{\,}x_2$，我们先构造输入层和第二层（隐藏层）

![构造逻辑运算的神经网络的第一层和第二层](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E6%9E%84%E9%80%A0%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%B8%80%E5%B1%82%E5%92%8C%E7%AC%AC%E4%BA%8C%E5%B1%82.png)

列出该神经网络对应的真值表

![构造逻辑运算的神经网络的第一层和第二层的真值表](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E6%9E%84%E9%80%A0%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%B8%80%E5%B1%82%E5%92%8C%E7%AC%AC%E4%BA%8C%E5%B1%82%E7%9A%84%E7%9C%9F%E5%80%BC%E8%A1%A8.png)

接下来设计下一层，这里是输出层

![构造逻辑运算的神经网络的第二层和输出层](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E6%9E%84%E9%80%A0%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%BA%8C%E5%B1%82%E5%92%8C%E8%BE%93%E5%87%BA%E5%B1%82.png)

相应的我们完成关于假设函数的预测值$h_{\theta}(x)$的真值表，预测值是我们对于逻辑运算表达式$x_1{\,}XNOR{\,}x_2$关于输入$x_1, x_2$的真值表，是已知的标签。

![构造逻辑运算的神经网络的第二层和输出层的真值表](https://raw.githubusercontent.com/MylittleTown/notes/master/MachineLearning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B-%E5%90%B4%E6%81%A9%E8%BE%BE/Related_images/%E7%AC%AC%E4%B8%83%E5%85%AB%E4%B9%9D%E7%AB%A0-%E6%9E%84%E9%80%A0%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%BA%8C%E5%B1%82%E5%92%8C%E8%BE%93%E5%87%BA%E5%B1%82%E7%9A%84%E7%9C%9F%E5%80%BC%E8%A1%A8.png)

通过上述实现逻辑运算表达式$x_1{\,}XNOR{\,}x_2$我们可以发现，设计复杂的非线性假设模型对应的神经网络，我们可以采用“分治”法，每一层的神经元对应一个逻辑表达式，且由上一层的逻辑表达式通过特定的逻辑运算获得，这些逻辑运算都能通过设置$\theta_{ij}^w$来实现神经网络，其中i 指向$s_{w+1}$层的神经元编号，j 指向$s_w$层的神经元编号。隐藏层的神经单元我们通常理解为中间特征。

抽象来说，多层的神经网络每次进入下一层它都能在上一层对应的相对简单的函数基础之上计算更加复杂的方程或函数或称为特征，最后这些“中间”特征被用于逻辑回归分类器的最后一层，也就是输出层来准确的预测出$h_{\theta}(x)$，这也就是神经网络能用来学习复杂的假设模型的原因。

### 多元分类

在神经网络中实现多类别分类，采用的方法本质上是一对多法的拓展，输出层对应分类的类别个数设置相同个数的输出神经元取值为$\{0, 1\}$，那么输出层即$h_{\theta}(x)$应该表示为N 维列向量，其中N 表示类别个数，同样，我们构建的训练集中输入特征样本$x^{(i)}$的标签$y^{(i)}$也表示为N 维列向量

### 代价函数

假设我们有一个与下图类似的神经网络结构和训练集$\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}$，其中有m 组训练样本$(x^{(i)},y^{(i)})$，然后定义$L$ 来表示这个神经网络结构的总层数，所以对于下图的神经网络结构$L=4$，$s_l$表示第$l$ 层的单元数，也就是神经元的数量，这其中不包括第$l$ 层的偏置单元

这里我们选择逻辑回归模型正则化的代价函数：
$$
J(\theta)=-\frac{1}{m}\left(\sum_{i=1}^my^{(i)}log\,h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))\right)+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2
$$
其中m 表示训练集样本数量，n 表示特征数量；对应的我们给出神经网络的代价函数表达式
$$
J(\theta)=-\frac{1}{m}\left(\sum_{i=1}^m{\sum_{k=1}^Ky_k^{(i)}log(h_{\theta}(x^{(i)}))_k}+(1-y_k^{(i)})log(1-(h_{\theta}(x^{(i)}))_k)\right)\\+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\theta_{ji}^{(l)})^2
$$
第一部分的求和项表示的是K 个输出单元之和，也就是每一个逻辑回归算法的代价函数，然后按照四次输出的顺序依次把这些代价函数加起来；正则化部分（第二部分）的L-1 代表从$l=1$ 开始的序号的参数矩阵$\theta^{(l)}$总共有L-1 个；正则化部分中我们通常不取i = 0 的情况，因为在第$l$ 层计算下一层的激励项（使用Sigmoid 函数g）时，i = 0的单元对应的参数$\theta_{j0}^{(l)}$的项为$\theta_{j0}^{(l)}x_0$或$\theta_{j0}^{(l)}a_0$，而$x_0$或$a_0$作为第$l$ 层的偏置单元等于常数1，不需要正则化这一项来避免过拟合（当然，这只是一个合理化的约定，即使在计算过程中求和包含了i = 0 的项，也就是求和i 从0 加到$s_l$，整个代价函数的表达式依然有效，并且不会有大的区别）；另外，求和项应用于$y_k$和$h_k$是因为我们这个代价函数是将第K 个输出单元的值和$y_k$的值的大小作比较，而$y_k$的值就是我们在训练集中对每个训练样本设置的标签，表示其应属于哪个类的量

### 反向传播算法

对于神经网络的代价函数，我们要做的就是找到参数$\theta$ 使得$J(\theta)$取到最小值。为了使用梯度下降法或者其他某种高级优化算法，我们需要做的就是获得输入参数$\theta$ 并计算$J(\theta)$和偏导项，那么如何计算偏导项就是关键。

反向传播算法从直观上说就是对每一个节点我们计算第$l$ 层的第j 个节点的误差，用$\delta_j^{(l)}$表示，下面假设神经网络总层数为L，对于输出层的每个节点（神经元）有$\delta_j^{(L)}=a_j^{(L)}-y_j$，其中$a_j^{(L)}$作为输出层的单元也可以写成$h_\theta(x)$，即逻辑回归模型的预测值，所以$\delta_j^{(L)}$就是假设的输出值和训练集样本的标签y 值之间的差，表达式中$y_j$ 的下标j 就是训练集的向量y 的第j 个元素的值；当然，上述表达式可以向量化表示为$\delta^{(L)}=a^{(L)}-y$，这里的每一项都是一个向量，并且向量维数等于输出单元的个数。

下面是关于隐藏层的$\delta$项：$\delta^{(l)}=(\theta^{(l)})^T\delta^{(l+1)}.*g'(z^{(l)})$，其中$g'(z^{(l)})$表示激活函数Sigmoid 函数对$z^{(l)}$求导，结果为$g'(z^{(l)})=a^{(l)}.*(1-a^{(l)})$，因为表达式中$z^{(l)}$和$a^{(l)}$都是向量，所以是用点乘，1其实是与$a^{(l)}$相同的维度的列向量（这里谈谈对于这个表达式的理解，我们可以看作是对原式$a^{(l)}=g(z^{(l)})$两边求导，根据链式求导法则得到$d\,a=\frac{d\,g}{d\,z}.*d\,z$，这里的$d$ 表示求导）

如果不是特别严谨的情况下，我们可以证明我们要求的偏导数项$\frac{\partial}{\partial\theta_{ji}^{(l)}}J(\theta)=a_j^{(l)}\delta_i^{(l+1)}$，当然，这里我们忽略了正则化项**（虽然老师课上是写成代价函数$J(\theta)$关于每个参数$\theta_{ij}^{(l)}$的偏导数，但是根据“之后提到的梯度下降法”中关于这个偏导数项的应用，我有理由相信这里实际上是逻辑回归模型的损失函数Cost 关于每个参数$\theta_{ij}^{(l)}$的偏导数，定义$\delta_j^{(l)}=\frac{\partial}{\partial{z_j^{(l)}}}cost(k)$，将$a_j^{(l)}=\frac{\partial}{\partial \theta_{ij}}z_i^{(l+1)}$代入原式，由求导链式法则可推导出表达式$\frac{\partial}{\partial\theta_{ij}^{(l)}}Cost(k)=a_j^{(l)}\delta_i^{(l+1)}$成立，其中k 表示训练样本编号，之后我会将原来的偏导数项中$J({\theta})$修改成 函数Cost()）**

现在将上述的两个公式应用于一个非常大的训练样本，假设我们有一个m 个样本的训练集$\{(x^{(1)},y^{(1)},...,(x^{(m)},y^{(m)}\}$，初始化一个变量（空矩阵）$\Delta_{ij}^{(l)}=0$，在之后的计算过程中可以理解为一个存放代价函数对$\theta$求偏导的临时结果的数据结构。接下来我们将遍历训练集，对于每次遍历都对应一个训练集样本$(x^{(k)},y^{(k)})$，并将这个样本的特征向量作为输入传入神经网络的输入层，即$a^{(1)}=x^{(k)}$，然后根据运用正向传播来计算下一层的激活值$a^{(2)},a^{(3)},...,a^{(L)}$，此时神经网络初步结构就已经完成了，根据误差$\delta^{(l)}$的定义输出层满足$\delta^{(L)}=a^{(L)}-y^{(k)}$，然后我们将通过反向传播算法计算$\delta^{(L-1)},\delta^{(L-2)},...,\delta^{(2)}$，这里默认第一层作为输入层没有误差，即$\delta^{(1)}=0$，并且需要注意的是，这里的变量都是多维列向量。最后，我们根据之前提到的代价函数偏导数的表达式$\frac{\partial}{\partial\theta_{ji}^{(l)}}Cost(k)=a_j^{(l)}\delta_i^{(l+1)}$从第2层开始到第L 层在遍历训练样本的循环内赋值一次$\Delta_{ij}^{(l)}$，$\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}$，该表达式对应的是单一层（即$l$ 层），遍历过程中更新（累积）单一层对应的$\Delta_{ij}^{(l)}$，其中i 和j 分别遍历第$l$ 层和第$l+1$ 层的单元节点，所以类似的，我们可以将关于单元编号的i 和j 用向量化表示得到新的表达式$\Delta^{(l)}:=\Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^T$，这里的变量都是多维向量

下面定义
$$
D_{ij}^{(l)}:=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\theta_{ij}^{(l)}\:\:if\:j\neq0\\
D_{ij}^{(l)}:=\frac{1}{m}\Delta_{ij}^{(l)}\:\:if\:j\,=\,0\\
$$
我们知道当$j = 0$的情况下对应偏置项，所以当$j=0$的时候$D_{ij}^{(l)}$的表达式没有写额外的标准化项，而这个$D_{ij}^{(l)}$实际上就是我们代价函数关于每个参数$\theta_{ij}^{(l)}$的偏导数

可以这么理解，神经网络的结构相当于是很多个逻辑回归模型组合而成，每个除了输入层的神经单元都对应一个逻辑回归模型，包括假设函数$a_j^{(l+1)}=g(z_j^{(l+1)})=h_{\theta_{ji}^{(l)}}(a_i^{(l)})$，神经网络的梯度下降法中更新的步长实际上就是由每个简单的逻辑回归模型的代价函数关于参数$\theta_(ij)^{(l)}$的偏导数共同组成

### 梯度检验

之前介绍的反向传播算法含有许多细节，实现起来比较困难，并且它有一个不好的特性，在代码实现时很容易产生一些微妙的bug（程序错误），当它与梯度下降法或是其他算法一同工作时看起来它确实能正常运行并且代价函数$J(\theta)$在每次梯度下降的迭代中也在不断减小，但是到了最后，我们得到的神经网络其误差将会比无bug 的情况下高出一个量级并且我们也许无法察觉到这个结果是由bug 所导致的

这里引入双侧差分和单侧差分，即对于$\theta$为实数的条件下，函数$J(\theta)$在某一点$\theta_0$的导数近似值
$$
\qquad\qquad\qquad\qquad\quad
\frac{d}{d\theta}J(\theta)\approx\frac{J(\theta+\varepsilon)-J(\theta-\varepsilon)}{2\varepsilon}\ \ \ \ \ (\varepsilon取值10^{-4}左右)\\
\frac{d}{d\theta}J(\theta)\approx\frac{J(\theta+\varepsilon)-J(\theta)}{\varepsilon}
$$
后者单侧差分是传统的导数的定义，前者为双侧差分，虽然结构上与单侧差分相似，但是两者并不等同，当且仅当函数$J(\theta)$在$\theta_0$上可导，即该点左右存在极限的情况下两者相等。我们通常使用双侧差分作为函数在某点的导数的近似值。

下面考虑更加普遍的情况，也就是当$\theta$为向量参数的时候，设$\theta$是n 维向量，$\theta=[\theta_1,\theta_2,...,\theta_n]$。我们可以用类似的思想来估计所有的偏导数项，如下
$$
\frac{\partial}{\partial\theta_1}J(\theta)\approx\frac{J(\theta_1+\varepsilon,\theta_2,\theta_3,...,\theta_n)-J(\theta_1-\varepsilon,\theta_2,\theta_3,...,\theta_n)}{2\varepsilon}\\
\frac{\partial}{\partial\theta_2}J(\theta)\approx\frac{J(\theta_1,\theta_2+\varepsilon,\theta_3,...,\theta_n)-J(\theta_1,\theta_2-\varepsilon,\theta_3,...,\theta_n)}{2\varepsilon}\\
\vdots\\
\frac{\partial}{\partial\theta_n}J(\theta)\approx\frac{J(\theta_1,\theta_2,\theta_3,...,\theta_n+\varepsilon)-J(\theta_1,\theta_2,\theta_3,...,\theta_n-\varepsilon)}{2\varepsilon}
$$
如果我们换一下概念，$\theta_k$的下标改为上标对应的是其在神经网络中某两层之间的参数矩阵，即$\theta^k$，那么就变成如下表达式
$$
\frac{\partial}{\partial\theta^{(1)}}J(\theta)\approx\frac{J(\theta^{(1)}+\varepsilon,\theta^{(2)},\theta^{(3)},...,\theta^{(n)}-J(\theta^{(1)}-\varepsilon,\theta^{(2)},\theta^{(3)},...,\theta^{(n)})}{2\varepsilon}\\
\frac{\partial}{\partial\theta^{(2)}}J(\theta)\approx\frac{J(\theta^{(1)},\theta^{(2)}+\varepsilon,\theta^{(3)},...,\theta^{(n)})-J(\theta^{(1)},\theta^{(2)}-\varepsilon,\theta^{(3)},...,\theta^{(n)})}{2\varepsilon}\\
\vdots\\
\frac{\partial}{\partial\theta^{(n)}}J(\theta)\approx\frac{J(\theta^{(1)},\theta^{(2)},\theta^{(3)},...,\theta^{(n)}+\varepsilon)-J(\theta^{(1)},\theta^{(2)},\theta^{(3)},...,\theta^{(n)}-\varepsilon)}{2\varepsilon}
$$
当我们在神经网络中使用这种方法时，通过for 循环来完成我们对神经网络中代价函数的所有参数的偏导数的计算，然后与我们在反向传播中的得到的梯度（计算代价函数关于所有参数的导数或者偏导数）进行比较，如果二者相等或者在数值上非常的近似（只有几位小数的差距），那么我们就可以确信反向传播的实现是正确的*（Q: 神经网络中我们对于代价函数没有明确的定义，能不能姑且认为其代价函数就是训练样本代入神经网络后输出层的逻辑回归模型的代价函数，也就是说，对于上面的n 个表达式，每个表达式都需要修改对应层的参数矩阵$\theta^{(k)}$为$\theta^{(k)}+\varepsilon$ 和$\theta^{(k)}-\varepsilon$代入前向传播从输入层到输出层然后计算代价函数，两次计算结果相减除以$2\varepsilon$作为近似梯度？如果这个想法是成立的，那么确实每次梯度检测都是一个计算量庞大的过程）*

这里我们得说明的是，无论是梯度检查还是反向传播算法中得到的梯度都反映的是代价函数$J(\theta)$在某一点的导数近似值，也就是梯度下降更新的一“步”，对于反向传播算法代码实现过程中由于bug 出现的误差我们都可以通过梯度检测来判断反向传播的梯度有效，为此我们只需检测一“步”，一旦检测完成，在使用我们的代码进行学习或是训练网络之前，重要的是关闭梯度检测，这时因为上面介绍的梯度检测的代码是一个计算量非常大的，也是非常慢的计算导数的程序，不如反向传播算法计算梯度的高效率，如果在每次梯度下降迭代或在每次代价函数的内循环里都运行一次梯度检测，程序就会变得非常慢。*（Q: 所以我是不是可以理解为反向传播算法中在代码实现的bug 与参数数值无关？）*

### 随机初始化

当我们执行一个算法例如梯度下降法或者其他高级优化算法时，我们都需要为变量$\theta$，也就是参数矩阵选取一些初始值。对于梯度下降法，我们需要对$\theta$进行初始化，这样我们就可以计算每一步的更新步长，一步步通过梯度下降来最小化代价函数 J。

尽管在逻辑回归模型中将$\theta$ 的初始值设置为相同的值，例如零矩阵，即参数矩阵的元素全部设为0，这个做法是被允许的（比较神经网络的反向传播就会理解，因为逻辑回归模型相当于只有两层，输入层和输出层，它的步长不止与参数$\theta$有关，而且与输入的训练样本$x^{(i)},y^{(i)}$直接相关）但实际上在训练网络时将所有参数初始化为0 起不到任何作用。

将所有参数都初始化为0，代入神经网络的正向传播和反向传播算法中可以发现，对于任何输入样本$x^{(i)}$，对应的隐藏层每一层的单元都相等，也就是说，对于第$l$ 层（$1 < l< L$），$a_k^{(l)}=a_{k+1}^{(l)}$，从而在计算反向传播的$\delta$ 时，同一层的$\delta$ 相等，根据之前对代价函数推导得到的表达式我们可以发现代价函数关于参数的偏导数都是相等的，即$\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta)=\frac{\partial}{\partial\theta_{i\,j+1}^{(l)}}J(\theta)$，该等式可以**在任何一次梯度下降的更新中**成立，那么也就是说代价函数关于该层的所有的参数进行梯度下降更新后，这些参数的结果都会相等，只是不再为0

总结一下就是，即使梯度下降进行了一次迭代，但是任何一层隐藏层的任何一对单元依然会以相同的函数输入来计算，那么对应的连接到下一层的权重参数$\theta$ 都将会继续相等，下一层的隐藏单元也会继续相等，意味着这些隐藏单元都在计算相同的特征，这是一种高度冗余的现象，因为最后输出层的逻辑回归模型的输出单元将只会得到一个特征，这样阻止了神经网络去学习任何有趣，有意义的东西。

所以，为了解决这一个“对称权重”问题（也就是所有权重都一样），在神经网络中对参数进行初始化时，要使用随机初始化的思想。具体来说，对每一个参数矩阵$\theta$ 中的元素我们将其初始化为一个范围在$[-\varepsilon,\varepsilon]$的随机值，当然这里的$\varepsilon$ 和之前梯度检测中提到的$\varepsilon$ 指代的不是同一变量。在Matlab 中我们随机初始化的代码可以如下：

```matlab
Theta1 = rand(10,11)*(2*INIT_EPSILON)-INIT_EPSILON;
Theta2 = rand(1,11)*(2*INIT_EPSILON)-INIT_EPSILON;
```

这里调用的$rand(m,n)$ 函数会生成一个m*n 的随机矩阵，矩阵中的值都是介于0到1 之间的实数，是0 到1 之间的连续值

总而言之，为了训练神经网络，应首先要将权重也就是参数矩阵中元素的值随机初始化为一个接近于0的且范围在$-\varepsilon$ 到$\varepsilon$ 之间的数，然后进行反向传播，再进行梯度检验，最后使用梯度下降或者其他高级优化算法来最小化代价函数

