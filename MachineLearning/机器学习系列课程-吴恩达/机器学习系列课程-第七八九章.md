### 过拟合问题

欠拟合：用于描述某个模型没有很好的拟合训练集，具有高偏差

过拟合：当使用高级函数拟合训练集时，其图像会显示出振荡，也就是不停的上下波动，另一种说法就是这个模型具有高方差。这一类模型的函数可能出现太过庞大，变量太多的问题，导致没有足够的数据来约束它来获得一个好的假设函数。

总结下来就是，过度拟合的问题通常会出现在变量过多的时候，这时训练出的假设（函数）能很好的拟合训练集，这个时候的代价函数实际上可能非常接近于0 或者恰好等于0，这个时候我们会得到一个千方百计的拟合训练集的函数曲线导致它无法泛化（这里的“泛化”指的是一个假设模型应用到新样本的能力）到新的样本中，即无法预测新样本。换句话说，在**不重要的特征**下，相似结果的样本在该维度下差异会很大，故方差（在回归模型中我们通常称之为损失值或亏损值，损失值在样本集合上的累加就是代价函数）会很大

在回归模型中我们通常绘制假设模型曲线来选择合适的多项式阶次，每个阶次通常对应一个参数，多项式最高阶次与我们参数向量theta 的维度相关，很多时候一个问题包含多个特征变量，我们可以根据每个变量与预测值y 对应关系绘制的**曲线决定阶次**，然后通过合理的变换替代来减少特征个数，即用新的特征替换原问题中的特征，从而降低模型的假设函数的最高阶次，降低模型的复杂度

### 正则化

思想：如果我们的参数值较小，意味着减小高频（高阶次）分量，使曲线平滑，得到一个更简单的模型，换句话说，我们通过给参数theta(i) 添加惩罚项，在最小化代价函数求出参数向量theta 的过程中使得theta(i) 值非常小，当参数接近0 的时候，我们得到的假设函数就会越平滑，也越简单，也更不容易出现过拟合的问题

所以在正则化中，我们要做的就是修改代价函数来缩小特定的参数，在后面添加一个额外的正则化项来缩小k 个参数的值，值得一提的是，这里选择参数theta(i) 的平方为了确保代价函数对theta(i) 求偏导后的函数与theta(i) 相关。如果不知道该选哪些参数去缩小，这里的k 可以等于特征个数n

![image-20210109100905385](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210109100905385.png)

上述正则化项中的lamda 称为正则化参数，它的作用就是控制两个不同目标之间的取舍，第一个目标与目标函数的第一项有关，也就是对训练集样本的损失值求和项，第二个目标是要我们要保持参数theta(i) 尽量的小。也就是说，正则化参数lamda 的作用是控制这两个目标之间的平衡关系，即更好的拟合训练集的目标和将参数控制的更小的目标，从而保持假设模型的相对简单，避免出现过拟合的情况

在不知道如何选择缩小哪些参数时，将k 设置为n，即缩小所有参数，当正则化参数lamda 被设置的太大的话，其结果就是我们对所有参数的惩罚程度太大，那么最后这些参数都会接近于0，这样的话，相当于把假设函数的全部项都“忽略”掉了，假设模型只剩下一个theta(0)

![image-20210109102131820](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210109102131820.png)

差不多相当于用一条直线去拟合数据，这就是一个欠拟合的例子

### 线性回归的正则化

对于正则化线性回归，我们的惩罚对象是参数theta1, theta2, ....一直到theta(n)，默认为没有惩罚theta(0)，当我们使用梯度下降法做线性回归时，将反复去更新参数theta(j) 的值，下式

![image-20210109103602507](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210109103602507.png)

这样做的话，就可以对正则化代价函数J(theta) 用梯度下降法进行最小化，我们可以将上面的更新步长的表达式等价的写成如下的形式

![image-20210109103903239](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210109103903239.png)

通常上面的式子中第一项theta(j) 的系数的值将会是一个只比1 略小一点的数，因为通常学习率alpha 很小，但训练集样本数量m 却很大，整体来看这一项相当于把theta(j) 向0 的方向缩小了一点点，这使得theta(j) 变小了一点点，正式的解释就是theta(j) 的平方范数（平方范数通常表示与原点的距离）变小了

在使用正规方程时的正则化可以表示为如下形式

![image-20210109111603750](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210109111603750.png)

其中lamda 的系数是一个(n+1)x(n+1) 维的矩阵

### 逻辑回归（Logistic）的正则化

关于逻辑回归模型的正则化，我们需要将其代价函数做一些修改，在后面增加一项（这里的k 和线性回归模型正则化中的分析相同）

![image-20210109150958362](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210109150958362.png)

不同于线性回归模型中我们采用平方误差的求和来表示损失值累计，这里第一项求和的系数是1/m，因为不需要考虑使用梯度下降法时求偏导过程中的数值“2”（当然，实数系数的倍数对模型本身影响不大）类似的，新增的一项的系数（lamda/2m）也是为了方便后续梯度下降法的更新过程中的合并简化。

新增的这一项的作用是惩罚参数theta1, theta2, ... theta(n) 防止它们过大，这样做的话，产生的效果就是即使当你拟合的模型阶次（阶数）很高且参数很多，只要添加了这个正则化项保持参数较小，仍然可以得到一条更能合理划分不同类别（在二元分类问题中划分正样本和负样本）的决策边界（假设函数模型）

下面来推导逻辑回归模型使用正则化的梯度下降法的更新表达式

![image-20210109151827753](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210109151827753.png)

这里要注意的是，虽然上述表达式与之前线性回归模型使用正则化后的梯度下降法的更新表达式结构相同，但是我们一定要记得两者假设函数h_theta(x) 的定义不同，所以它们不是同一种算法。

### 非线性假设

首先我们需要声明一个观点，当初始特征个数n 很大时，将这些**高阶多项式项数**（例如对于仅三个特征x1, x2, x3组成的二次项包括x1^2, x2^2, x3^2, x1x2, x1x3, x2x3）包括到特征里会使得假设模型的特征空间急剧膨胀，当特征个数n 很大时，增加特征来建立非线性分类器并不是一个好做法。

### 神经网络

这里我想到从一本书上看到的理论，我们人类大脑在生活中不断的去存储各种事物的“形象”，我们在生活中大脑充当的角色或者说起到的作用就是将任务分成很多只需要判断“是”“否”的步骤，当我们完成了这些步骤的判断后，就能得出结论或者完成动作。类似于我们描述一个人的行为的句子可以按照语法分成多个句法成分，我们所要做的就是就是将这些成分补充完整，例如“画一幅图画”可以分为“画”“一幅”“图画”三个成分，大脑第一步要从存储的动作中补充完整谓语是“画”“写”“扔”等等动作，然后是从“一幅”“两幅”等等量词来决定状语，最后决定画的是一幅“画”。显然，大脑完成这些操作需要具备足够的存储做判断。

神经网络所作的假设是人类大脑所做的这些五花八门的奇妙的事情不需要用上千个不同的程序去实现，相反，大脑处理的方法只需要一个学习算法就可以了。

人工神经网络中我们将使用一个很简单的模型来模拟神经元的工作，我们将神经元模拟成一个逻辑单元，如下图

![image-20210109170857154](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210109170857154.png)

其中黄色的圆圈表示类似于神经元细胞体的东西，与特征量$x_1, x_2, x_3$之间的连接表示树突或者说输入通道，传递给神经元细胞体一些信息，然后神经元做一些计算并通过它的输出通道，也就是黄色圆圈和$h_\theta(x)$之间的连线表示轴突，输出计算结果

必要的时候会增加一个额外的节点$x_0$，该节点被称作偏置单元或偏置神经元，在模型使用过程中$x_0$总是等于1

在神经网络术语中，激活函数是指代非线性函数g(z)，这里$g(z)=\frac{1}{1+e^{-z}}$，也称为Sigmoid 或者Logistic 激活函数；在神经网络术语中，人们也会将$\theta$称为模型的权重

上面的图代表的是单个的神经元，神经网络其实就是一组神经元连接在一起的集合

![image-20210109173431619](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210109173431619.png)

图中表示的神经网络中有3个输入单元$x_1, x_2, x_3$，3个神经元分别为$a_1, a_2, a_3$以及最后一层第三层有一个节点，正是这第三个节点输出假设函数$h(x)$计算的结果。如果用神经网络的术语来描述，网络中的第一层被称为输入层，我们在这一层输入特征$x_1, x_2, x_3$，最后一层也被称为输出层，这一层的神经元输出假设函数的最终计算结果，中间的第二层被称作隐藏层，神经网络中可以有不止一个隐藏层，实际上任何非输入层或非输出层的层都被称为隐藏层

神经网络究竟在做什么？为了解释这个神经网络具体的计算步骤，这里还有些记号要解释：

- $a_i^{(j)}$表示第j 层第i 个神经元或单元的激活项，所谓激活项是指由一个具体神经元计算并输出的值
- $\theta^{(j)}$表示权重矩阵，它控制从某一层到下一层的映射

根据上面的定义，我们就可以将上图中表示的计算表达如下：
$$
a_1^{(2)}=g(\theta_{10}^{(1)}x_0+\theta_{11}^{(1)}x_1+\theta_{12}^{(1)}x_2+\theta_{13}^{(1)}x_3)\\
a_2^{(2)}=g(\theta_{20}^{(1)}x_0+\theta_{21}^{(1)}x_1+\theta_{22}^{(1)}x_2+\theta_{23}^{(1)}x_3)\\
a_3^{(2)}=g(\theta_{30}^{(1)}x_0+\theta_{31}^{(1)}x_1+\theta_{32}^{(1)}x_2+\theta_{33}^{(1)}x_3)\\
h_{\theta}(x)=a_1^{(3)}=g(\theta_{10}^{(2)}a_0^{(2)}+\theta_{11}^{(2)}a_1^{(2)}+\theta_{12}^{(2)}a_2^{(2)}+\theta_{13}^{(2)}a_3^{(2)})
$$
其中$\theta_{ij}^{(k)}$需要说明的是，i 表示的是映射的下一层的单元编号，j 表示映射的当前层的单元编号，不要混淆；更一般的，如果一个网络在第j 层有$s_j$个单元，在j+1 层有$s_{j+1}$个单元，那么两层之间的参数矩阵$\theta^{(j)}$即控制第j 层到第j+1 层映射的矩阵，它的维度为$s_{j+1}*(s_j+1)$；另外，这里有$x_0, a_0$都是当前层的偏置单元，偏置单元也就是参数矩阵$\theta^{(j)}$计算维度时$(s_j+1)$中的“1” 的原因

关于激活函数g(z) 给出定义$a_i^{(j)}=g(z_i^{(j)})$，这里的底i 表示该单元在当前层的编号，j 表示当前层的编号。接下来将介绍如何通过将神经网络的计算向量化来高效的进行计算，我们将特征向量x 定义为$\left[\begin{matrix}x_0\\x_1\\x_2\\x_3\end{matrix}\right]$其中$x_0$ 照常等于1，作为偏置单元；定义$z^{(2)}$为z 值组成的向量，即$\left[\begin{matrix}z_1^{(2)}\\z_2^{(2)}\\z_3^{(2)}\end{matrix}\right]$，可以看到$z^{(2)}$是一个三维向量；下面我们将根据上述的四个表达式来计算$a_1^{(2)}, a_2^{(2)}, a_3^{(2)}$ :
$$
z^{(2)}=\theta^{(1)}x\\
a^{(2)}=g(z^{(2)})
$$
需要明白的是这里的$z^{(2)}$和$a^{(2)}$都是三维向量，这里的激活项g 是Sigmoid 函数会逐个元素的作用于$z^{(2)}$中的每个元素。注意一点，这里的$\theta$ 不再表示为列向量，而是一个维度为3*4 的矩阵。同样，隐藏层也具有额外的偏置单元，在计算的时候需要加上$a_0^{(2)}=1$，这样一来，$a^{(2)}$就是一个四维的特征向量。

最后要计算假设函数的实际输出值，只需要计算$z^{(3)}$，它根据上面的四个表达式中的第四个式子用$a^{(2)}$和$\theta^{(2)}$可表示为
$$
z^{(3)}=\theta^{(2)}a^{(2)}\\
h_{\theta}(x)=a^{(3)}=g(z^{(3)})
$$
以上这个计算h(x) 的过程称为“前向传播”。形象的概括就是，这个神经网络所做的事情就像是逻辑回归，但是它不是使用原本的特征向量x :$\left[\begin{matrix}x_1\\x_2\\x_3\end{matrix}\right]$作为特征，而是用$a_1, a_2, a_3$作为新的特征。有趣的是，特征项$a_1, a_2, a_3$是学习得到的函数输入值，具体来说，就是从第一层映射到第二层的函数，这个函数由参数矩阵$\theta^{(1)}$决定，也就是说，在神经网络中它没有用输入特征$x_1, x_2, x_3$来训练逻辑回归，而是自己训练逻辑回归的输入$a_1, a_2, a_3$。根据为参数矩阵$\theta$ 选择的不同参数，可以学习得到一些新的特征就可以得到一个更好的假设函数，比使用原始输入$x_1, x_2, x_3$ 时得到的假设更好。

当然输入的特征不仅限于单个特征组成的一次项，也可以选择多项式项作为输入项，但这个算法可以灵活的尝试快速学习任意的特征项，最终把这些$a_1, a_2, a_3$输入最后的单元

**注意：介绍完神经网络的向量化计算，可以解释输出层外每一层中的偏置单元的作用，如果不加偏置项的话，多少层隐藏层都等于一层，因为多个参数矩阵相乘可以得到一个矩阵，等价于这个矩阵和输入层的特征向量相乘得到输出层的激活项**

下面将通过使用神经网络来实现逻辑运算的例子解释为什么神经网络可以用来学习复杂的非线性假设模型，以及处理非线性假设模型的思路。最经典的非线性分类器的例子就是“异或”逻辑运算$x_1{\:}XOR{\:}x_2$（其中$x_1, x_2$都是二进制）

![image-20210112205951101](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210112205951101.png)

为了建立能够拟合“异或”逻辑运算的神经网络，我们先从一个比较简单的，能够拟合“与”逻辑运算的单个神经元的网络入手。这里定义前提
$$
x_1, x_2{\,}\in\{0, 1\}\\
y=x_1{\,}AND{\,}x_2
$$
网络可以描述为

![image-20210112210650515](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210112210650515.png)

现在给这个神经网络的参数矩阵$\theta$赋值为$\left[\begin{matrix}-30\\20\\20\end{matrix}\right]$，这样我们的假设模型h(x) 就可以表示为$h_{\theta}(x)=g(-30+20x_1+20x_2)$，根据激活项Sigmoid 函数的性质可以推导出真值表

![image-20210112211350163](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210112211350163.png)

通过观察真值表的结果值可以发现$h_{\theta}(x)$ 近似等于$x_1{\,}AND{\,}x_2$。下面按照同样的思路我们使用含有单个神经元的神经网络来实现“或”逻辑运算，其网络可以描述为

![image-20210112211649334](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210112211649334.png)

对应的假设模型就是$h_{\theta}(x)=g(-10+20x_1+20x_2)$，推导出其真值表

![image-20210112211807431](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210112211807431.png)

从表中的结果值可以看出它的确实现了“或”逻辑运算的功能。下面我们将在这些例子的基础之上解释一个拥有多层神经元的神经网络如何计算更复杂的逻辑运算，例如XOR函数，或者XNOR函数。

首先，类似逻辑“与”运算和逻辑“或”运算，我们构造一个计算逻辑“非”运算的神经网络，如下

![image-20210115201236657](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210115201236657.png)

给该神经网络设置权重参数$\theta=\left[\begin{matrix}10\\-20\end{matrix}\right]$，这样假设模型是在计算$h(x)=g(10-20x_1)$，经过核验可以发现这个神经网络实现了逻辑“非”运算。（我们也可以发现，若要实现逻辑“非”运算，大体思想就是在预期得到“非”结果的变量前面放一个很大的负权重）

综上所述，我们得到了实现逻辑“与”运算，逻辑“或”运算以及逻辑“非”运算的神经网络

![image-20210115201949959](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210115201949959.png)

为了实现逻辑表达式$x_1{\,}XNOR{\,}x_2$，我们先构造输入层和第二层（隐藏层）

![image-20210115202259454](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210115202259454.png)

列出该神经网络对应的真值表

![image-20210115202347018](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210115202347018.png)

接下来设计下一层，这里是输出层

![image-20210115202444793](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210115202444793.png)

相应的我们完成关于假设函数的预测值$h_{\theta}(x)$的真值表，预测值是我们对于逻辑运算表达式$x_1{\,}XNOR{\,}x_2$关于输入$x_1, x_2$的真值表，是已知的标签。

![image-20210115203529670](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210115203529670.png)

通过上述实现逻辑运算表达式$x_1{\,}XNOR{\,}x_2$我们可以发现，设计复杂的非线性假设模型对应的神经网络，我们可以采用“分治”法，每一层的神经元对应一个逻辑表达式，且由上一层的逻辑表达式通过特定的逻辑运算获得，这些逻辑运算都能通过设置$\theta_{ij}^w$来实现神经网络，其中i 指向$s_{w+1}$层的神经元编号，j 指向$s_w$层的神经元编号。隐藏层的神经单元我们通常理解为中间特征。

抽象来说，多层的神经网络每次进入下一层它都能在上一层对应的相对简单的函数基础之上计算更加复杂的方程或函数或称为特征，最后这些“中间”特征被用于逻辑回归分类器的最后一层，也就是输出层来准确的预测出$h_{\theta}(x)$，这也就是神经网络能用来学习复杂的假设模型的原因。

### 多元分类

在神经网络中实现多类别分类，采用的方法本质上是一对多法的拓展，输出层对应分类的类别个数设置相同个数的输出神经元取值为$\{0, 1\}$，那么输出层即$h_{\theta}(x)$应该表示为N 维列向量，其中N 表示类别个数，同样，我们构建的训练集中输入特征样本$x^{(i)}$的标签$y^{(i)}$也表示为N 维列向量

### 代价函数

假设我们有一个与下图类似的神经网络结构和训练集$\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}$，其中有m 组训练样本$(x^{(i)},y^{(i)})$，然后定义$L$ 来表示这个神经网络结构的总层数，所以对于下图的神经网络结构$L=4$，$s_l$表示第$l$ 层的单元数，也就是神经元的数量，这其中不包括第$l$ 层的偏置单元

这里我们选择逻辑回归模型正则化的代价函数：
$$
J(\theta)=-\frac{1}{m}\left(\sum_{i=1}^my^{(i)}log\,h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))\right)+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2
$$
其中m 表示训练集样本数量，n 表示特征数量；对应的我们给出神经网络的代价函数表达式
$$
J(\theta)=-\frac{1}{m}\left(\sum_{i=1}^m{\sum_{k=1}^Ky_k^{(i)}log(h_{\theta}(x^{(i)}))_k}+(1-y_k^{(i)})log(1-(h_{\theta}(x^{(i)}))_k)\right)\\+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\theta_{ji}^{(l)})^2
$$
第一部分的求和项表示的是K 个输出单元之和，也就是每一个逻辑回归算法的代价函数，然后按照四次输出的顺序依次把这些代价函数加起来；正则化部分（第二部分）的L-1 代表从$l=1$ 开始的序号的参数矩阵$\theta^{(l)}$总共有L-1 个；正则化部分中我们通常不取i = 0 的情况，因为在第$l$ 层计算下一层的激励项（使用Sigmoid 函数g）时，i = 0的单元对应的参数$\theta_{j0}^{(l)}$的项为$\theta_{j0}^{(l)}x_0$或$\theta_{j0}^{(l)}a_0$，而$x_0$或$a_0$作为第$l$ 层的偏置单元等于常数1，不需要正则化这一项来避免过拟合（当然，这只是一个合理化的约定，即使在计算过程中求和包含了i = 0 的项，也就是求和i 从0 加到$s_l$，整个代价函数的表达式依然有效，并且不会有大的区别）；另外，求和项应用于$y_k$和$h_k$是因为我们这个代价函数是将第K 个输出单元的值和$y_k$的值的大小作比较，而$y_k$的值就是我们在训练集中对每个训练样本设置的标签，表示其应属于哪个类的量

### 反向传播算法

对于神经网络的代价函数，我们要做的就是找到参数$\theta$ 使得$J(\theta)$取到最小值。为了使用梯度下降法或者其他某种高级优化算法，我们需要做的就是获得输入参数$\theta$ 并计算$J(\theta)$和偏导项，那么如何计算偏导项就是关键。

反向传播算法从直观上说就是对每一个节点我们计算第$l$ 层的第j 个节点的误差，用$\delta_j^{(l)}$表示，下面假设神经网络总层数为L，对于输出层的每个节点（神经元）有$\delta_j^{(L)}=a_j^{(L)}-y_j$，其中$a_j^{(L)}$作为输出层的单元也可以写成$h_\theta(x)$，即逻辑回归模型的预测值，所以$\delta_j^{(L)}$就是假设的输出值和训练集样本的标签y 值之间的差，表达式中$y_j$ 的下标j 就是训练集的向量y 的第j 个元素的值；当然，上述表达式可以向量化表示为$\delta^{(L)}=a^{(L)}-y$，这里的每一项都是一个向量，并且向量维数等于输出单元的个数。

下面是关于隐藏层的$\delta$项：$\delta^{(l)}=(\theta^{(l)})^T\delta^{(l+1)}.*g'(z^{(l)})$，其中$g'(z^{(l)})$表示激活函数Sigmoid 函数对$z^{(l)}$求导，结果为$g'(z^{(l)})=a^{(l)}.*(1-a^{(l)})$，因为表达式中$z^{(l)}$和$a^{(l)}$都是向量，所以是用点乘，1其实是与$a^{(l)}$相同的维度的列向量（这里谈谈对于这个表达式的理解，我们可以看作是对原式$a^{(l)}=g(z^{(l)})$两边求导，根据链式求导法则得到$d\,a=\frac{d\,g}{d\,z}.*d\,z$，这里的$d$ 表示求导）

如果不是特别严谨的情况下，我们可以证明我们要求的偏导数项$\frac{\partial}{\partial\theta_{ji}^{(l)}}J(\theta)=a_j^{(l)}\delta_i^{(l+1)}$，当然，这里我们忽略了正则化项*（Q: 是不是通过推导偏导数，过程中定义误差$\delta$，从而有了上面关于$\delta^{(l)}$的表达式？）*

现在将上述的两个公式应用于一个非常大的训练样本，假设我们有一个m 个样本的训练集$\{(x^{(1)},y^{(1)},...,(x^{(m)},y^{(m)}\}$，初始化一个变量（空矩阵）$\Delta_{ij}^{(l)}=0$，在之后的计算过程中可以理解为一个存放代价函数对$\theta$求偏导的临时结果的数据结构。接下来我们将遍历训练集，对于每次遍历都对应一个训练集样本$(x^{(k)},y^{(k)})$，并将这个样本的特征向量作为输入传入神经网络的输入层，即$a^{(1)}=x^{(k)}$，然后根据运用正向传播来计算下一层的激活值$a^{(2)},a^{(3)},...,a^{(L)}$，此时神经网络初步结构就已经完成了，根据误差$\delta^{(l)}$的定义输出层满足$\delta^{(L)}=a^{(L)}-y^{(k)}$，然后我们将通过反向传播算法计算$\delta^{(L-1)},\delta^{(L-2)},...,\delta^{(2)}$，这里默认第一层作为输入层没有误差，即$\delta^{(1)}=0$，并且需要注意的是，这里的变量都是多维列向量。最后，我们根据之前提到的代价函数偏导数的表达式$\frac{\partial}{\partial\theta_{ji}^{(l)}}J(\theta)=a_j^{(l)}\delta_i^{(l+1)}$从第2层开始到第L 层更新（累积）$\Delta_{ij}^{(l)}$，$\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}$，其中i 和j 分别遍历第$l$ 层和第$l+1$ 层的单元节点，所以类似的，我们可以将关于单元编号的i 和j 用向量化表示得到新的表达式$\Delta^{(l)}:=\Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^T$，这里的变量都是多维向量