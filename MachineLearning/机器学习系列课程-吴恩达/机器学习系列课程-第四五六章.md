### 多特征（多元）

标识如下：

- n 用来表示特征量的数目
- m 来表示样本的数量，或者说是训练样本数
- x^(i) 来表示第i 个训练样本的输入特征值，这里的i 其实就是训练集的索引，这样x^(i) 其实就表示一个n 维向量
- x^(i)_j 表示的是第i 个训练样本中第j 个特征来的值

线性回归模型中的假设函数在多特征条件下的表示如下：

![image-20210103112644416](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210103112644416.png)

为了表示方便，我们将x0 的值设为1，具体而言，意味着对于第i 个样本，都有一个向量x^(i) 并且x^(i)_0 等于1，可以理解为定义了一个额外的第0 个特征量，这样一来，特征向量x 是一个从0开始标记的n+1 维向量，参数theta 就能表示为theta0 到thetan 的n+1 维向量，上述表达式简化为：

![image-20210103113356056](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210103113356056.png)

### 多元梯度下降法

相比较之前在只有一个特征量的条件下的线性回归模型，多元梯度下降法表示如下：

![image-20210103114147831](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210103114147831.png)

梯度下降算法中的导数项部分表示如下：

![image-20210103114750375](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210103114750375.png)

这里j = 0, 1, 2, ..., n，其中x^(i)_0 为1

在2个特征量的问题中，通常theta 向量考虑(theta0, theta1, theta2)，以theta1 和theta2 为坐标轴可以画出代价函数J 在该平面上的等高线，相邻等高线，相隔距离越近，说明梯度越大，考虑theta(i) 对应的某个特征量的取值范围大，其在导数项的影响越大（从表达式中可以看出），所以与相应的坐标轴平行的方向的相邻等高线会更加密集。从每次参数向量theta 更新的图形角度解释就是“梯度下降的方向是等高线在某一点的法向量”，那么对于这种情况下的扁平椭圆上某一点的法向量并不是指向圆心，所以相比较等高线为圆的情况获取最优解的迭代速度更慢。一般情况下，我们执行特征缩放时，会将特征的取值约束到-1到+1的范围内，当然，也存在如果某些特征量有现实意义或语义的话有固定范围，我们也会尽量让特征向量的每个分量的取值范围接近，避免出现上述情况，优化梯度下降算法。

这里介绍特征缩放方法，除了将特征除以最大值的方法以外，通常还有均值归一化方法，将原始特征减去平均值后除以（最大值减去最小值）特征量的取值范围，这样新的特征量的取值范围就可以接近[-0.5, 0.5]

如果代价函数随着迭代次数增加不断变大而不是减小的话，那就需要考虑减小学习率alpha。事实上，在适用于线性回归模型的代价函数的基本假设下，已经证明只要学习率alpha 足够小，那么每次迭代之后代价函数J(theta) 都会下降；当然，在设置学习率的时候也不能过小，因为那样的话梯度下降算法可能收敛的很慢。我们可以设置一个规则例如挑选alpha 为0.001，0.01，0.1，1，...

在挑选特征时，如果两个特征的共线性太强，严格来说跑不了回归，当然，如果采用运算合并多个特征或者一个特征（幂乘，对数等）形成新的特征，也会丢失特征信息，例如对于长，宽两个特征，可能选取新特征--面积（长*宽），但是这样就会丢失样本对象的“形状”信息，我们通常将这个方法称为“特征衍生”，这种方式不限于线性回归模型，还可用于非线性回归拟合

### 正规方程（区别于迭代方法的直接解法）

对于参数向量theta 是一个n+1 维向量，代价函数J(theta)，我们逐个对参数theta(j) 求J 的偏导数，然后把它们全部置零

这里我们给出参数theta 的表达式

![image-20210103171109141](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210103171109141.png)

注意的点就是原表达式应该是

![image-20210103171147367](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210103171147367.png)

其中矩阵X 的转置乘以X 就变成了一个满秩的方阵，具有逆矩阵。这个方程是求最佳近似向量，作为结果的参数向量theta 为n+1 维向量，为什么说是“最佳近似向量”，因为众所周知，m 个等式只能解m 个参数的方程，我们通过这个等式推导出来的参数向量theta 只是**其中一个满足要求的解**

使用正规方程的方法相比较于梯度下降算法的迭代避免了特征缩放，也不需要人工选择学习率alpha 以及之后的多次迭代，但是，由于需要计算矩阵X 的转置和X 相乘（n x n 矩阵）后的逆矩阵（对大多数计算应用来说，实现逆矩阵计算的代价以矩阵维度的三次方增长），这个在特征数目n 很大的时候会很缓慢，而梯度下降算法却能表现正常

如果矩阵X(X^T) 不可逆的，通常有两种最常见的原因，第一个原因是如果由于某些原因原问题中包含了多余的特征（例如，在预测住房价格时，如果x1 是以平凡英尺为单位的房子面积，x2 是以平方米为单位的房子面积，因为1米等于3.28英尺，这两个特征值将始终满足x1 = (3.28^2) * x2，那么这两个特征是不可以构成上面的线性方程组的，这样的话X(X^T) 是不可逆的；第二个原因是如果学习算法有很多特征，也就是m <= n，这样线性方程构成的矩阵是不满秩的，有可能X(X^T) 不可逆

总结来说就是有几个列向量线性相关，而python 或者octave 的工具库中的函数pinv 之所以可以求解逆矩阵是因为任何矩阵都有满秩分解，这里涉及广义逆

### 二元分类问题和线性回归

首先看看二元分类问题的性质，对于二元分类问题，我们尝试预测的变量y 都是可以有两个取值的变量，下面我们将分类问题的y 值可能取值假设为0或1

如果使用线性回归，假设的输出值会远大于1或者小于0，那么如果对二元分类问题使用线性回归，即使所有训练样本的标签都是y 等于0或1，线性回归模型的输出值都可能远大于1或者远小于0，原因在于线性回归算法的直线的特征变量默认为无限制的，实数域的（这个在梯度下降算法的更新表达式中往往体现在较大的导数项，意味着可能每一次更新的步长会过大，为了解决这个问题我们需要调整更小的学习率alpha，这对于预测值h_theta(x) 在0 到1 附近的样本往往更新的步长过小，影响算法性能）

当然，因为线性回归模型中h_theta(x) 值域从负无穷到正无穷的性质，符合下面将介绍的Sigmoid 函数的定义域，我们将用Sigmoid 函数完成分类器的优化

### 假设表示（假设陈述）

假设陈述也就是，当有一个分类问题的时候，如何选择要使用哪个方程来表示我们的假设

Logistic 回归模型的假设函数表示如下：

![image-20210107171423271](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210107171423271.png)

其中g(z) 又称为"Sigmoid 函数"或者"Logistic 函数"，该函数的图像如下图

![image-20210107171551450](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210107171551450.png)

描述起来就是，一开始接近0，然后上升，直到在原点处达到0.5，之后再次变平。可以看出Sigmoid 函数渐近于1，另一端趋向于0，其中横轴表示z，随着z 趋向于负无穷，g(z) 趋向于0，当z 趋于无穷大时，g(z) 接近于1

我们的Logistic 回归模型中假设函数的输出就变成了“对于输入的样本特征值输出值y 等于1 的概率估计”。用概率表示

![image-20210107172501448](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210107172501448.png)

从Sigmoid 函数的图像可以看出，如果我们要预测y = 1，只需要(theta)^T X（在Sigmoid 函数中就是自变量z）大于或等于0，另一方面，我们将预测y = 0，只需要(theta)^T X 小于0，当然，对于Logistic 回归模型中h(x) 的值如果正好等于0.5，其实就是决策边界，可以预测为正类，也可以是负类。这里(theta)^T X = 0 这条直线就是假设函数的决策边界。

决策边界是假设函数的一个属性，决定于其参数，而不是训练集的属性，与有没有训练集样本无关。准确来说，我们用训练集来拟合参数theta，而参数向量theta 确定了决策边界

### Logistic 回归模型

首先我们来定义一个概念Cost 项：

![image-20210107204443154](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210107204443154.png)

在线性回归模型中我们的代价函数通过下面的定义简化：

![image-20210107204714733](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210107204714733.png)

但是在Logistic 回归模型中使用这个代价函数，因为对于Logistic 回归来说，这里的h_theta(x) 函数是Sigmoid 函数，是非线性的，将这个函数代入到Cost 项中并最终代入到 J(theta)，这个代价函数会变成参数向量theta 的非凸函数，如果画出 J(theta) 的图像，就会发现可能是一个具有很多局部最优值的函数，称呼这个函数的正式术语就是**非凸函数**，对于这个函数，从图像分析，如果将梯度下降法用在这样一个函数上，不能保证它会收敛到全局最小值。不利于我们通过梯度下降法推导出合适的参数向量theta。

如果代价函数 J(theta) 是一个凸函数，是一个单弓形函数，当我们对它使用梯度下降法时，我们就可以保证梯度下降法会收敛到该函数的全局最小值。

所以下面我们将定义一个新的Cost 项：

![image-20210107210214886](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210107210214886.png)

从原理角度推导：（关于这步定义是如何推导的，这里记一个解释：根据伯努利分布，最大化似然概率，等价于最小化对数似然，而最小对数似然就是这个表达式）

从性能角度推导：关于性能方面，首先我们线性回归模型中的假设函数h_theta(x) 是表示为(theta)^T X，即两个矩阵的乘积，在分类问题中，尤其是二分类问题中我们通过Sigmoid 函数将值域从负无穷到正无穷变化为0 到1，但由于Sigmoid 函数对参数向量theta 是非线性的，所以原来的Cost 项在使用梯度下降法求全局最小（最小目标或最小代价函数）时往往是非凸函数无法准确收敛到全局最小，新的Cost 项中对数函数可以把指数变为线性，去掉了指数化带来的影响。

如果以假设函数h(X) 为自变量，Cost 项函数值为因变量画出图像，这个Cost 项函数有一些有趣而且很好的性质，首先，可以注意到如果y = 1 而且h(X) = 1，也就是说，如果假设函数预测值是1 而且y 刚好等于预测的，那么这个代价值等于0，接下来，注意到如果反过来h(X) = 1 也就是预测y = 1（这里是假设函数在Logistic 回归模型中的定义，即h(X) = 1 表示预测y = 1 的概率为1） 并且如果y 确实等于1，那么代价值等于0，因为我们正确预测了；同样注意到h(X) 趋于0 时，也就是当假设函数的输出趋于0时，代价值激增并且趋于无穷，换句话说就是假设函数输出0 代表假设函数说y = 1 的概率等于0，如果y 最终等于1，那么我们的预测就是错误的，要用非常非常大的代价值“惩罚”这个学习算法，所以就会在图像中表示为这个代价值Cost 项的值趋于无穷。

同理可分析y = 0 的情况。

（Q: 那是不是只要满足这样的性质就能作为Logistic 回归模型的代价函数呢？即必要性和充分性）

接下来我们将要简化代价函数，将Cost 项的分段函数合并成一个等式，这将使我们更方便的写出代价函数

![image-20210108194821672](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210108194821672.png)

在对Logistic 回归模型使用梯度下降法时，更新的表达式通过计算偏导数得到如下

![image-20210108195813674](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210108195813674.png)

虽然会发现上面的表达式和之前线性回归模型使用梯度下降法的更新步长的表达式结构相同，但是需要注意的是，两者的不同在于假设函数h_theta(x)，线性回归模型和Logistic 回归模型分别对应h1 和h2

![image-20210108200232115](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20210108200232115.png)

此外，细心点会注意到Logistic 回归模型的更新步长的表达式中少了1/m，其实对于实数1/m 我们默认为将其合并到学习率alpha 中对模型没有影响。

### 高级优化

- 共轭梯度法
- BFGS
- L-BFGS

上述三种算法的优点：

- 使用其中任何一个算法，通常不需要手动选择学习率alpha。这些算法具备一个智能内循环称为线搜索算法，它可以自动尝试不同的学习速率alpha 并自动选择一个较好的学习速率alpha，甚至可以为每次迭代选择不同的学习速率，就可以避免手动选择。
- 实际上，这三个算法的收敛远远快于梯度下降

缺点就是它们比梯度下降法复杂很多

### 多元分类：一对多

原理介绍：将训练集转化为多个独立的二元分类问题

这里关于“为什么不能在第一个基础上再次分类” 的问题，从统计学的角度来分析，关于条件概率的计算，不能再次分类是因为并不能100% 确定某一个二元分类模型的假设函数的输出值即关于预测值y 属于某个分类的事件发生的概率，所以需要先求出每个二元分类的概率在判断。

总结下来就是，我们训练一个逻辑回归分类器h_theta(x)(i) 预测类别i y = i 的概率，为了作出预测，我们给出一个新的输入值x，期望获得预测，要做的就是在k 个分类器运行输入x，然后选择h 最大的类别，也就是要选择分类器，选择出k 个中可信度最高，效果最好的那个分类器i0（这里的k 表示原问题中标签y 的可能值个数），而这个对应最高的概率值的分类器i0，我们预测y = i0