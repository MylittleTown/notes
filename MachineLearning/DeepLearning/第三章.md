## 目标检测

除了图像目标识别，还需要在原始图像中找到目标位置$(b_x,b_y,b_h,b_w)$，分别代表目标所在方框的中心点坐标，方框的高度和宽度。其中，通常以图像左上角为坐标系(0, 0)，右下角为(1, 1)。

假设目标检测模型（假定输入图像中只有一个对象）需要检测的类别数量为m，那么输出标签y 通常为(m + 5)*1 特征向量，包含$p_c$表示图像中含有目标的概率；$(b_x,b_y,b_h,b_w)$对应如果存在目标的话，该目标所在方框的中心点的位置，宽度和高度；布尔值$(c_1,c_2,\cdots,c_m)$对应每个类别判定为真，或假

综上所述，对于目标检测模型的损失函数可以表示为如下：
$$
\zeta(\hat{y},y)=\begin{cases}
(\hat{y_1}-y_1)^2+(\hat{y_2}-y_2)^2
+\cdots+(\hat{y_m}-y_m)^2&if&y_1=1\\
(\hat{y_1}-y_1)^2& if&y_1=0
\end{cases}
$$
通常我们可以使用卷积避免在滑动窗口过程中的重复计算步骤，减少计算成本，但是这样依旧避免不了边界框位置无法精确确定的问题。所以引入YOLO 算法。

YOLO 算法的思路是将输入图像分割成多个相同的小区域，并在每个单元格中应用目标检测算法判定是否存在目标，对应每个目标类别的概率，在遍历所有单元格后根据Intersection over union(IOU，交并比) 将相同类别且相邻的区域合并归纳为目标边界框。

这里解释下什么是交并比，在训练模型的过程中，我们通常会已知检测对象以及对应的边界框，因此通过YOLO 算法获得的新边界框有可能会与标注的边界框的区域存在交集和并集，这时IOU就可以表示为：$IOU=\frac{size\_of\_intersection}{size\_of\_union}$，最好的情况就是训练模型得出的边界框和标注边界框重合，IOU = 1；或者说当IOU > 0.5 时我们也认为模型得出的边界框效果不错。当然，0.5 这个标准是人为定的，如果提高这个标准，那么我们模型得出的边界框就会越与标注的边界框重合（这个结果对吴恩达老师《深度学习-卷积神经网络》中YOLO 算法的示例和本文上面提到的YOLO 算法的基本思路都是可行的）

对于目标检测问题中原始图像存在多个检测对象，根据IOU 方法将会获得很多个交并比大于阈值的“合适”的边界框，为了正确的检测图像中所有的对象，这里不能单纯的取最大IOU，而是采用非极大值抑制的方法获得局部最优。具体思路如下：

- 丢弃所有IOU 小于阈值的边界框
- 如果还有边界框，使用while 循环遍历它们
  - 选取当前所有边界框中IOU 值最大的边界框
  - 丢弃所有和上一步的边界框的IOU 值超过0.5 的边界框

目前学习到的YOLO 算法只针对分割出来的单元格中只包含了一个检测对象，输出结果$y=\left[\begin{matrix}p_c\\b_x\\b_y\\b_h\\b_w\\c_1\\\vdots\\c_m\end{matrix}\right]$，这里输出向量中只能针对$\{c_1,c_2,\cdots,c_m\}$中某一种类别判断检测对象属于该类别的概率$p_c$，为了满足单元格内可能出现多个检测对象的目标检测算法的目的，引入“锚点框”

目前了解到的锚点框（**建议阅读相关论文**），指的是对于只有两个检测对象的单元格，采用$y'=\left[\begin{matrix}y_1^{(0)}\\y_2^{(1)}\\\vdots\\y_n^{(0)}\end{matrix}\right]$，这里右上标表示的是对象存在与否，n 表示使用的锚点框数量。在训练过程中，已知检测对象的边界框，和该边界框IOU 值更大的锚点框对应的$y_i^{(j)}(j=0,1)$将决定该检测对象的类别

然后，“锚点框”也是存在局限性的，例如同一单元格内存在三个及以上的检测对象或者两个检测对象的预测的边界框IOU 值较大，也就是说几乎重叠。事实上，如果YOLO 算法中对原始图像的分割数量越大，单元格区域越小，那么两个及以上检测对象的边界框中心出现在同一单元格内的可能性也就越低，这样就可以避免使用“锚点框”



在滑动窗口的基础上，提出的Region proposal（候选区域）能够采用更少且更加有效的窗口完成目标检测，而不是遍历所有滑动窗口。这涉及到了“切割算法”，将原始图像中潜在区域对应的边界框挑选（聚类）出来用于目标检测。基于候选区域的卷积网络称为“R-CNN”。该方法的优化重点也是目前的缺陷在于挑选候选区域的算法的速度



对目标检测问题的进一步发展是，对于输入图像，能输出图像中所有象素对应的物体的轮廓，而不仅限于生成边界框，这里涉及到了“语义分割”。已知卷积层和池化层输出的图像尺寸小于输入图像，那么想要完成“语义分割”的目标获得原始图像中每个像素点对应的类别，就需要转置卷积将卷积层和池化层输出的图像尺寸放大回原始尺寸。下面介绍什么是“转置卷积”

同卷积过程类似，除了过滤器尺寸f，还有输入图像的尺寸$(n_w,n_h)$，步长stride，填充数量p 以及转置卷积输出结果尺寸$(n_w',n_h')$。首先按照填充数量对输出图像尺寸进行放大，遍历输入图像的每个象素，并将取出的象素和过滤器中每个值相乘，计算结果按照对应位置填入输出图像，循环上述步骤，并保证下一次对应的输出图像的位置步长为设定值stride，如果stride < f，这时遍历过程中相邻两次得到的输出区域存在重叠，这时将重叠部分的值相加，求和结果填入对应位置成为新的输出值。

通过转置卷积，我们可以将少量输入，转化成更大量的输出

那么U-Net 卷积网络中如何优化？事实证明，在基础的U-Net 卷积网络中，跳过从较早的卷积层到较晚的层的连接，也就是将较早的层的激活块直接复制到较晚的层与其相连接，可以使其工作的更好。这个可以理解为，在高层次（较晚）的卷积层中包含了很多抽象的特征，但是在卷积网络不断卷积、池化的过程中不断损失了原始的图片信息，通过将早期的图像复制过来能让较晚的层既包含了原始图片的初级信息，又包含了卷积得到的高层次信息。