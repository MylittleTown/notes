在计算机视觉文献中，作为边界检测器的过滤器（这里以3 * 3 为例）除了如下定义外，还有几种：Sobel Filter 以及Scharr Filter。

其中标准过滤器保证边界两边的差异尽量大，因此选用1，-1，而对于边界处的数据（或者希望忽略影响的数据）尽可能减小对卷积过程的影响，有如下结构：
$$
\begin{matrix}
1&0&-1\\
1&0&-1\\
1&0&-1
\end{matrix}
$$
而Sobel Filter 的结构中，它给中间行赋予了更大的权重，从而可能使得它更加稳定，如下：
$$
\begin{matrix}
1&0&-1\\
2&0&-2\\
1&0&-1
\end{matrix}
$$
还有Scharr Filter 的结构：
$$
\begin{matrix}
3&0&-3\\
10&0&-10\\
3&0&-3
\end{matrix}
$$
事实上，在通过卷积神经网络进行计算机视觉问题的建模过程中，很多时候都是通过反向传播算法计算得出类似3 * 3 过滤器的9 个参数

上面列举的三个过滤器都是垂直边界过滤器，相应的还有不同角度斜角的边界检测过滤器。

## 1. padding（填充）

以6 * 6 的原始图像输入为例，步长设置为1，过滤器大小为3 * 3，那么最终得到的输出是4 * 4 的矩阵，这是因为3 * 3 过滤器可能放置的位置的数目是4 * 4，事实上，为了适应原始输入的6 * 6 的矩阵，3 * 3 的过滤器只有4 * 4 可能的位置，这其中通用的公式是：

假设图片大小为n * n，想使用f * f 的过滤器，这样输出的维度将会是$(n - f + 1)*(n - f + 1)$

我们会发现每一次使用一次卷积操作，输出的图像都会缩小，且速度很快，导致很有可能无法完成很多次卷积操作；另外，分析卷积过程，图片角落或者边际上的像素点只会在输出中被使用一次，因为它只通过过滤器一次，相比较而言，在图像中间的象素，会有很多过滤器在那个象素上重叠，所以相对而言，角落或者边界上的象素被使用的次数少很多，因此我们通常会丢失许多图片上靠近边界的信息。

为了解决上述两个问题，常用的解决办法是在每次卷积操作前，填充（padding）图片。

例如通过在原始输入的6 * 6 的图像额外的边缘填充图片，额外边缘的大小为1 个象素，这时输入已经变成了一个通过填充得到的8 * 8 的图片，按照上述输出图像大小的通用公式计算可以得到，这时通过3 * 3 的过滤器计算得到的输出图像大小依旧为6 * 6，保持了图片的原始大小。

通常填充的象素使用0，在上面的例子中填充的数量，即额外边缘的大小为1 象素，我们用p 表示。这时输出图像大小的通用公式为$(n + 2p -f + 1)*(n + 2p - f + 1)$

填充的数量通常有两个常见的选择，即valid 卷积和same 卷积

其中valid 卷积表示没有填充，在这种情况下，当我们用一个f * f 的过滤器去卷积一个n * n 的图片时，会得到一个$(n - f + 1)*(n - f + 1)$维度的输出 

另一个常用的填充方式的选择是same 卷积，表示在卷积操作前填充图片，使输出大小等于输入大小，即$n + 2p -f + 1 = n \Rightarrow p = \frac{f - 1}{2}$，当然这也表示，当f 为奇数的时候，通过设置合理的填充数量可以保证输出和输入的大小相同

## 2. Stride（步长）

考虑输入图像大小为n * n，采用f * f 大小的过滤器，对图像使用p 层填充，假设步长为S。输出图像大小的通用公式可以表示为$\lfloor\frac{n + 2p - f}{S} + 1\rfloor*\lfloor\frac{n + 2p - f}{S} + 1\rfloor$



单层卷积神经网络结构和计算公式：

假设l 层表示一个卷积层，其中$f^{[l]}$表示过滤器大小，$p^{[l]}$表示l 层的填充数量，$s^{[l]}$表示l 层输入图片使用过滤器做卷积时使用的步长，$n_c^{[l]}$表示l 层的过滤器数量，也可以指示l 层的图像深度

又假设l 层输入图像的大小为$n_H^{[l-1]}*n_W^{[l-1]}*n_c^{[l-1]}$，输出图像大小表示为$n_H^{[l]}*n_W^{[l]}*n_c^{[l]}$。根据之前对单层卷积操作输出图像大小计算的通用公式有
$$
n_H^{[l]}=\lfloor\frac{n_H^{[l-1]} + 2p^{[l]} - f^{[l]}}{S^{[l]}} + 1\rfloor\\
n_W^{[l]}=\lfloor\frac{n_W^{[l-1]} + 2p^{[l]} - f^{[l]}}{S^{[l]}} + 1\rfloor
$$
l 层每个过滤器的大小为$f^{[l]}*f^{[l]}*n_c^{[l-1]}$；通过ReLU非线性激励函数得到的输出图像大小为$n_H^{[l]}*n_W^{[l]}*n_c^{[l]}$；类比神经网络，l 层的所有过滤器组成的权重矩阵数据大小为$f^{[l]}*f^{[l]}*n_c^{[l-1]}*n_c^{[l]}$，其中$n_c^{[l]}$对应l 层输出图像的深度；l 层的偏差项数据量大小为$n_c^{[l]}$



除了卷积层之外，卷积神经网络还包含了池化层，以此来提高计通过减小输入的大小来降低输出值的数量，**池化层输出深度为1 的数据（有待考证）**，提高计算速度，并使得一些特征的检测功能更加强大

列举几种池化方法：max pooling，average pooling

以下面的矩阵为例，输出2 * 2 的矩阵

$\begin{matrix}
1&3&2&1\\
2&9&1&1\\
1&3&2&3\\
5&6&1&2
\end{matrix} \Rightarrow \begin{matrix}9&2\\6&3\end{matrix}$

max pooling 池化方法将原始输入的矩阵采用过滤器其中$f = 2, s = 2$，并在每个部分中选取最大的值从而得到输出的2 * 2 矩阵中对应位置的值。

类似的average pooling 池化方法对于每个过滤器选取的部分计算平均值

总结来说，池化过程相当于下采样，降低输入冗余数据，减小参数，有效减少后续层所需要的参数；另一方面，对于输入图像，当其中的象素在邻域发生微小位移时，池化层的输出是不变的，使得网络的鲁棒性增强了，有一定的抗干扰的作用。



对于卷积网络中的最后一层，通过softmax 函数对数据进行分类（softmax 层的节点个数即分类问题的类别数量，感觉像是将最后一层通过softmax 函数转换成了每个类别的概率），计算方法是：

假设有一个数组V，$V_i$表示V 中第i 个元素，那么这个元素的softmax 值是

$S_i=\frac{e^{V_i}}{\sum_je^{V_j}}$，其中$(i\leqslant j)$，也就是说，softmax 就是计算该元素的指数与所有元素的指数和之比



卷积网路和只用完全连接的神经层相比有两个优势：参数共享和连接的稀疏性

参数共享是源于在特征检测器中，例如垂直边缘检测对于图像的一部分是有用的，那么对于另一部分可能也是有用的，换言之，特征在不同位置可能是相似的；以数据集为例，两个不同分布的数据集，也可能足够的相似，所以可以共用相同的特征探测器，效果是一样的

连接的稀疏性体现在卷积网络通过建立稀疏的联系避免只有相对较少的参数。也就是，对于输出矩阵中的元素，都只对应输入矩阵中满足过滤器大小的那一部分的数据，其他原始输入的数据对这个输出的象素没有影响，反过来，其他输入的数据对不属于它过滤器选取的位置的其他输出没有任何的影响。

