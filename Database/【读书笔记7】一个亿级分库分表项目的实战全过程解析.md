### 前言

首先了解一下为什么要分库分表？海量数据的存储和访问成为了MySQL 数据库的瓶颈问题，日益增长的业务数据，无疑对MySQL 数据库造成了相当大的负载，同时对于系统稳定性和扩展性提出了很高的要求。

而且单台服务器的资源（CPU，磁盘，内存等）总是有限的，最终数据库所能承载的数据量，数据处理能力都将遭遇瓶颈。

目前来说一般有两种方案：

- 一种是更换存储，不使用MySQL，比如可以使用HBase，polarDB，TiDB 等分布式存储
- 如果出于各种原因考虑，还是想继续使用MySQL，一般会采用第二种方式，那就是分库分表

下面的内容将专注于梳理分库分表从架构设计到发布上线的完整过程，同时总结其中的注意事项和最佳实践。包括五个部分：

- 业务重构
- 存储架构设计
- 改造上线
- 稳定性保障
- 项目管理

### 第一阶段：业务重构

本次项目的第一大难点，在于业务重构。

而本次拆分项目涉及到的两张大表A 和B，单表将近八千万的数据，是从单体应用时代遗留下来的，从一开始就没有很好的领域驱动/ MSA（微服务）架构设计，逻辑发散非常严重，到现在已经涉及50+ 个在线服务和20+ 个离线业务的直接读写。

微服务是一种全新的互联网架构，它的基本理念是将一个肥大的系统拆分成若干个小的服务组件，组件之间的通讯采用轻量的协议完成，譬如REST API。微服务本质上是SOA（Serviceoriented Architecture）的扩展延伸。相对来说，微服务的可操作性更强，可以逐步安排合理资源，对一个大的系统进行分解，或是至少停止让它继续臃肿下去。

![微服务架构](https://raw.githubusercontent.com/MylittleTown/notes/master/Linked_pictures/%E7%AC%94%E8%AE%B07-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84.png)

*（Q: 这里的DB 采用相同的逻辑结构吗？如果每个服务对应的DB 的逻辑结构不同，那之后关于联合查询的拆分分析还有必要将冗余，融合和废弃结果通知所有业务方吗，还是只需要通知对应的业务方？）*

微服务具有如下优点：

- 按照业务功能的独立垂直开发，易于开发，理解和维护
- 支持异构开发语言，不会受限于任何技术栈
- 部署周期短，自动化部署
- 局部修改很容易部署，有利于持续集成和持续交付
- 故障隔离，一个服务出现问题不会影响整个应用

单体应用通俗的讲，就是将应用程序的所有功能都打包成一个独立的单元，可以是JAR, WAR, EAR 或者其他归档格式。单体应用有如下优点：

- 适合小团队创业初期进行快速开发
- 易于测试：因为没有额外的依赖，每项测试都可以在部署完成后立刻开始
- 部署简单：只需要将单个归档文件复制到Tomcat webapps 目录下
- 不存在分布式事务问题

但是，不管如何模块化，单体应用最终都会因为团队壮大，接入应用越来越多等出现问题。主要体现如下方面：

- **不够灵活**：对应用程序做任何细微的修改都需要将整个应用程序重新构建，重新部署。开发人员需要等到整个应用程序部署完成后才能看到变化*（Q: 相比较而言，是不是微服务架构中的小服务部件都有各自的存储单元和处理单元，所以一个业务的扩展或者修改不会对另一个服务本身的逻辑造成影响，但是比单体应用多了一项关于接口方面的测试）*。如果多个开发人员共同开发一个应用程序，那么还要等待其他开发人员完成了各自的开发。这降低了团队的灵活性和功能交付频率。
- **妨碍持续交付**：单体应用可能会比较大，构建和部署时间也相应的比较长，不利于频繁部署，阻碍持续交付
- **受技术栈限制**：对于这类应用，技术是在开发之前经过慎重评估后选定的，每个团队都必须使用相同的开发语言
- 某个服务出现OOM（Out of Memory）后对整体应用产生影响

因此，如何保证业务改造的彻底性，全面性是重中之重，不能出现有遗漏的情况。另外，表A 和表B 各自有二三十个字段，两表的主键存在一一对应关系，因此，本次分库分表项目中，还需要将两个表进行重构融合，将多余/ 无用的字段剔除。

1. 查询统计

   在线业务通过分布式链路追踪系统进行查询，按照表名作为查询条件，然后按照服务维度进行聚合，找到所有相关服务，写一个文档记录相关团队和服务。

   这里特别注意下，很多不是只有在线应用在使用，很多离线算法和数据分析的业务也在使用，这里需要一并的处理好，做好线下跨团队的沟通和调研工作，以免切换后影响正常的数据分析。

2. 查询拆分与迁移

   创建一个jar 包，根据查询统计结果，与服务owner 合作将服务中的相关查询都迁移到这个jar 包中（本项目的jar 包叫做projectdb，之后也将用这个名字介绍）。此处为1.0.0 版本。

   然后将原本服务内的xxxMapper.xxxMethod() 全部改成projectdb.xxxMethod() 进行调用。

   这样做有两个好处：

   - 方便做后续的查询拆分分析
   - 方便后续直接将jar 包中的查询替换为改造后 中台服务 的rpc 调用，业务方只需升级jar 包版本，即可快速从sql 调用改为rpc 查询

   rpc（Remote Procedure Call） -- 远程过程调用，是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。采用客户机/服务器模式。请求程序就是一个客户机，而服务提供程序就是一个服务器。

   这一步骤相当于是按照业务方将各个查询服务分组，务必梳理各个服务作全面的迁移，不能遗漏，否则可能会导致拆分分析不全面，遗漏了相关字段。由于本次拆分项目涉及到的服务太多，需要收拢到一个jar 包，更方便后期的改造。如果实际分库分表项目中仅仅涉及一两个服务的，这一步是可以不做的。

3. 联合查询的拆分分析

   根据上述收拢的jar 包中的查询，结合实际情况将查询进行分类和判断，把一些历史遗留的问题，和已经废弃的字段做一些整理。

   整理过程可以从以下几个点思考：

   - 哪些查询是无法拆分的？例如分页查询（尽可能的改造，实在改不了的只能以冗余列的形式）
   - 哪些查询是可以业务上join 拆分的？
   - 哪些表/字段是可以融合的？
   - 哪些字段是冗余的？
   - 哪些字段可以直接废弃了？
   - 根据业务具体场景和sql 整体统计，识别关键的分表键。其余查询走搜索平台

   思考后得到一个查询改造总体思路和方案。同时在本项目中需要将两张表融合为一张表，废弃冗余字段和无效字段。

4. 新表设计

   这一步基于上一步联合查询的拆分分析，得出旧表融合，冗余，废弃字段的结果，设计新表的字段。

   产生新表设计结构后，必须发给各个相关业务方进行review，并保证所有业务方都通过该表的设计。有必要的话可以进行一次线下review。

   如果新表的过程中，对部分字段进行了废弃，必须通知所有业务方进行确认*（Q: 这里有关“通知所有业务方进行确认”，前提是所有业务方使用的DB 的逻辑结构或者关系模式是相同的吧，如果不同，是不是不用通知所有？）*。对于新表的设计，除了字段的梳理，也需要根据查询，重新设计，优化索引。

5. 第一次升级

   新表设计完成后，先做一次jar 包内sql 查询的改造，将旧的字段全部更新为新表的字段。此处为2.0.0 版本。然后让所有服务升级jar 包版本，以此来保证这些废弃字段确实不使用了，新的表结构字段能够完全覆盖过去的业务场景。

   特别注意的是，由于涉及服务众多，可以将服务按照非核心 与 核心的区别，然后分批次上线，避免出现问题导致严重故障或者大范围回滚。

6. 最佳实践

   - **尽量不改变原表的字段名称**：在做新表融合的时候，一开始只是简单归并表A 和表B 的表，因此很多字段名相同的字段做了重命名。后来字段精简过程中，删除了很多重复字段，但是没有将重命名的字段改回来。导致后期上线的过程中，不可避免的需要业务方进行重构字段名。因此，新表设计的时候，除非逼不得已，不要修改原表的字段名称。*（Q: 这里关于由于重复字段名导致的重命名有什么方法避免吗？）*
   - **新表的索引需要仔细斟酌**：新表的索引不能简单照搬旧表，而是需要根据查询拆分分析后，重新设计。尤其是一些字段的融合后，可能可以归并一些索引，或者设计一些更高性能的索引。

### 第二阶段：存储架构设计（核心）

对于任何分库分表的项目，存储架构的设计都是最核心的部分。

1. 整体架构

   根据第一阶段整理的查询梳理结果，总结了这样的查询规律：

   - 80% 以上的查询都是通过或者带有字段pk1，字段pk2，字段pk3这三个维度进行查询的，其中pk1 和pk2 由于历史原因存在一一对应关系
   - 20% 的查询千奇百怪，包括模糊查询，其他字段查询等等

   因此，设计了如下的整体架构，引入了数据库中间件，数据同步工具，搜索引擎（阿里云opensearch/ES）等。

   ![微服务的存储架构设计](https://raw.githubusercontent.com/MylittleTown/notes/master/Linked_pictures/%E7%AC%94%E8%AE%B07-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1.png)

   **MySQL分表存储**

   MySQL 分表的维度是根据查询拆分分析的结果确定的。我们发现pk1/pk2/pk3 可以覆盖80% 以上的主要查询。让这些查询根据分表键直接走MySQL 数据库即可。

   分表键即 **分库/分表字段**，是在水平拆分过程中用于生成拆分规则的数据表字段。数据表拆分的首要原则，就是要尽可能找到数据表中的数据在业务逻辑上的主体，并确定大部分（或核心的）数据库操作都是围绕这个主体的数据进行，然后可使用该主体对应的字段作为分表键，进行分库分表。

   业务逻辑上的主体通常是与业务的应用场景相关的某一个单独的字段（也存在业务必须要有多个分表键，没有办法归一成一个，存在主分表键，即主维度，在主维度上的数据能够增删查改；辅助分表键，辅助维度，在辅助维度上只能进行数据查询），下面的一些典型应用场景都有明确的业务逻辑主体，可用于分表键：

   - 面向用户的互联网应用，都是围绕用户维度来做各种操作，那么业务逻辑主体就是用户，可使用用户对应的字段作为分表键；
   - 侧重于卖家的电商应用，都是围绕卖家维度来进行各种操作，那么业务逻辑主体就是卖家，可使用卖家对应的字段作为分表键；

   如果确实实在找不到合适的业务逻辑主体作为分表键，那么可以考虑下面的方法来选择分表键：

   - 根据数据分布和访问的均衡度来考虑分表键，尽量将数据表中的数据相对均匀的分布在不同的物理分库/分表中，适用于大量分析型查询的应用场景（查询并发度大部分能维持为1）
   - 按照数字（字符串）类型与时间类型字段相结合作为分表键，进行分库和分表，适用于日志检索类的应用场景

   注意：无论选择什么拆分键，采用何种拆分策略，都要注意拆分值（例如学生表中学号大于254作为拆分值）是否存在热点的问题，尽量规避热点数据来选择拆分键；不一定需要拿数据库主键当作分表键，也可以拿其他业务值当分表键。拿主键当分表键的好处是可以散列均衡，减少热点问题。

   原则上一般最多维护一个分表的全量数据，因为过多的全量数据会造成存储的浪费，数据同步的额外开销，更多的不确定性，不易扩展等问题。但是由于本项目pk1 和pk3 的查询语句都对实时性有比较高的要求，因此，维护了pk1 和pk3 作为分表键的两份全量数据。

   而pk2 和pk1 由于历史原因，存在一一对应关系，可以仅保留一份映射表即可，只存储pk1 和pk2 两个字段。

   **搜索平台索引存储**

   搜索平台索引，可以覆盖剩余20% 的零散查询。这些查询往往不是根据分表键进行的，或者是带有模糊查询要求。

   对于搜索平台来说，一般不存储全量数据（尤其是一些大varchar 字段），只存储主键和查询需要的索引字段，搜索得到结果后，根据主键去MySQL 存储中拿到需要的记录。（这里我想到了上一篇关于“从HBase 逆向迁移到Kafka” 的过程中，将HBase 存储的海量数据的RowKey 存储到HDFS 中方便拿取）

   当然，从后期实践结果来看，这里还是需要做一些权衡：

   - 有些非索引字段，如果不是很大，也可以冗余进来，类似覆盖索引，避免多一次SQL 查询（索引是一个高效找到行的办法，当能通过检索索引就可以读取想要的数据，那就不需要再到数据表中读取行了。如果一个索引包含了或者覆盖了满足查询语句中的条件与字段的数据就叫“覆盖索引”，就是select 的数据列只用从索引中就能够取得，即查询列要被所使用的索引覆盖）
   - 如果表结构比较简单，字段不大，甚至可以考虑全量存储，提高查询性能，降低MySQL 数据库的压力

   提示：搜索引擎和数据库之间同步是必然存在延迟的。所以对于根据分表id *（Q: 分表id 一般不做为分表键吧？所以对于分表id 作为索引字段的查询推荐使用直接查询数据库）*查询的语句，尽量保证直接查询数据库，这样不会带来一致性问题的隐患。

   **数据同步**

   一般新表和旧表直接可以采用数据同步或者双写的方式进行处理，两种方式有各自的优缺点。

   数据同步的优点：

   - 减少了代码的入侵
   - 方便扩展

   数据同步的缺点：

   - 如果服务切换为写入新表后出现问题，无法回滚
   - 新表数据对于旧表会有延迟

   双写的优点：

   - 基本不存在延迟
   - 能够快速回滚

   双写的缺点：

   - 对代码入侵比较多（即在程序执行业务操作的过程中需要额外增加写入新表的操作，且双写过后这些代码都没有意义了），拆分完成后还要去删除旧的代码

   一般根据具体情况选择一种方式就行。本次项目的具体同步关系根据整体存储架构，包括了四个部分：

   1. 旧表到新表全量主表的同步：一开始是为了减少代码入侵，方便扩展，采用了数据同步的方式。而且由于业务过多，担心有未统计到的服务没有及时改造，所以数据同步能避免这些情况导致数据丢失。但是在分批上线过程中发现，当延迟存在时，一部分应用已经升级，一部分应用尚未升级的情况下，未升级的服务仍然往旧表写数据，而升级后的应用会从新表读取数据，很多新写入的记录无法读到，对具体业务场景造成了比较严重的影响。因此，为了满足应用对于实时性的要求，我们在数据同步的基础上，重新在3.0.0版本中改造成了双写的形式。
   2. 新表全量主表到全量副表的同步
   3. 新表全量主表到映射表的同步*（Q: 映射表是用于主表和副表之间的映射关系存储吗？）*
   4. 新表全量主表到搜索引擎数据源的同步（这里搜索引擎中除了保留上述说的20%的零散查询所需的数据字段，还存储拆分分析后设计的新表的数据）

   其中2，3，4都是从新表全量主表到其他数据源的数据同步，因为没有强实时性的要求，因此，为了方便扩展，全部采用了数据同步的方式，没有进行更多的多写操作。

2. 容量评估

   在申请MySQL 存储和搜索平台索引资源前，需要进行容量评估，包括存储容量和性能指标。具体线上流量评估可以通过监控系统查看QPS，存储容量可以简单认为是线上各个表存储容量的总和。但是在全量同步的过程中，我们发现需要的实际容量的需求会大于预估。

3. 数据校验

   从上文可以看到，在本次项目中，存在大量的业务改造，属于异构迁移。从过去的一些分库分表项目来说，大多是同构/对等拆分，因此不会存在很多复杂逻辑，所以对于数据迁移的校验往往比较忽视。

   在完全对等迁移的情况下，一般确实比较少出现问题，但是，类似这样有比较多改造的异构迁移，校验绝对是重中之重！因此，必须对数据同步的结果做校验，保证业务逻辑改造正确，数据同步一致性正确。

4. 最佳实践

   1. **分库分表引起的流量放大问题**

      在做容量评估的时候，需要关注一个重要问题。就是妇女表带来的查询流量放大。这个放大有两方面的原因：

      - 索引表的二次查询。比如根据pk2 查询的，需要先通过pk2 查询pk1，然后根据pk1 查询返回结果；
      - in 的分批查询。如果一个select ... in ... 的查询，数据库中间件会根据分表键，将查询拆分落到对应的物理分表上，相当于原本一次的查询，放大为多次查询。

      因此，我们需要注意：

      - 业务层面尽量限制in 查询数量，避免流量过于放大
      - 容量评估时，需要考虑这部分放大因素，做适当冗余，另外，后续会提到业务改造上线分批进行，保证可以及时扩容
      - 分64，128还是256张表有个合理预估，拆得越多，理论上会放大越多，因此不要无谓的分过多的表，根据业务规模做适当估计
      - 对于映射表的查询，由于存在明显的冷热数据，所以我们又在中间加了一层缓存，减少数据库的压力（热数据可以存储在缓存中，避免过大的热数据需求挤满I/O）

   2. **分表键的变更方案**

      本项目中，存在一种业务情况会变更字段pk3，但是pk3 作为分表键，在数据库中间件中是不能修改的，因此，只能**在中台中**修改pk3 的更新逻辑，采用先删除，后添加的方式。

      这里需要注意，删除和添加操作的事务原子性。当然，简单处理也可以通过日志的方式，进行告警和校准。

   3. **数据同步一致性问题**

      我们都知道，数据同步中一个关键点就是（消息）数据的顺序性，如果不能保证接受的数据和产生的数据的顺序严格一致，就有可能因为（消息）数据乱序带来数据覆盖，最终带来不一致问题。

      本项目中的数据同步工具底层使用的消息队列是Kafka，Kafka 对于消息的存储，只能做到局部有序性（具体来说是每一个partition 的有序）。我们可以把同一主键的消息路由至同一分区，这样一致性一般可以保证。但是，如果存在一对多的关系，就无法保证每一行变更有序，见如下例子。

      ![使用MapReduce同步数据的不一致性](https://raw.githubusercontent.com/MylittleTown/notes/master/Linked_pictures/%E7%AC%94%E8%AE%B07-%E4%BD%BF%E7%94%A8MapReduce%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%8D%E4%B8%80%E8%87%B4%E6%80%A7.png)

      那么需要通过反查数据源获取最新数据保证一致性。但是，反查也需要考虑两个问题：

      - 如果消息变更来源于读写实例，而反查数据库是检查只读案例，那就会存在读写实例延迟导致的数据不一致问题。因此，需要保证 消息变更来源 和 反查数据库 的实例是同一个；
      - 反查对数据库会带来额外性能开销，需要仔细评估全量时候的影响
      
   4. **数据实时性问题**
   
      延迟主要需要注意几方面的问题，并根据业务实际情况做评估和衡量。
   
      - 数据同步平台的秒级延迟
      - 如果消息订阅和反查数据库都是落在只读实例上，那么除了上述数据同步平台的秒级延迟，还会有数据库主从同步的延迟*（Q: 如果只读，为什么会因为主从同步的延迟造成实时性问题，难道两个操作不能对同一个库吗？）*
      - 宽表到搜索平台的秒级延迟
   
      宽表从字面意义上讲就是字段比较多的数据库表。通常是指业务主题相关的指标，维度，属性关联在一起（面向业务）的一张数据库表。由于把不同的内容都放在同一张表存储，宽表已经不符合三范式的模型设计规范，随之带来的主要坏处就是数据的大量冗余，与之相对应的好处就是查询性能的提高与便捷。
   
      窄表是严格按照数据库设计三范式。尽量减少数据冗余，但是缺点是修改一个数据可能需要修改多张表。特点是方便扩展，能适应各种复杂的数据结构，无论有多少配置，都不用修改表结构。
   
   5. **分表后存储容量优化**
   
      由于数据同步过程中，对于单表而言，不是严格按照递增插入的，因此会产生很多“存储空洞”，使得同步后的存储总量远大于预估的容量。
   
      因此，在新库申请的时候，存储容量多申请50%。
   
      关于这个问题可以从MySQL 的存储引擎层谈起，关于InnoDB 的索引模型。在InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。而InnoDB 中，使用了B+ 树索引模型，每一个索引会对应一棵B+ 树，根据叶子节点的内容，索引类型分为主键索引和非主键索引，主键索引的叶子节点存的是整行数据（Row），非主键索引的叶子节点内容是主键的值。如下图：
   
      ![MySQL存储引擎的索引模型](https://raw.githubusercontent.com/MylittleTown/notes/master/Linked_pictures/%E7%AC%94%E8%AE%B07-MySQL%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E7%9A%84%E7%B4%A2%E5%BC%95%E6%A8%A1%E5%9E%8B.png)
   
      从图中可以看出，基于非主键索引的查询需要多扫描一棵索引树才能找到对应的数据。**所以我们在应用中应该尽量使用主键查询**。
   
      B+ 树为了维护索引的有序性，在增删改数据的时候需要做必要的维护。假设，删除R4 这个记录，InnoDB 引擎只会把R4 这个记录标记为删除。如果之后要再插入一个ID 在30和60之间的记录时，会复用这个位置。但是，无论如何，磁盘文件的大小并不会缩小，也就是说，这些标记为可复用，而并没有实际被使用的空间，就是一些“存储空洞”。
   
      同样的，在插入数据时，如果新插入的行的主键值为60，需要逻辑上挪动后面的数据，空出位置，如果R5 所在的数据页已经满了，根据B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去，这个过程称为页分裂，数据库的性能也自然会受到影响。**除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，插入一条记录使得整体空间利用率降低大约50%**
   
      这种情况在数据同步时，通过多个线程进行同步，插入各个分表并不是按照递增的顺序插入的，因此，会产生巨量的“数据空洞”，造成存储空间变大；同理，当一个表在多次被申请数据，进行增删改操作后，会出现高水位的情况，占据大量存储空间，影响数据库服务器性能。如果能把这些空洞去掉，就能达到收缩表空间的目的，可以考虑重建表。
   
5. 本章小结

   到此，分库分表的第二阶段告一段落。这一阶段涉及到很多设计思路，一方面是设计高可用，易扩展的存储架构，包括MySQL 数据冗余数量，搜索平台的索引设计，流量放大，分表键修改等；另一方面是“数据同步”本身是一个非常复杂的操作，需要考虑实时性，一致性与一对多等问题，需要引起高度重视。

   **因此，更依赖于数据校验对最终业务逻辑正确，数据同步正确的检验**

### 第三阶段：改造和上线

前两个阶段完成后，开始业务切换流程，主要步骤如下：

- 中台服务采用单读 双写模式
- 旧表往新表进行数据同步
- 所有服务升级依赖的projectDB 版本，上线RPC，如果出现问题，降版本即可回滚（上线成功后，单读新库，双写新旧库）
- 检查监控确保没有 中台服务 以外的其他服务访问旧库旧表
- 停止数据同步
- 删除旧表

1. 查询改造

   如何验证前两个阶段设计是否合理？能否完全覆盖查询的修改 是一个前提条件。当新表设计完成后，就可以以新表为标准，修改老的查询。

   **读查询的改造**

   可能查询会涉及以下几个方面：

   - 根据查询条件，需要将pk1 和pk2 的inner join 改为对应分表键的新表表名
   - 部分sql 的废弃字段处理
   - 非分表键查询改为走搜索平台（除去pk1, pk2, pk3 三个维度的模糊查询使用搜索引擎）的查询，注意保证语义一致
   - 注意单元测试避免低级错误，主要是DAO 层面

   DAO 层是基于MVC 架构来说的分层模型，分层的主要作用是解耦，其中DAO 层的作用是封装对数据库的访问：增删查改，不涉及业务逻辑，只是达到按照某个条件获得指定数据的要求*（Q: 是按照业务逻辑的需求字段存放很多条不同的sql 语句，还是按照业务涉及的字段相关的每次只操作一个字段的sql 语句？好像以上两种情况都无法和业务逻辑完全脱离，应该是只保留操作语句的结构，字段对象继承自统一一个抽象类，再通过修改字符串的方式动态编写sql 语句）*

   只有当新表结构和存储架构能完全适应查询改造，才能认为前面的设计暂时没有问题。当然，这里还有一个前提条件，就是相关查询已经全部收拢，没有遗漏。

   **写查询的改造**

   除了相关字段的更改以外，更重要的是，需要改造为旧表，新表的双写模式。这里可能**涉及到具体业务写入逻辑**，本项目尤为复杂，需要改造过程中**与业务方充分沟通**，保证写入逻辑正确。可以在双写上各加一个配置开关，方便切换。如果双写中发现新库写入有问题，可以快速关闭。同时，双写过程中不关闭旧库到新库的数据同步。*（Q: 意思是以旧库同步到新库的数据为准吗？毕竟双写需要直接根据新的写入逻辑向新库中写数据）*

   为什么呢？主要还是由于项目的特殊性。由于涉及到几十个服务，为了降低风险，必须分批上线。因此，存在比较麻烦的中间态，一部分服务是老逻辑，一部分服务是新逻辑，必须保证中间态的数据正确性。

2. 服务化改造

   为什么需要新建一个服务来承载改造后的查询呢？一方面是为了改造能够方便的升级与回滚切换，另一方面是为了将查询收拢，作为一个中台化的服务（我认为这里指的类似MVC 架构中的DAO 层）来提供相应的查询能力。

   将改造后的新的查询放在服务中，然后jar 包中的原本查询，全部替换成这个服务的client 调用。同时，升级jar 包版本到3.0.0

3. 服务分批上线

   为了降低风险，需要安排从非核心服务到核心服务的分批上线。注意，分批上线过程中，由于写服务往往是核心服务，所以安排在后面。可能出现非核心的读服务上线了，这时候会有**读新表，写旧表的中间状态**。

   - 所有相关服务使用重构分支升级projectdb 版本到3.0.0 并部署内网环境

   - 业务服务依赖于中台服务，需要订阅服务

   - **开重构分支（不要与正常迭代分支合并）**，部署内网，内网预计测试两周以上

     使用一个新的重构分支是为了在内网测试两周的时候，不影响业务正常迭代。每周更新的业务分支可以merge 到重构分支上部署内网，然后外网使用业务分支merge 到master 上部署。

   - 分批上线过程中，如果碰到依赖冲突的问题，需要及时解决并及时更新到该文档中

   - 服务上线前，**必须要求业务开发或者测试，明确评估具体api 和风险点**，做好回归

4. 旧表下线流程

   - 检查监控确保没有中台服务以外的其他服务访问旧库旧表
   - 检查数据库上的sql 审计，确保没有其他服务仍然读取旧表数据
   - 停止数据同步（此时旧表新表双写模式还在进行）
   - 删除旧表

5. 最佳实践

   **写完立即读可能读不到**

   在分批上线的过程中，遇到了写完立即读可能读不到的情况。由于业务众多，采用了分批上线的方式降低风险，存在一部分应用已经升级，一部分应用尚未升级的情况。未升级的服务仍然往旧表写数据，而升级后的应用会从新表读取数据（此时应该保持着读新表，写旧表的模式），升级非核心服务 -- 读服务应该不会有问题，目前考虑的是升级应用中的写服务之前。当延迟存在时，很多新写入的记录无法读到，对具体业务场景造成了比较严重的影响。

   延迟的主要原因有两个：

   - 写服务还没有升级，还没有开始双写，还是写旧表，这时候会有读新表，写旧表的中间状态，新旧表存在同步延迟（同一主库下）

   - 为了避免主库压力，新表数据是从旧表获取变更，然后反查旧表只读实例的数据进行同步的，主从库本身存在一定延迟（主从库结构）

     MySQL 主从同步结构，由客户端访问主库，从库复制，同步主库的所有操作，单项复制时，建议从库设置为只读。

     主从同步原理（从库从主库上同步数据的工作过程）：主库启用binlog 日志，设置binlog 日志格式，设置server_id；从库运行两个线程，Slave_IO用于复制主库主机binlog 日志文件里的SQL 到本机的relay_log 文件里，Slave_SQL 执行本机relay_log 文件里的SQL 语句，重现主库的数据操作，这两个操作都是离线完成

   解决方案一般有两种：

   - 数据同步改为双写逻辑
   - 在读接口做补偿，如果新表查不到，到旧表再查一次

   **数据库中间件唯一ID 替换自增主键**

   由于分表后，继续使用单表的自增主键，会导致全局主键冲突。因此，需要使用分布式唯一ID 来代替自增主键，这里的唯一ID 是考虑新表按照服务分为多张表后需要满足的唯一性，类似单表的自增ID。

   数据库自增sequence 的分布式ID 生成器，是一个依赖MySQL 的存在，它的基本原理是在MySQL 中存入一个数值，每有一个机器去获取ID 的时候，都会在当前ID 上累加一定的数量比如说2000，然后把当前的值加上2000返回给服务器。这样每一台机器都可以继续重复此操作获得唯一ID 区间。

   ![分表后全局唯一ID生成](https://raw.githubusercontent.com/MylittleTown/notes/master/Linked_pictures/%E7%AC%94%E8%AE%B07-%E5%88%86%E8%A1%A8%E5%90%8E%E5%85%A8%E5%B1%80%E5%94%AF%E4%B8%80ID%20%E7%94%9F%E6%88%90.png)

   比如说，现在有8个服务节点，每个服务节点使用一个sequence 功能来产生ID，每个sequence 的起始ID 不同，并且依次递增，步长都是8 。这种方法在用户防止产生的ID 重复时实现起来比较简单，也能达到性能目标，但是服务节点固定，步长也固定，缺乏可扩展性，将来如果还要增加服务节点，就不好扩展。
   
   但是仅仅有全局唯一ID 显然还不够，因为这里还会存在新旧表的ID 冲突问题。*（Q: 下面的冲突问题发生的前提是新旧表双写模式一条记录在新表和旧表的ID 是相等的，而进行新旧表同步的过程中，新表中记录的ID 可以自主设置）*
   
   因为服务比较多，为了降低风险需要分批上线。因此，存在一部分服务还是单写旧表的逻辑（写旧表，读新表，新旧表同步），一部分服务是双写的逻辑。这样的状态中，旧表的ID 策略使用的是auto_increment。如果只有单项数据来往的话（旧表到新表的同步，新表因为分表，所以存在多个用于该服务组件的表），只需要给旧表的ID 预留一个区间段，sequence 从一个较大的起始值开始就能避免冲突。
   
   但该项目中，还有新表数据和旧表数据的双写，如果采用上述方案，较大的ID 写入到旧表，旧表的auto_increment 将会被重置到该值，这样单写旧表的服务产生的递增ID 的记录必然会出现冲突。总结来说，就是由于在中间状态下，为了保证数据一致性，数据同步和双写模式同时存在于一个服务组件对应的存储层。
   
   所以这里交换了双方的区间段，旧库从较大的auto_increment 起始值开始，新表选择的ID（也就是sequence 的范围）从大于旧表的最大记录的ID 开始递增，小于旧表auto_increment 设置的起始值，很好的避免了ID 冲突问题。
   
   切换前：
   
   sequence 的起始ID 设置为当前旧表的自增ID 大小，然后旧表的自增ID 需要改大，预留一段区间，给旧表的自增ID 继续使用，防止未升级业务写入旧表的数据同步到新库后产生ID 冲突。
   
   切换后：
   
   无需任何改造，断开数据同步即可。
   
   优点：
   
   只用一份代码；切换可以使用开关进行，不用升级改造；如果万一中途旧表的auto_increment 被异常数据变大了，也不会造成什么影响。
   
   缺点：
   
   如果旧表写失败了，新表写成功了，需要日志辅助处理。
   
6. 本章小结

   完成旧表下线后，整个分库分表的改造就完成了。

### 稳定性保障

这一章主要再次强调稳定性的保障手段。作为本次项目的重要目标之一，稳定性其实贯穿在整个项目周期内，基本上在上文各个环节都已经有提到，每一个环节都要引起足够的重视，仔细设计和评估方案。

- 新表设计必须跟业务方充分沟通，保障review
- 对于“数据同步”，必须有数据校验保障数据正确性，可能导致数据不正确的原因上文已经提到了很多，包括实时性，一致性的问题。保障数据正确是上线的大前提。
- 每一阶段的变动，都必须做好快速回滚的预案
- 上线过程，都以分批上线的形式，从非核心业务开始做试点，避免故障扩大
- 监控告警要配置全面，出现问题及时收到告警，快速相应
- 单元测试，业务功能测试等要充分

