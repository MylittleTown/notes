### 前言

首先了解一下为什么要分库分表？海量数据的存储和访问成为了MySQL 数据库的瓶颈问题，日益增长的业务数据，无疑对MySQL 数据库造成了相当大的负载，同时对于系统稳定性和扩展性提出了很高的要求。

而且单台服务器的资源（CPU，磁盘，内存等）总是有限的，最终数据库所能承载的数据量，数据处理能力都将遭遇瓶颈。

目前来说一般有两种方案：

- 一种是更换存储，不使用MySQL，比如可以使用HBase，polarDB，TiDB 等分布式存储
- 如果出于各种原因考虑，还是想继续使用MySQL，一般会采用第二种方式，那就是分库分表

下面的内容将专注于梳理分库分表从架构设计到发布上线的完整过程，同时总结其中的注意事项和最佳实践。包括五个部分：

- 业务重构
- 存储架构设计
- 改造上线
- 稳定性保障
- 项目管理

### 第一阶段：业务重构

本次项目的第一大难点，在于业务重构。

而本次拆分项目涉及到的两张大表A 和B，单表将近八千万的数据，是从单体应用时代遗留下来的，从一开始就没有很好的领域驱动/ MSA（微服务）架构设计，逻辑发散非常严重，到现在已经涉及50+ 个在线服务和20+ 个离线业务的直接读写。

微服务是一种全新的互联网架构，它的基本理念是将一个肥大的系统拆分成若干个小的服务组件，组件之间的通讯采用轻量的协议完成，譬如REST API。微服务本质上是SOA（Serviceoriented Architecture）的扩展延伸。相对来说，微服务的可操作性更强，可以逐步安排合理资源，对一个大的系统进行分解，或是至少停止让它继续臃肿下去。

![image-20201207111553932](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20201207111553932.png)

*（Q: 这里的DB 采用相同的逻辑结构吗？如果每个服务对应的DB 的逻辑结构不同，那之后关于联合查询的拆分分析还有必要将冗余，融合和废弃结果通知所有业务方吗，还是只需要通知对应的业务方？）*

微服务具有如下优点：

- 按照业务功能的独立垂直开发，易于开发，理解和维护
- 支持异构开发语言，不会受限于任何技术栈
- 部署周期短，自动化部署
- 局部修改很容易部署，有利于持续集成和持续交付
- 故障隔离，一个服务出现问题不会影响整个应用

单体应用通俗的讲，就是将应用程序的所有功能都打包成一个独立的单元，可以是JAR, WAR, EAR 或者其他归档格式。单体应用有如下优点：

- 适合小团队创业初期进行快速开发
- 易于测试：因为没有额外的依赖，每项测试都可以在部署完成后立刻开始
- 部署简单：只需要将单个归档文件复制到Tomcat webapps 目录下
- 不存在分布式事务问题

但是，不管如何模块化，单体应用最终都会因为团队壮大，接入应用越来越多等出现问题。主要体现如下方面：

- **不够灵活**：对应用程序做任何细微的修改都需要将整个应用程序重新构建，重新部署。开发人员需要等到整个应用程序部署完成后才能看到变化*（Q: 相比较而言，是不是微服务架构中的小服务部件都有各自的存储单元和处理单元，所以一个业务的扩展或者修改不会对另一个服务本身的逻辑造成影响，但是比单体应用多了一项关于接口方面的测试）*。如果多个开发人员共同开发一个应用程序，那么还要等待其他开发人员完成了各自的开发。这降低了团队的灵活性和功能交付频率。
- **妨碍持续交付**：单体应用可能会比较大，构建和部署时间也相应的比较长，不利于频繁部署，阻碍持续交付
- **受技术栈限制**：对于这类应用，技术是在开发之前经过慎重评估后选定的，每个团队都必须使用相同的开发语言
- 某个服务出现OOM（Out of Memory）后对整体应用产生影响

因此，如何保证业务改造的彻底性，全面性是重中之重，不能出现有遗漏的情况。另外，表A 和表B 各自有二三十个字段，两表的主键存在一一对应关系，因此，本次分库分表项目中，还需要将两个表进行重构融合，将多余/ 无用的字段剔除。

1. 查询统计

   在线业务通过分布式链路追踪系统进行查询，按照表名作为查询条件，然后按照服务维度进行聚合，找到所有相关服务，写一个文档记录相关团队和服务。

   这里特别注意下，很多不是只有在线应用在使用，很多离线算法和数据分析的业务也在使用，这里需要一并的处理好，做好线下跨团队的沟通和调研工作，以免切换后影响正常的数据分析。

2. 查询拆分与迁移

   创建一个jar 包，根据查询统计结果，与服务owner 合作将服务中的相关查询都迁移到这个jar 包中（本项目的jar 包叫做projectdb，之后也将用这个名字介绍）。此处为1.0.0 版本。

   然后将原本服务内的xxxMapper.xxxMethod() 全部改成projectdb.xxxMethod() 进行调用。

   这样做有两个好处：

   - 方便做后续的查询拆分分析
   - 方便后续直接将jar 包中的查询替换为改造后 中台服务 的rpc 调用，业务方只需升级jar 包版本，即可快速从sql 调用改为rpc 查询

   rpc（Remote Procedure Call） -- 远程过程调用，是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。采用客户机/服务器模式。请求程序就是一个客户机，而服务提供程序就是一个服务器。

   这一步骤相当于是按照业务方将各个查询服务分组，务必梳理各个服务作全面的迁移，不能遗漏，否则可能会导致拆分分析不全面，遗漏了相关字段。由于本次拆分项目涉及到的服务太多，需要收拢到一个jar 包，更方便后期的改造。如果实际分库分表项目中仅仅涉及一两个服务的，这一步是可以不做的。

3. 联合查询的拆分分析

   根据上述收拢的jar 包中的查询，结合实际情况将查询进行分类和判断，把一些历史遗留的问题，和已经废弃的字段做一些整理。

   整理过程可以从以下几个点思考：

   - 哪些查询是无法拆分的？例如分页查询（尽可能的改造，实在改不了的只能以冗余列的形式）
   - 哪些查询是可以业务上join 拆分的？
   - 哪些表/字段是可以融合的？
   - 哪些字段是冗余的？
   - 哪些字段可以直接废弃了？
   - 根据业务具体场景和sql 整体统计，识别关键的分表键。其余查询走搜索平台

   思考后得到一个查询改造总体思路和方案。同时在本项目中需要将两张表融合为一张表，废弃冗余字段和无效字段。

4. 新表设计

   这一步基于上一步联合查询的拆分分析，得出旧表融合，冗余，废弃字段的结果，设计新表的字段。

   产生新表设计结构后，必须发给各个相关业务方进行review，并保证所有业务方都通过该表的设计。有必要的话可以进行一次线下review。

   如果新表的过程中，对部分字段进行了废弃，必须通知所有业务方进行确认*（Q: 这里有关“通知所有业务方进行确认”，前提是所有业务方使用的DB 的逻辑结构或者关系模式是相同的吧，如果不同，是不是不用通知所有？）*。对于新表的设计，除了字段的梳理，也需要根据查询，重新设计，优化索引。

5. 第一次升级

   新表设计完成后，先做一次jar 包内sql 查询的改造，将旧的字段全部更新为新表的字段。此处为2.0.0 版本。然后让所有服务升级jar 包版本，以此来保证这些废弃字段确实不使用了，新的表结构字段能够完全覆盖过去的业务场景。

   特别注意的是，由于涉及服务众多，可以将服务按照非核心 与 核心的区别，然后分批次上线，避免出现问题导致严重故障或者大范围回滚。

6. 最佳实践

   - **尽量不改变原表的字段名称**：在做新表融合的时候，一开始只是简单归并表A 和表B 的表，因此很多字段名相同的字段做了重命名。后来字段精简过程中，删除了很多重复字段，但是没有将重命名的字段改回来。导致后期上线的过程中，不可避免的需要业务方进行重构字段名。因此，新表设计的时候，除非逼不得已，不要修改原表的字段名称。*（Q: 这里关于由于重复字段名导致的重命名有什么方法避免吗？）*
   - **新表的索引需要仔细斟酌**：新表的索引不能简单照搬旧表，而是需要根据查询拆分分析后，重新设计。尤其是一些字段的融合后，可能可以归并一些索引，或者设计一些更高性能的索引。

### 第二阶段：存储架构设计（核心）

对于任何分库分表的项目，存储架构的设计都是最核心的部分。

1. 整体架构

   根据第一阶段整理的查询梳理结果，总结了这样的查询规律：

   - 80% 以上的查询都是通过或者带有字段pk1，字段pk2，字段pk3这三个维度进行查询的，其中pk1 和pk2 由于历史原因存在一一对应关系
   - 20% 的查询千奇百怪，包括模糊查询，其他字段查询等等

   因此，设计了如下的整体架构，引入了数据库中间件，数据同步工具，搜索引擎（阿里云opensearch/ES）等。

   ![image-20201207150136552](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20201207150136552.png)

   **MySQL分表存储**

   MySQL 分表的维度是根据查询拆分分析的结果确定的。我们发现pk1/pk2/pk3 可以覆盖80% 以上的主要查询。让这些查询根据分表键直接走MySQL 数据库即可。

   分表键即 **分库/分表字段**，是在水平拆分过程中用于生成拆分规则的数据表字段。数据表拆分的首要原则，就是要尽可能找到数据表中的数据在业务逻辑上的主体，并确定大部分（或核心的）数据库操作都是围绕这个主体的数据进行，然后可使用该主体对应的字段作为分表键，进行分库分表。

   业务逻辑上的主体通常是与业务的应用场景相关的某一个单独的字段（也存在业务必须要有多个分表键，没有办法归一成一个，存在主分表键，即主维度，在主维度上的数据能够增删查改；辅助分表键，辅助维度，在辅助维度上只能进行数据查询），下面的一些典型应用场景都有明确的业务逻辑主体，可用于分表键：

   - 面向用户的互联网应用，都是围绕用户维度来做各种操作，那么业务逻辑主体就是用户，可使用用户对应的字段作为分表键；
   - 侧重于卖家的电商应用，都是围绕卖家维度来进行各种操作，那么业务逻辑主体就是卖家，可使用卖家对应的字段作为分表键；

   如果确实实在找不到合适的业务逻辑主体作为分表键，那么可以考虑下面的方法来选择分表键：

   - 根据数据分布和访问的均衡度来考虑分表键，尽量将数据表中的数据相对均匀的分布在不同的物理分库/分表中，适用于大量分析型查询的应用场景（查询并发度大部分能维持为1）
   - 按照数字（字符串）类型与时间类型字段相结合作为分表键，进行分库和分表，适用于日志检索类的应用场景

   注意：无论选择什么拆分键，采用何种拆分策略，都要注意拆分值（例如学生表中学号大于254作为拆分值）是否存在热点的问题，尽量规避热点数据来选择拆分键；不一定需要拿数据库主键当作分表键，也可以拿其他业务值当分表键。拿主键当分表键的好处是可以散列均衡，减少热点问题。

   原则上一般最多维护一个分表的全量数据，因为过多的全量数据会造成存储的浪费，数据同步的额外开销，更多的不确定性，不易扩展等问题。但是由于本项目pk1 和pk3 的查询语句都对实时性有比较高的要求，因此，维护了pk1 和pk3 作为分表键的两份全量数据。

   而pk2 和pk1 由于历史原因，存在一一对应关系，可以仅保留一份映射表即可，只存储pk1 和pk2 两个字段。

   **搜索平台索引存储**

   搜索平台索引，可以覆盖剩余20% 的零散查询。这些查询往往不是根据分表键进行的，或者是带有模糊查询要求。

   对于搜索平台来说，一般不存储全量数据（尤其是一些大varchar 字段），只存储主键和查询需要的索引字段，搜索得到结果后，根据主键去MySQL 存储中拿到需要的记录。（这里我想到了上一篇关于“从HBase 逆向迁移到Kafka” 的过程中，将HBase 存储的海量数据的RowKey 存储到HDFS 中方便拿取）

   当然，从后期实践结果来看，这里还是需要做一些权衡：

   - 有些非索引字段，如果不是很大，也可以冗余进来，类似覆盖索引，避免多一次SQL 查询（索引是一个高效找到行的办法，当能通过检索索引就可以读取想要的数据，那就不需要再到数据表中读取行了。如果一个索引包含了或者覆盖了满足查询语句中的条件与字段的数据就叫“覆盖索引”，就是select 的数据列只用从索引中就能够取得，即查询列要被所使用的索引覆盖）
   - 如果表结构比较简单，字段不大，甚至可以考虑全量存储，提高查询性能，降低MySQL 数据库的压力

   提示：搜索引擎和数据库之间同步是必然存在延迟的。所以对于根据分表id *（Q: 分表id 一般不做为分表键吧？所以对于分表id 作为索引字段的查询推荐使用直接查询数据库）*查询的语句，尽量保证直接查询数据库，这样不会带来一致性问题的隐患。

   **数据同步**

   一般新表和旧表直接可以采用数据同步或者双写的方式进行处理，两种方式有各自的优缺点。

   数据同步的优点：

   - 减少了代码的入侵
   - 方便扩展

   数据同步的缺点：

   - 如果服务切换为写入新表后出现问题，无法回滚
   - 新表数据对于旧表会有延迟

   双写的优点：

   - 基本不存在延迟
   - 能够快速回滚

   双写的缺点：

   - 对代码入侵比较多（即在程序执行业务操作的过程中需要额外增加写入新表的操作，且双写过后这些代码都没有意义了），拆分完成后还要去删除旧的代码

   一般根据具体情况选择一种方式就行。本次项目的具体同步关系根据整体存储架构，包括了四个部分：

   1. 旧表到新表全量主表的同步：一开始是为了减少代码入侵，方便扩展，采用了数据同步的方式。而且由于业务过多，担心有未统计到的服务没有及时改造，所以数据同步能避免这些情况导致数据丢失。但是在分批上线过程中发现，当延迟存在时，一部分应用已经升级，一部分应用尚未升级的情况下，未升级的服务仍然往旧表写数据，而升级后的应用会从新表读取数据，很多新写入的记录无法读到，对具体业务场景造成了比较严重的影响。因此，为了满足应用对于实时性的要求，我们在数据同步的基础上，重新在3.0.0版本中改造成了双写的形式。
   2. 新表全量主表到全量副表的同步
   3. 新表全量主表到映射表的同步*（Q: 映射表是用于主表和副表之间的映射关系存储吗？）*
   4. 新表全量主表到搜索引擎数据源的同步（这里搜索引擎中除了保留上述说的20%的零散查询所需的数据字段，还存储拆分分析后设计的新表的数据）

   其中2，3，4都是从新表全量主表到其他数据源的数据同步，因为没有强实时性的要求，因此，为了方便扩展，全部采用了数据同步的方式，没有进行更多的多写操作。

2. 容量评估

   在申请MySQL 存储和搜索平台索引资源前，需要进行容量评估，包括存储容量和性能指标。具体线上流量评估可以通过监控系统查看QPS，存储容量可以简单认为是线上各个表存储容量的总和。但是在全量同步的过程中，我们发现需要的实际容量的需求会大于预估。

3. 数据校验

   从上文可以看到，在本次项目中，存在大量的业务改造，属于异构迁移。从过去的一些分库分表项目来说，大多是同构/对等拆分，因此不会存在很多复杂逻辑，所以对于数据迁移的校验往往比较忽视。

   在完全对等迁移的情况下，一般确实比较少出现问题，但是，类似这样有比较多改造的异构迁移，校验绝对是重中之重！因此，必须对数据同步的结果做校验，保证业务逻辑改造正确，数据同步一致性正确。

4. 最佳实践

   1. **分库分表引起的流量放大问题**

      在做容量评估的时候，需要关注一个重要问题。就是妇女表带来的查询流量放大。这个放大有两方面的原因：

      - 索引表的二次查询。比如根据pk2 查询的，需要先通过pk2 查询pk1，然后根据pk1 查询返回结果；
      - in 的分批查询。如果一个select ... in ... 的查询，数据库中间件会根据分表键，将查询拆分落到对应的物理分表上，相当于原本一次的查询，放大为多次查询。

      因此，我们需要注意：

      - 业务层面尽量限制in 查询数量，避免流量过于放大
      - 容量评估时，需要考虑这部分放大因素，做适当冗余，另外，后续会提到业务改造上线分批进行，保证可以及时扩容
      - 分64，128还是256张表有个合理预估，拆得越多，理论上会放大越多，因此不要无谓的分过多的表，根据业务规模做适当估计
      - 对于映射表的查询，由于存在明显的冷热数据，所以我们又在中间加了一层缓存，减少数据库的压力（热数据可以存储在缓存中，避免过大的热数据需求挤满I/O）

   2. **分表键的变更方案**

      本项目中，存在一种业务情况会变更字段pk3，但是pk3 作为分表键，在数据库中间件中是不能修改的，因此，只能**在中台中**修改pk3 的更新逻辑，采用先删除，后添加的方式。

      这里需要注意，删除和添加操作的事务原子性。当然，简单处理也可以通过日志的方式，进行告警和校准。

   3. **数据同步一致性问题**

      我们都知道，数据同步中一个关键点就是（消息）数据的顺序性，如果不能保证接受的数据和产生的数据的顺序严格一致，就有可能因为（消息）数据乱序带来数据覆盖，最终带来不一致问题。

      本项目中的数据同步工具底层使用的消息队列是Kafka，Kafka 对于消息的存储，只能做到局部有序性（具体来说是每一个partition 的有序）。我们可以把同一主键的消息路由至同一分区，这样一致性一般可以保证。但是，如果存在一对多的关系，就无法保证每一行变更有序，见如下例子。

      ![image-20201208104549519](C:\Users\92486\AppData\Roaming\Typora\typora-user-images\image-20201208104549519.png)

      那么需要通过反查数据源获取最新数据保证一致性。但是，反查也需要考虑两个问题：

      - 如果消息变更来源于读写实例，而反查数据库是检查只读案例，那就会存在读写实例延迟导致的数据不一致问题。