### 图状结构数据广泛存在

字节跳动的所有产品的大部分业务数据，几乎都可以归入到以下三种：

1. 用户信息，用户和用户的关系（关注，好友等） -- 社交属性
2. 内容（视频，文章，广告等） -- 产品价值
3. 用户和内容的联系（点赞，评论，转发，点击广告等） -- 用户参与度

这三种数据关联在一起，形成图状（Graph）结构数据。



为了满足social graph 的在线增删查改场景，字节跳动自研了分布式图存储系统 -- *ByteGraph*。针对上述图状结构数据，*ByteGraph* 支持有向属性图数据模型，支持*Gremlin* 查询语言，支持灵活丰富的写入和查询接口，读写吞吐可扩展到千万QPS，延迟毫秒级。目前，*ByteGraph* 支持了头条，抖音，TikTok，西瓜，火山等字节跳动全部产品线，遍布全球机房。



*Gremlin*是*JanusGraph* 的查询语言，用于从图中检索数据和修改图中的数据。*Gremlin* 是一种面向路径的语言，可以简洁的表示复杂的图遍历和变种操作。*Gremlin* 是一种功能语言，遍历运算符被链接在一起以形成类似路径的表达式。例如，“从大力神*Hercules* 出发，先走到他的父亲，再走到他父亲的父亲，然后返回祖父的名字。”



*ByteGraph* 主要用于在线OLTP 场景（OLTP 指的是*On-line Transaction Processing*，联机事务处理过程，也称为面向交易的处理过程，其特征是前台接收的用户数据可以立即传送到计算中心进行处理，并在很短时间内给出处理结果，是对用户操作快速响应的方式之一）

### 自研图数据库（ByteGraph）介绍

图数据库本质也是一个存储系统，它和常见的KV 存储系统，MySQL 存储系统的相比主要区别在于目标数据的逻辑关系不同和访问模式不同，对于数据内在关系是图模型以及在图上游走类和模式匹配类的查询，比如社交关系查询，图数据库会有更大的性能优势和更加简洁高效的接口。



ByteGraph 图数据库应用场景特点，总结为：

- 海量数据存储：百亿点，百亿边的数据规模；并且图符合幂律分布，比如少量大V粉丝达到几千万
- 海量吞吐：最大集群QPS达到数千万
- 低延迟：要求访问延迟pct99 需要限制在毫秒级
- 读多写少：读流量是写流量的接近百倍之多
- 轻量查询多，重量查询少：90%查询是图上二度以内查询（二度：点的度为2）
- 容灾架构演进：要能支持字节跳动城域网，广域网，洲际网络之间主备容灾，异地多活等不同容灾部署方案

“容灾”与“备份”不是同一个概念，“容灾”是目的，要求的是应用的接管，而“备份”要求的是数据的复制，只是实现容灾的其中一种手段，不是唯一；数据备份比较容易理解，保证数据的不丢失，而应用的接管是要接管服务器的处理工作，使得应用不间断。

容灾备份架构：

1. 双活数据中心：两边都是活动在线提供服务的，两个数据中心都是对等的，不分主从，并可同时部署业务
2. 传统主备模式：一个是主数据中心用于承担用户的业务，一个是备份数据中心用于备份主数据中心的数据，配置，业务等。备份方式一般有热备，冷备，双活



数据模型，把数据之间的关系“翻译”成有向属性图，称之为“构图”过程

用户关系之于*ByteGraph*，第一步就是需要把用户抽象为点，第二步把“关注关系”，“好友关系”抽象为边



![图数据库架构](https://raw.githubusercontent.com/MylittleTown/notes/master/Linked_pictures/%E7%AC%94%E8%AE%B01-%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9E%B6%E6%9E%84.png)

1. 查询层（bgdb）

   bgdb 层和MySQL 的SQL 层一样，主要工作是做读写请求的解析和处理；其中，所谓“处理”可以分为以下三个步骤：

   - 将客户端发来的Gremlin 查询语句做语法解析，生成执行计划
   - 并根据一定的路由规则找到目标数据所在的存储节点（bgkv），将执行计划中的读写请求发送给多个bgkv
   - 将bgkv 读写结果汇总以及过滤处理，得到最终结果，返回给客户端

   bgdb 层没有状态，可以水平扩容，用Go语言开发。

   ![图数据库架构中查询层设计](https://raw.githubusercontent.com/MylittleTown/notes/master/Linked_pictures/%E7%AC%94%E8%AE%B01-%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9E%B6%E6%9E%84%E4%B8%AD%E6%9F%A5%E8%AF%A2%E5%B1%82%E8%AE%BE%E8%AE%A1.png)

2. 存储/事务引擎层（bgkv）

   bgkv 层是由多个进程实例组成，每个实例管理整个集群数据的一个子集。

   bgkv 层的实现和功能有点类似内存数据库，提供高性能的数据读写功能，其特点是：

   - 接口不同：只提供点边读写接口
   - 支持算子下推：通过把计算（算子）移动到存储（bgkv）上，能够有效提升读性能；举例：比如某个大V 最近一年一直在涨粉，bgkv 支持查询最近的100个粉丝，则不必读出所有的百万粉丝
   - 缓存存储有机结合：其作为KV  store 的缓存层，提供缓存管理功能，支持缓存加载，换出，缓存和磁盘同步异步sync 等复杂功能

   从上述描述可以看出，bgkv 的性能和内存使用效率是非常关键的，因此采用C++ 编写。

3. 磁盘存储层（KV Cluster）

   为了能够提供海量存储空间和较高的可靠性，可用性，数据必须最终落入磁盘，我们底层存储是选择了公司自研的分布式KV Store。

   （有关KV Store，即KV 存储的笔记见下一篇）
   
4. 如何把图存储在KV 数据库中

   在字节跳动的业务场景中，存在很多访问热度和“数据密度”极高的场景，比如抖音的大V，热门的文章等，其粉丝数或者点赞数会超过千万级别；但作为KV Store，希望业务方的KV 对的大小（Byte 数）是控制在KB 量级的，且最好是大小均匀的：对于太大的value，是会瞬间打满I/O 路径的，无法保证线上稳定性；对于特别小的value，则存储效率比较低。

   采用边聚合方式，使得KV Store 中的value 大小是均匀的，解决图中某个点有千万条边的出度的存储和线上毫秒级的增删查改操作，具体可以用以下四条来描述：

   1. 一个点和其所有相连的边组成了一数据组；不同的起点及其重点是属于不同的Group，是存储在不同的KV 对的；比如用户A 的粉丝和用户B 的粉丝，就是分成不同KV 存储
   2. 对于某一个点及其出边，当出度数量比较小（KB 级别），将其所有出度即所有终点序列化为一个KV 对，我们称之为一级存储方式
   3. 当一个点的出度逐渐增多，比如一个普通用户逐渐成长为抖音大V，我们则采用分布式B-Tree 组织这百万粉丝，称之为二级存储
   4. 一级存储和二级存储之间可以在线并发安全的互相切换

   **一级存储方式**

   一级存储方式中，只有一个KV 对，key 和value 的编码：

   - key: 某个起点id + 起点type + 边type
   - value: 此起点的所有出边（Edge）及其边上属性聚合作为value，但不包括终点的属性

   **二级存储方式（点的出度大于阈值）**

   如果一个大V 疯狂涨粉，则存储粉丝的value 就会越来越大，剞劂这个问题的思路也很朴素：拆成多个KV 对。

   ByteGraph 的方式就是把所有的出度和终点拆成多个KV 对，所有KV 对形成一颗逻辑上的分布式B-Tree，其中树的节点关系是靠KV 中key 来指向的，并非内存指针；B-Tree 是分布式的，是指构成这棵树的各级节点是分布在集群多个实例上的，并不是单机索引关系。具体关系如下图所示：

   ![KV数据库中二级存储方式](https://raw.githubusercontent.com/MylittleTown/notes/master/Linked_pictures/%E7%AC%94%E8%AE%B01-KV%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E4%BA%8C%E7%BA%A7%E5%AD%98%E5%82%A8%E6%96%B9%E5%BC%8F.png)

   其中，整棵B-Tree 由多组KV 对组成，按照关系可以分为三种数据：

   - 根节点：根节点本质是一个KV 系统中的一个key，其编码方式和一级存储中的key 相同
   - Meta 数据：本质是一个KV 中的value，和根节点组成了KV 对；内部存储了多个PartKey，其中每个PartKey 都是一个KV 对中的key，其对应的value 数据就是下面介绍的Part 数据
   - Part 数据：每个Part 存储部分出边的属性和终点ID，每个Part 都是一个KV 对的value，其对应的key 存储在Meta 数据中

   从上述描述可以看出，对于一个出度很多的点和其边的数据（比如大V 和其粉丝），在ByteGraph 中，是存储为多个KV 的，面对增删查改的需求，都需要在B-Tree 上做二分查找。相比于一条边一个KV 对或者所有边存储成一个KV 对的方式，B-Tree 的组织方式能够有效的在读放大和写放大之间做一些调整。 

### 热点数据读写解决

热点数据在字节跳动的线上业务中广泛存在：热点视频，热点文章，大V 用户，热点广告等等；热点数据可能会出现瞬时大量读写。ByteGraph 在线上业务的实践中，打磨出一套应对性方案。

1. 热点读

   热点读的场景随处可见，比如线上实际场景：某个热点视频被频繁刷新，查看点赞数量等。在这种场景下，意味着访问有很强的数据局部性，**缓存命中率会很高**，因此，我们设计实现了多级的Query Cache 机制以及热点请求转发机制；在bgdb 查询层缓存查询结果（考虑读操作的事务不适合dbkv 层，故在bgdb 层设置缓存操作结果），bgdb 单节点缓存命中读性能20w QPS 以上，而且多个bgdb 可以并发处理同一个热点的读请求（注意：是并发处理读请求，写请求应该要考虑一致性），则系统整体应对热点的“弹性”是非常充足的。

2. 热点写

   热点读和热点写通常是相伴相生的，热点写的例子也是随处可见，比如：热点新闻被疯狂转发，热点视频被疯狂点赞等等。对于数据库而言，**热点写入导致的性能退化的背后原因有两个：行锁冲突高或者磁盘写入IOPS被打满**，下面来分析：

   1. 行锁冲突高：目前ByteGraph 是单行事务模型，只有内存结构锁，这个锁的并发量是每秒千万级，基本不会构成写入瓶颈
   2. 磁盘IOPS被打满：
      - IOPS（I/O Count Per Second）的概念：磁盘每秒的写入请求数量是有上限的，不同型号的固态硬盘的IOPS各异，但都有一个上限，当上游写入流量超过这个阈值的时候，请求就会排队，造成整个数据通路堵塞，延迟就会呈现指数上涨最终服务变成不可用
      - Group Commit 解决方案：Group Commit 是数据库中的一个成熟的技术方案，简单来讲，就是多个写请求在bgkv 内存中汇聚起来，聚成一个Batch 写入KV Store，则对外体现的写入速率就是BatchSize * IOPS。

### 图的索引

就像关系型数据库一样，图数据库也可以构建索引。默认情况下，对于同一个起点，我们会采用边上的属性（时间戳）作为主键索引；但为了加速查询，我们也支持其他元素（终点，其他属性）来构建二级的聚簇索引，这样很多查找就从全部遍历优化成了二分查找，使得查询速度大幅提升。

ByteGraph 默认按照边上的时间戳来排序存储，因此对于以下请求，查询效率很高：

- 查询最近的若干个点赞
- 查询某个指定时间范围窗口内加的好友

方向的索引可能有些费解，举个例子说明下：给定两个用户来查询是否存在粉丝关系，其中一个用户是大V，另一个是普通用户，大V 的粉丝可达千万，但普通用户的关注者一般不会很多；因此，如果用普通用户作为起点大V作为终点，查询代价就会低很多。也就是说，我们对于“粉丝关系”这种边，key 通常存放普通用户作为起点。

​      

​      
